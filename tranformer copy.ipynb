{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tdqm import tdqm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" \\\n",
    "Our transformer model will have an input size of (32, 128, 5) \\\n",
    "32 -> batch size \\\n",
    "128 -> sequence length \\\n",
    "5 -> number of features (Open, Low, High, Close) \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are separated into sequences of 128 days, 4 price features and 1 volume feature\n",
    "seq_len = 128\n",
    "# target_len = 3\n",
    "\n",
    "# during a single step the model receives 32 sequences\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock for the deep learning\n",
    "ticker = \"GOOGL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset (use the function in data/data.py)\n",
    "df = pd.read_csv(\n",
    "    \"data/data.csv\", usecols=[\"Date\", \"High\", \"Low\", \"Open\", \"Close\", \"Volume\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>92.391</td>\n",
       "      <td>89.701999</td>\n",
       "      <td>90.684000</td>\n",
       "      <td>91.674001</td>\n",
       "      <td>37950530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>93.741</td>\n",
       "      <td>91.001999</td>\n",
       "      <td>91.970000</td>\n",
       "      <td>92.860001</td>\n",
       "      <td>37004050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>95.041</td>\n",
       "      <td>92.394999</td>\n",
       "      <td>93.220000</td>\n",
       "      <td>94.087001</td>\n",
       "      <td>34804010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>96.079</td>\n",
       "      <td>93.388999</td>\n",
       "      <td>94.189001</td>\n",
       "      <td>95.074001</td>\n",
       "      <td>34001380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>96.937</td>\n",
       "      <td>94.267000</td>\n",
       "      <td>95.176000</td>\n",
       "      <td>95.927001</td>\n",
       "      <td>33791360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    High        Low       Open      Close      Volume\n",
       "6661  2022-11-14  92.391  89.701999  90.684000  91.674001  37950530.0\n",
       "6662  2022-11-15  93.741  91.001999  91.970000  92.860001  37004050.0\n",
       "6663  2022-11-16  95.041  92.394999  93.220000  94.087001  34804010.0\n",
       "6664  2022-11-17  96.079  93.388999  94.189001  95.074001  34001380.0\n",
       "6665  2022-11-18  96.937  94.267000  95.176000  95.927001  33791360.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid dividing by 0\n",
    "df[\"Volume\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n",
    "\n",
    "# Sort the values based on date\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "# Apply moving average with a window of 10 days to all columns\n",
    "df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].rolling(10).mean() \n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open\"] = df[\"Open\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"High\"] = df[\"High\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Low\"] = df[\"Low\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Close\"] = df[\"Close\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Volume\"] = df[\"Volume\"].pct_change()\n",
    "\n",
    "# Drop the rows with the NaN created by the percentage change\n",
    "df.dropna(how=\"any\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values to create the separation of the dataset\n",
    "times = sorted(df.index.values)\n",
    "last_10pct = sorted(df.index.values)[-int(0.1 * len(times))]\n",
    "last_20pct = sorted(df.index.values)[-int(0.2 * len(times))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max price columns\n",
    "min_return = min(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].min(axis=0)\n",
    ")\n",
    "max_return = max(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].max(axis=0)\n",
    ")\n",
    "\n",
    "# Min-max normalize price columns (0-1 range)\n",
    "df[\"Open\"] = (df[\"Open\"] - min_return) / (max_return - min_return)\n",
    "df[\"High\"] = (df[\"High\"] - min_return) / (max_return - min_return)\n",
    "df[\"Low\"] = (df[\"Low\"] - min_return) / (max_return - min_return)\n",
    "df[\"Close\"] = (df[\"Close\"] - min_return) / (max_return - min_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max volume column\n",
    "min_volume = df[(df.index < last_20pct)][\"Volume\"].min(axis=0)\n",
    "max_volume = df[(df.index < last_20pct)][\"Volume\"].max(axis=0)\n",
    "\n",
    "# Min-max normalize volume columns (0-1 range)\n",
    "df[\"Volume\"] = (df[\"Volume\"] - min_volume) / (max_volume - min_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5325, 5)\n",
      "Validation data shape: (666, 5)\n",
      "Test data shape: (665, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_17753/2609535530.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_17753/2609535530.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_17753/2609535530.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop(columns=[\"Date\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.533315</td>\n",
       "      <td>0.624841</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.555345</td>\n",
       "      <td>0.137539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.403434</td>\n",
       "      <td>0.485776</td>\n",
       "      <td>0.530637</td>\n",
       "      <td>0.358239</td>\n",
       "      <td>0.249561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.374853</td>\n",
       "      <td>0.488245</td>\n",
       "      <td>0.481612</td>\n",
       "      <td>0.363512</td>\n",
       "      <td>0.223069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.362567</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>0.488121</td>\n",
       "      <td>0.328155</td>\n",
       "      <td>0.241760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.281407</td>\n",
       "      <td>0.291742</td>\n",
       "      <td>0.271953</td>\n",
       "      <td>0.329953</td>\n",
       "      <td>0.364811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        High       Low      Open     Close    Volume\n",
       "10  0.533315  0.624841  0.594562  0.555345  0.137539\n",
       "11  0.403434  0.485776  0.530637  0.358239  0.249561\n",
       "12  0.374853  0.488245  0.481612  0.363512  0.223069\n",
       "13  0.362567  0.446667  0.488121  0.328155  0.241760\n",
       "14  0.281407  0.291742  0.271953  0.329953  0.364811"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
    "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
    "df_test = df[(df.index >= last_10pct)]\n",
    "\n",
    "# Drop the date column from the splitted datasets\n",
    "df_train.drop(columns=[\"Date\"], inplace=True)\n",
    "df_val.drop(columns=[\"Date\"], inplace=True)\n",
    "df_test.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Train data into arrays np.ndarray\n",
    "train_data = df_train.values\n",
    "val_data = df_val.values\n",
    "test_data = df_test.values\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.T#[:,0:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "# src = torch.rand(10, 5, 128)\n",
    "# out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerData(Dataset):\n",
    "    def __init__(self, data: np.ndarray, seq_len: int) -> None:\n",
    "        \"\"\"Init function of dataset class\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): data from the dataframe to numpy\n",
    "            seq_len (int): len of values to base for the prediction\n",
    "        \"\"\"\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for i in range(seq_len, len(data)):\n",
    "            # Chunks of  data with a length of 128 df-rows\n",
    "            self.inputs.append(data[i-seq_len:i])\n",
    "            \n",
    "            # Value of 4th column (Close Price) of df-row 128+1\n",
    "            self.targets.append(data[:, 3][i])\n",
    "        \n",
    "        self.inputs, self.targets = torch.FloatTensor(np.array(self.inputs)), torch.FloatTensor(np.array(self.targets))\n",
    "        # print(self.inputs)\n",
    "        # print(self.inputs.shape)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Get item at a certain index\n",
    "\n",
    "        Args:\n",
    "            idx (int): index to get the value\n",
    "\n",
    "        Returns:\n",
    "            dict: returns the input and the target\n",
    "        \"\"\"\n",
    "        return {'inputs': self.inputs[idx],\n",
    "                'targets': self.targets[idx]}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Length function \n",
    "\n",
    "        Returns:\n",
    "            int: length of the Dataset\n",
    "        \"\"\"\n",
    "        return min(len(self.inputs), len(self.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TickerData(data=train_data, seq_len=seq_len)\n",
    "val_data = TickerData(data=val_data, seq_len=seq_len)\n",
    "test_data = TickerData(data=test_data, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5197"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _,batch in enumerate(train_loader):\n",
    "#     x = batch[\"inputs\"]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "# input_window = 100\n",
    "# output_window = 5\n",
    "# batch_size = 10 # batch size\n",
    "device = torch.device(\"cpu\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()       \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         #pe.requires_grad = False\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # Add extra dimension to self.pe\n",
    "#         self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(x)\n",
    "#         # print(x.size())\n",
    "#         print(self.pe)\n",
    "#         # print(self.pe.size())\n",
    "#         # self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "# class T2V(nn.Module):\n",
    "    \n",
    "#     def __init__(self, output_dim=None):\n",
    "#         super(T2V, self).__init__()\n",
    "#         self.output_dim = output_dim\n",
    "        \n",
    "#         self.W = nn.Parameter(torch.Tensor(input_shape[-1], self.output_dim))\n",
    "#         nn.init.uniform_(self.W)\n",
    "        \n",
    "#         self.P = nn.Parameter(torch.Tensor(input_shape[1], self.output_dim))\n",
    "#         nn.init.uniform_(self.P)\n",
    "        \n",
    "#         self.w = nn.Parameter(torch.Tensor(input_shape[1], 1))\n",
    "#         nn.init.uniform_(self.w)\n",
    "        \n",
    "#         self.p = nn.Parameter(torch.Tensor(input_shape[1], 1))\n",
    "#         nn.init.uniform_(self.p)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         original = self.w * x + self.p\n",
    "#         sin_trans = torch.sin(torch.matmul(x, self.W) + self.P)\n",
    "        \n",
    "#         return torch.cat([sin_trans, original], dim=-1)\n",
    "\n",
    "# class Time2Vector(nn.Module):\n",
    "#   def __init__(self, seq_len):\n",
    "#     super(Time2Vector, self).__init__()\n",
    "#     self.seq_len = seq_len\n",
    "\n",
    "#     # Initialize weights and biases with shape (seq_len)\n",
    "#     self.weights_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "#     self.bias_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "#     self.weights_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "#     self.bias_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     '''Calculate linear and periodic time features'''\n",
    "#     # print(x[:,:,:4])\n",
    "#     x = torch.mean(x[:,:,:4], dim=-1) \n",
    "#     # print(x)\n",
    "#     time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "#     time_linear = time_linear.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "#     time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)\n",
    "#     time_periodic = time_periodic.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "#     return torch.nan_to_num(torch.cat([time_linear, time_periodic], dim=-1).swapaxes(1,2)) # shape = (batch, seq_len, 2)\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.weights_linear = nn.Parameter(torch.Tensor(int(self.seq_len)))\n",
    "        nn.init.uniform_(self.weights_linear)\n",
    "\n",
    "        self.bias_linear = nn.Parameter(torch.Tensor(int(self.seq_len)))\n",
    "        nn.init.uniform_(self.bias_linear)\n",
    "\n",
    "        self.weights_periodic = nn.Parameter(torch.Tensor(int(self.seq_len)))\n",
    "        nn.init.uniform_(self.weights_periodic)\n",
    "\n",
    "        self.bias_periodic = nn.Parameter(torch.Tensor(int(self.seq_len)))\n",
    "        nn.init.uniform_(self.bias_periodic)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x[:,:,:4], dim=-1)\n",
    "        time_linear = self.weights_linear * x + self.bias_linear\n",
    "        time_linear = time_linear.unsqueeze(-1)\n",
    "\n",
    "        time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)\n",
    "        time_periodic = time_periodic.unsqueeze(-1)\n",
    "\n",
    "        return torch.cat([time_linear, time_periodic], dim=-1)\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=128,num_layers=1,dropout=0.0):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        # self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.t2v = Time2Vector(128)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)     \n",
    "        self.decoder = nn.Linear(128,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        \n",
    "        if self.src_mask is None or self.src_mask.size(0) != 7:\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(7).to(device)\n",
    "            self.src_mask = mask\n",
    "        # print(src.size())\n",
    "        # src = self.pos_encoder(src)\n",
    "        # time2vector = self.t2v(src)\n",
    "        # print(torch.count_nonzero(torch.isnan(time2vector)), time2vector)\n",
    "        # print(torch.count_nonzero(torch.isnan(src.swapaxes(1,2))),src.swapaxes(1,2))\n",
    "        # src = torch.concatenate((self.t2v(src), src.swapaxes(1,2)), axis=1)\n",
    "        t2v = self.t2v(src)\n",
    "        # print(t2v.size())\n",
    "        src = torch.concatenate((t2v, src), axis=2)\n",
    "        # print(torch.isnan(src).any())\n",
    "        # print(src.size())\n",
    "        # print(src.size())\n",
    "        # output = self.transformer_encoder(src.swapaxes(1,2),self.src_mask)#, self.src_mask)\n",
    "        output = self.transformer_encoder(src.swapaxes(1,2), self.src_mask)\n",
    "        # print(output.size())\n",
    "        output = self.decoder(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# src = torch.rand(32, 5, 128)\n",
    "# out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.005 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_loader):\n",
    "#     # data, targets = get_batch(train_data, i,batch_size)\n",
    "#     data, targets = batch['inputs'], batch['targets']\n",
    "#     print(targets)\n",
    "#     print(targets[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_loss = np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, prev_loss):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # data, targets = get_batch(train_data, i,batch_size)\n",
    "        data, targets = batch['inputs'].to(device), batch['targets'].to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.size())\n",
    "        output = model(data)  \n",
    "        # print(output)\n",
    "        # if calculate_loss_over_all_values:\n",
    "        #     loss = criterion(output)\n",
    "        # else:\n",
    "        loss = criterion(output, targets)\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "        if loss < prev_loss:\n",
    "            \n",
    "            torch.save(model.state_dict(), 'tranformer_models/state_dict.pt')  # save model state (wights, etc.)\n",
    "            torch.save(model, 'tranformer_models/model_complete.pt')  # save complete model\n",
    "            prev_loss = loss\n",
    "\n",
    "    return prev_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor([1, 2, np.nan])\n",
    "# torch.isnan(x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            data, target = batch['inputs'].to(device), batch[\"targets\"].to(device).unsqueeze(1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-1:], target[-1:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    plt.plot(test_result,color=\"red\")\n",
    "    plt.plot(truth[:500],color=\"blue\")\n",
    "    plt.plot(test_result-truth,color=\"green\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    plt.close()\n",
    "    \n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, batch in enumerate(val_loader):\n",
    "#     # print(batch[\"targets\"])\n",
    "#     data, targets = batch['inputs'], batch[\"targets\"]\n",
    "#     print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_future(eval_model, data_source,steps):\n",
    "#     eval_model.eval() \n",
    "#     total_loss = 0.\n",
    "#     test_result = torch.Tensor(0)    \n",
    "#     truth = torch.Tensor(0)\n",
    "#     _ , data = get_batch(data_source, 0,1)\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0, steps,1):\n",
    "#             input = torch.clone(data[-input_window:])\n",
    "#             input[-output_window:] = 0     \n",
    "#             output = eval_model(data[-input_window:])                        \n",
    "#             data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "#     data = data.cpu().view(-1)\n",
    "    \n",
    "#     plt.plot(data,color=\"red\")       \n",
    "#     plt.plot(data[:input_window],color=\"blue\")\n",
    "#     plt.grid(True, which='both')\n",
    "#     plt.axhline(y=0, color='k')\n",
    "#     plt.savefig('graph/transformer-future%d.png'%steps)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            # print(batch[\"targets\"])\n",
    "            data, targets = batch['inputs'].to(device), batch[\"targets\"].to(device).unsqueeze(1)\n",
    "            \n",
    "            # print(data.size())\n",
    "            output = eval_model(data)\n",
    "            # print(output)\n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-1:], targets[-1:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    32/  162 batches | lr 0.005000 | 18.59 ms | loss 2.19613 | ppl     8.99\n",
      "| epoch   1 |    64/  162 batches | lr 0.005000 | 11.91 ms | loss 0.02534 | ppl     1.03\n",
      "| epoch   1 |    96/  162 batches | lr 0.005000 | 12.07 ms | loss 0.03847 | ppl     1.04\n",
      "| epoch   1 |   128/  162 batches | lr 0.005000 | 12.76 ms | loss 0.01921 | ppl     1.02\n",
      "| epoch   1 |   160/  162 batches | lr 0.005000 | 12.33 ms | loss 0.00661 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.24s | valid loss 0.04161 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    32/  162 batches | lr 0.004802 | 12.67 ms | loss 0.01490 | ppl     1.02\n",
      "| epoch   2 |    64/  162 batches | lr 0.004802 | 11.80 ms | loss 0.02406 | ppl     1.02\n",
      "| epoch   2 |    96/  162 batches | lr 0.004802 | 12.65 ms | loss 0.01103 | ppl     1.01\n",
      "| epoch   2 |   128/  162 batches | lr 0.004802 | 12.71 ms | loss 0.00709 | ppl     1.01\n",
      "| epoch   2 |   160/  162 batches | lr 0.004802 | 12.20 ms | loss 0.00558 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.05s | valid loss 0.04182 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    32/  162 batches | lr 0.004706 | 12.23 ms | loss 0.01037 | ppl     1.01\n",
      "| epoch   3 |    64/  162 batches | lr 0.004706 | 12.30 ms | loss 0.01344 | ppl     1.01\n",
      "| epoch   3 |    96/  162 batches | lr 0.004706 | 12.71 ms | loss 0.01036 | ppl     1.01\n",
      "| epoch   3 |   128/  162 batches | lr 0.004706 | 11.95 ms | loss 0.00627 | ppl     1.01\n",
      "| epoch   3 |   160/  162 batches | lr 0.004706 | 11.51 ms | loss 0.00458 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.01s | valid loss 0.04472 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    32/  162 batches | lr 0.004612 | 12.00 ms | loss 0.01034 | ppl     1.01\n",
      "| epoch   4 |    64/  162 batches | lr 0.004612 | 11.78 ms | loss 0.01308 | ppl     1.01\n",
      "| epoch   4 |    96/  162 batches | lr 0.004612 | 11.37 ms | loss 0.00966 | ppl     1.01\n",
      "| epoch   4 |   128/  162 batches | lr 0.004612 | 11.53 ms | loss 0.00564 | ppl     1.01\n",
      "| epoch   4 |   160/  162 batches | lr 0.004612 | 11.91 ms | loss 0.00441 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.94s | valid loss 0.04573 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    32/  162 batches | lr 0.004520 | 11.87 ms | loss 0.01026 | ppl     1.01\n",
      "| epoch   5 |    64/  162 batches | lr 0.004520 | 11.83 ms | loss 0.01323 | ppl     1.01\n",
      "| epoch   5 |    96/  162 batches | lr 0.004520 | 12.42 ms | loss 0.00938 | ppl     1.01\n",
      "| epoch   5 |   128/  162 batches | lr 0.004520 | 11.76 ms | loss 0.00554 | ppl     1.01\n",
      "| epoch   5 |   160/  162 batches | lr 0.004520 | 11.28 ms | loss 0.00417 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.95s | valid loss 0.04785 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    32/  162 batches | lr 0.004429 | 11.37 ms | loss 0.01026 | ppl     1.01\n",
      "| epoch   6 |    64/  162 batches | lr 0.004429 | 11.10 ms | loss 0.01381 | ppl     1.01\n",
      "| epoch   6 |    96/  162 batches | lr 0.004429 | 11.07 ms | loss 0.00960 | ppl     1.01\n",
      "| epoch   6 |   128/  162 batches | lr 0.004429 | 11.06 ms | loss 0.00605 | ppl     1.01\n",
      "| epoch   6 |   160/  162 batches | lr 0.004429 | 11.10 ms | loss 0.00428 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  1.85s | valid loss 0.05033 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    32/  162 batches | lr 0.004341 | 11.46 ms | loss 0.01023 | ppl     1.01\n",
      "| epoch   7 |    64/  162 batches | lr 0.004341 | 11.07 ms | loss 0.01280 | ppl     1.01\n",
      "| epoch   7 |    96/  162 batches | lr 0.004341 | 11.10 ms | loss 0.00872 | ppl     1.01\n",
      "| epoch   7 |   128/  162 batches | lr 0.004341 | 11.06 ms | loss 0.00574 | ppl     1.01\n",
      "| epoch   7 |   160/  162 batches | lr 0.004341 | 11.05 ms | loss 0.00428 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  1.85s | valid loss 0.04841 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    32/  162 batches | lr 0.004254 | 11.52 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch   8 |    64/  162 batches | lr 0.004254 | 11.30 ms | loss 0.01275 | ppl     1.01\n",
      "| epoch   8 |    96/  162 batches | lr 0.004254 | 11.06 ms | loss 0.00892 | ppl     1.01\n",
      "| epoch   8 |   128/  162 batches | lr 0.004254 | 11.08 ms | loss 0.00548 | ppl     1.01\n",
      "| epoch   8 |   160/  162 batches | lr 0.004254 | 11.05 ms | loss 0.00412 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  1.86s | valid loss 0.04763 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    32/  162 batches | lr 0.004169 | 11.40 ms | loss 0.01013 | ppl     1.01\n",
      "| epoch   9 |    64/  162 batches | lr 0.004169 | 11.03 ms | loss 0.01342 | ppl     1.01\n",
      "| epoch   9 |    96/  162 batches | lr 0.004169 | 11.19 ms | loss 0.00948 | ppl     1.01\n",
      "| epoch   9 |   128/  162 batches | lr 0.004169 | 11.10 ms | loss 0.00535 | ppl     1.01\n",
      "| epoch   9 |   160/  162 batches | lr 0.004169 | 11.06 ms | loss 0.00414 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  1.85s | valid loss 0.04448 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    32/  162 batches | lr 0.004085 | 11.45 ms | loss 0.00999 | ppl     1.01\n",
      "| epoch  10 |    64/  162 batches | lr 0.004085 | 11.05 ms | loss 0.01386 | ppl     1.01\n",
      "| epoch  10 |    96/  162 batches | lr 0.004085 | 11.04 ms | loss 0.00812 | ppl     1.01\n",
      "| epoch  10 |   128/  162 batches | lr 0.004085 | 11.02 ms | loss 0.00558 | ppl     1.01\n",
      "| epoch  10 |   160/  162 batches | lr 0.004085 | 11.05 ms | loss 0.00419 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  1.84s | valid loss 0.04420 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    32/  162 batches | lr 0.004004 | 11.52 ms | loss 0.00997 | ppl     1.01\n",
      "| epoch  11 |    64/  162 batches | lr 0.004004 | 11.10 ms | loss 0.01402 | ppl     1.01\n",
      "| epoch  11 |    96/  162 batches | lr 0.004004 | 11.08 ms | loss 0.00722 | ppl     1.01\n",
      "| epoch  11 |   128/  162 batches | lr 0.004004 | 11.07 ms | loss 0.00557 | ppl     1.01\n",
      "| epoch  11 |   160/  162 batches | lr 0.004004 | 11.06 ms | loss 0.00414 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  1.85s | valid loss 0.04512 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    32/  162 batches | lr 0.003924 | 11.40 ms | loss 0.01001 | ppl     1.01\n",
      "| epoch  12 |    64/  162 batches | lr 0.003924 | 11.06 ms | loss 0.01391 | ppl     1.01\n",
      "| epoch  12 |    96/  162 batches | lr 0.003924 | 11.01 ms | loss 0.00876 | ppl     1.01\n",
      "| epoch  12 |   128/  162 batches | lr 0.003924 | 11.05 ms | loss 0.00548 | ppl     1.01\n",
      "| epoch  12 |   160/  162 batches | lr 0.003924 | 11.03 ms | loss 0.00406 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  1.84s | valid loss 0.04501 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    32/  162 batches | lr 0.003845 | 11.30 ms | loss 0.01006 | ppl     1.01\n",
      "| epoch  13 |    64/  162 batches | lr 0.003845 | 10.92 ms | loss 0.01427 | ppl     1.01\n",
      "| epoch  13 |    96/  162 batches | lr 0.003845 | 11.06 ms | loss 0.00684 | ppl     1.01\n",
      "| epoch  13 |   128/  162 batches | lr 0.003845 | 10.99 ms | loss 0.00535 | ppl     1.01\n",
      "| epoch  13 |   160/  162 batches | lr 0.003845 | 11.01 ms | loss 0.00409 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  1.83s | valid loss 0.04728 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    32/  162 batches | lr 0.003768 | 11.48 ms | loss 0.01013 | ppl     1.01\n",
      "| epoch  14 |    64/  162 batches | lr 0.003768 | 11.07 ms | loss 0.01427 | ppl     1.01\n",
      "| epoch  14 |    96/  162 batches | lr 0.003768 | 10.98 ms | loss 0.00736 | ppl     1.01\n",
      "| epoch  14 |   128/  162 batches | lr 0.003768 | 11.00 ms | loss 0.00535 | ppl     1.01\n",
      "| epoch  14 |   160/  162 batches | lr 0.003768 | 11.06 ms | loss 0.00408 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  1.84s | valid loss 0.04788 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    32/  162 batches | lr 0.003693 | 11.42 ms | loss 0.01018 | ppl     1.01\n",
      "| epoch  15 |    64/  162 batches | lr 0.003693 | 11.01 ms | loss 0.01423 | ppl     1.01\n",
      "| epoch  15 |    96/  162 batches | lr 0.003693 | 11.01 ms | loss 0.00748 | ppl     1.01\n",
      "| epoch  15 |   128/  162 batches | lr 0.003693 | 11.07 ms | loss 0.00527 | ppl     1.01\n",
      "| epoch  15 |   160/  162 batches | lr 0.003693 | 11.00 ms | loss 0.00402 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  1.84s | valid loss 0.04834 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    32/  162 batches | lr 0.003619 | 11.39 ms | loss 0.01027 | ppl     1.01\n",
      "| epoch  16 |    64/  162 batches | lr 0.003619 | 11.01 ms | loss 0.01411 | ppl     1.01\n",
      "| epoch  16 |    96/  162 batches | lr 0.003619 | 10.99 ms | loss 0.00694 | ppl     1.01\n",
      "| epoch  16 |   128/  162 batches | lr 0.003619 | 11.02 ms | loss 0.00531 | ppl     1.01\n",
      "| epoch  16 |   160/  162 batches | lr 0.003619 | 10.90 ms | loss 0.00403 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  1.83s | valid loss 0.04834 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    32/  162 batches | lr 0.003547 | 11.47 ms | loss 0.01026 | ppl     1.01\n",
      "| epoch  17 |    64/  162 batches | lr 0.003547 | 11.03 ms | loss 0.01396 | ppl     1.01\n",
      "| epoch  17 |    96/  162 batches | lr 0.003547 | 11.05 ms | loss 0.00650 | ppl     1.01\n",
      "| epoch  17 |   128/  162 batches | lr 0.003547 | 10.99 ms | loss 0.00540 | ppl     1.01\n",
      "| epoch  17 |   160/  162 batches | lr 0.003547 | 11.04 ms | loss 0.00414 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  1.84s | valid loss 0.04545 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    32/  162 batches | lr 0.003476 | 11.31 ms | loss 0.00997 | ppl     1.01\n",
      "| epoch  18 |    64/  162 batches | lr 0.003476 | 11.00 ms | loss 0.01387 | ppl     1.01\n",
      "| epoch  18 |    96/  162 batches | lr 0.003476 | 11.08 ms | loss 0.00631 | ppl     1.01\n",
      "| epoch  18 |   128/  162 batches | lr 0.003476 | 11.04 ms | loss 0.00535 | ppl     1.01\n",
      "| epoch  18 |   160/  162 batches | lr 0.003476 | 10.98 ms | loss 0.00410 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  1.84s | valid loss 0.04680 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    32/  162 batches | lr 0.003406 | 11.32 ms | loss 0.01007 | ppl     1.01\n",
      "| epoch  19 |    64/  162 batches | lr 0.003406 | 11.01 ms | loss 0.01377 | ppl     1.01\n",
      "| epoch  19 |    96/  162 batches | lr 0.003406 | 11.01 ms | loss 0.00623 | ppl     1.01\n",
      "| epoch  19 |   128/  162 batches | lr 0.003406 | 11.05 ms | loss 0.00532 | ppl     1.01\n",
      "| epoch  19 |   160/  162 batches | lr 0.003406 | 11.35 ms | loss 0.00407 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  1.85s | valid loss 0.04759 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    32/  162 batches | lr 0.003338 | 11.35 ms | loss 0.01012 | ppl     1.01\n",
      "| epoch  20 |    64/  162 batches | lr 0.003338 | 11.07 ms | loss 0.01353 | ppl     1.01\n",
      "| epoch  20 |    96/  162 batches | lr 0.003338 | 11.42 ms | loss 0.00614 | ppl     1.01\n",
      "| epoch  20 |   128/  162 batches | lr 0.003338 | 13.10 ms | loss 0.00528 | ppl     1.01\n",
      "| epoch  20 |   160/  162 batches | lr 0.003338 | 13.86 ms | loss 0.00404 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  2.01s | valid loss 0.04800 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    32/  162 batches | lr 0.003271 | 11.67 ms | loss 0.01018 | ppl     1.01\n",
      "| epoch  21 |    64/  162 batches | lr 0.003271 | 11.06 ms | loss 0.01326 | ppl     1.01\n",
      "| epoch  21 |    96/  162 batches | lr 0.003271 | 11.65 ms | loss 0.00605 | ppl     1.01\n",
      "| epoch  21 |   128/  162 batches | lr 0.003271 | 10.97 ms | loss 0.00525 | ppl     1.01\n",
      "| epoch  21 |   160/  162 batches | lr 0.003271 | 10.95 ms | loss 0.00400 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  1.87s | valid loss 0.04707 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    32/  162 batches | lr 0.003206 | 11.23 ms | loss 0.01012 | ppl     1.01\n",
      "| epoch  22 |    64/  162 batches | lr 0.003206 | 11.06 ms | loss 0.01300 | ppl     1.01\n",
      "| epoch  22 |    96/  162 batches | lr 0.003206 | 14.62 ms | loss 0.00601 | ppl     1.01\n",
      "| epoch  22 |   128/  162 batches | lr 0.003206 | 14.32 ms | loss 0.00525 | ppl     1.01\n",
      "| epoch  22 |   160/  162 batches | lr 0.003206 | 12.14 ms | loss 0.00399 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  2.09s | valid loss 0.04645 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    32/  162 batches | lr 0.003142 | 11.17 ms | loss 0.01006 | ppl     1.01\n",
      "| epoch  23 |    64/  162 batches | lr 0.003142 | 10.86 ms | loss 0.01285 | ppl     1.01\n",
      "| epoch  23 |    96/  162 batches | lr 0.003142 | 10.94 ms | loss 0.00591 | ppl     1.01\n",
      "| epoch  23 |   128/  162 batches | lr 0.003142 | 11.03 ms | loss 0.00518 | ppl     1.01\n",
      "| epoch  23 |   160/  162 batches | lr 0.003142 | 10.80 ms | loss 0.00392 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  1.82s | valid loss 0.04490 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    32/  162 batches | lr 0.003079 | 11.22 ms | loss 0.00992 | ppl     1.01\n",
      "| epoch  24 |    64/  162 batches | lr 0.003079 | 10.90 ms | loss 0.01274 | ppl     1.01\n",
      "| epoch  24 |    96/  162 batches | lr 0.003079 | 10.79 ms | loss 0.00588 | ppl     1.01\n",
      "| epoch  24 |   128/  162 batches | lr 0.003079 | 10.74 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  24 |   160/  162 batches | lr 0.003079 | 10.84 ms | loss 0.00389 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  1.81s | valid loss 0.04457 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    32/  162 batches | lr 0.003017 | 11.22 ms | loss 0.00987 | ppl     1.01\n",
      "| epoch  25 |    64/  162 batches | lr 0.003017 | 10.93 ms | loss 0.01265 | ppl     1.01\n",
      "| epoch  25 |    96/  162 batches | lr 0.003017 | 10.92 ms | loss 0.00586 | ppl     1.01\n",
      "| epoch  25 |   128/  162 batches | lr 0.003017 | 10.71 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  25 |   160/  162 batches | lr 0.003017 | 10.81 ms | loss 0.00386 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  1.81s | valid loss 0.04442 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    32/  162 batches | lr 0.002957 | 14.45 ms | loss 0.00983 | ppl     1.01\n",
      "| epoch  26 |    64/  162 batches | lr 0.002957 | 11.39 ms | loss 0.01260 | ppl     1.01\n",
      "| epoch  26 |    96/  162 batches | lr 0.002957 | 11.74 ms | loss 0.00587 | ppl     1.01\n",
      "| epoch  26 |   128/  162 batches | lr 0.002957 | 11.43 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  26 |   160/  162 batches | lr 0.002957 | 11.51 ms | loss 0.00385 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  2.00s | valid loss 0.04445 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    32/  162 batches | lr 0.002898 | 11.54 ms | loss 0.00979 | ppl     1.01\n",
      "| epoch  27 |    64/  162 batches | lr 0.002898 | 11.31 ms | loss 0.01269 | ppl     1.01\n",
      "| epoch  27 |    96/  162 batches | lr 0.002898 | 11.09 ms | loss 0.00590 | ppl     1.01\n",
      "| epoch  27 |   128/  162 batches | lr 0.002898 | 11.27 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  27 |   160/  162 batches | lr 0.002898 | 11.11 ms | loss 0.00384 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  1.86s | valid loss 0.04449 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    32/  162 batches | lr 0.002840 | 11.58 ms | loss 0.00974 | ppl     1.01\n",
      "| epoch  28 |    64/  162 batches | lr 0.002840 | 11.33 ms | loss 0.01276 | ppl     1.01\n",
      "| epoch  28 |    96/  162 batches | lr 0.002840 | 11.22 ms | loss 0.00591 | ppl     1.01\n",
      "| epoch  28 |   128/  162 batches | lr 0.002840 | 11.21 ms | loss 0.00510 | ppl     1.01\n",
      "| epoch  28 |   160/  162 batches | lr 0.002840 | 11.92 ms | loss 0.00382 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  1.90s | valid loss 0.04428 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    32/  162 batches | lr 0.002783 | 11.85 ms | loss 0.00974 | ppl     1.01\n",
      "| epoch  29 |    64/  162 batches | lr 0.002783 | 11.28 ms | loss 0.01251 | ppl     1.01\n",
      "| epoch  29 |    96/  162 batches | lr 0.002783 | 11.40 ms | loss 0.00588 | ppl     1.01\n",
      "| epoch  29 |   128/  162 batches | lr 0.002783 | 11.45 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  29 |   160/  162 batches | lr 0.002783 | 11.15 ms | loss 0.00383 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  1.89s | valid loss 0.04440 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    32/  162 batches | lr 0.002727 | 12.21 ms | loss 0.00968 | ppl     1.01\n",
      "| epoch  30 |    64/  162 batches | lr 0.002727 | 11.63 ms | loss 0.01263 | ppl     1.01\n",
      "| epoch  30 |    96/  162 batches | lr 0.002727 | 11.31 ms | loss 0.00606 | ppl     1.01\n",
      "| epoch  30 |   128/  162 batches | lr 0.002727 | 11.26 ms | loss 0.00516 | ppl     1.01\n",
      "| epoch  30 |   160/  162 batches | lr 0.002727 | 11.31 ms | loss 0.00389 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  1.91s | valid loss 0.04366 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    32/  162 batches | lr 0.002673 | 11.55 ms | loss 0.00967 | ppl     1.01\n",
      "| epoch  31 |    64/  162 batches | lr 0.002673 | 11.15 ms | loss 0.01226 | ppl     1.01\n",
      "| epoch  31 |    96/  162 batches | lr 0.002673 | 11.13 ms | loss 0.00586 | ppl     1.01\n",
      "| epoch  31 |   128/  162 batches | lr 0.002673 | 11.44 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  31 |   160/  162 batches | lr 0.002673 | 12.02 ms | loss 0.00379 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  1.90s | valid loss 0.04416 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    32/  162 batches | lr 0.002619 | 12.31 ms | loss 0.00967 | ppl     1.01\n",
      "| epoch  32 |    64/  162 batches | lr 0.002619 | 11.59 ms | loss 0.01227 | ppl     1.01\n",
      "| epoch  32 |    96/  162 batches | lr 0.002619 | 11.21 ms | loss 0.00590 | ppl     1.01\n",
      "| epoch  32 |   128/  162 batches | lr 0.002619 | 11.48 ms | loss 0.00510 | ppl     1.01\n",
      "| epoch  32 |   160/  162 batches | lr 0.002619 | 11.34 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  1.93s | valid loss 0.04428 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    32/  162 batches | lr 0.002567 | 12.07 ms | loss 0.00960 | ppl     1.01\n",
      "| epoch  33 |    64/  162 batches | lr 0.002567 | 11.37 ms | loss 0.01255 | ppl     1.01\n",
      "| epoch  33 |    96/  162 batches | lr 0.002567 | 11.50 ms | loss 0.00650 | ppl     1.01\n",
      "| epoch  33 |   128/  162 batches | lr 0.002567 | 11.67 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  33 |   160/  162 batches | lr 0.002567 | 11.19 ms | loss 0.00391 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  1.91s | valid loss 0.04359 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    32/  162 batches | lr 0.002516 | 11.40 ms | loss 0.00963 | ppl     1.01\n",
      "| epoch  34 |    64/  162 batches | lr 0.002516 | 11.67 ms | loss 0.01213 | ppl     1.01\n",
      "| epoch  34 |    96/  162 batches | lr 0.002516 | 11.40 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  34 |   128/  162 batches | lr 0.002516 | 11.12 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  34 |   160/  162 batches | lr 0.002516 | 10.93 ms | loss 0.00380 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  1.87s | valid loss 0.04400 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    32/  162 batches | lr 0.002465 | 11.39 ms | loss 0.00959 | ppl     1.01\n",
      "| epoch  35 |    64/  162 batches | lr 0.002465 | 10.68 ms | loss 0.01235 | ppl     1.01\n",
      "| epoch  35 |    96/  162 batches | lr 0.002465 | 10.66 ms | loss 0.00607 | ppl     1.01\n",
      "| epoch  35 |   128/  162 batches | lr 0.002465 | 11.27 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  35 |   160/  162 batches | lr 0.002465 | 11.33 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  1.83s | valid loss 0.04380 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    32/  162 batches | lr 0.002416 | 11.45 ms | loss 0.00957 | ppl     1.01\n",
      "| epoch  36 |    64/  162 batches | lr 0.002416 | 10.83 ms | loss 0.01212 | ppl     1.01\n",
      "| epoch  36 |    96/  162 batches | lr 0.002416 | 11.11 ms | loss 0.00602 | ppl     1.01\n",
      "| epoch  36 |   128/  162 batches | lr 0.002416 | 13.68 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  36 |   160/  162 batches | lr 0.002416 | 12.19 ms | loss 0.00386 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  1.97s | valid loss 0.04344 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    32/  162 batches | lr 0.002368 | 11.44 ms | loss 0.00954 | ppl     1.01\n",
      "| epoch  37 |    64/  162 batches | lr 0.002368 | 11.09 ms | loss 0.01212 | ppl     1.01\n",
      "| epoch  37 |    96/  162 batches | lr 0.002368 | 11.13 ms | loss 0.00608 | ppl     1.01\n",
      "| epoch  37 |   128/  162 batches | lr 0.002368 | 11.39 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  37 |   160/  162 batches | lr 0.002368 | 11.36 ms | loss 0.00380 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  1.87s | valid loss 0.04376 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    32/  162 batches | lr 0.002320 | 11.61 ms | loss 0.00954 | ppl     1.01\n",
      "| epoch  38 |    64/  162 batches | lr 0.002320 | 11.22 ms | loss 0.01204 | ppl     1.01\n",
      "| epoch  38 |    96/  162 batches | lr 0.002320 | 11.20 ms | loss 0.00604 | ppl     1.01\n",
      "| epoch  38 |   128/  162 batches | lr 0.002320 | 11.94 ms | loss 0.00515 | ppl     1.01\n",
      "| epoch  38 |   160/  162 batches | lr 0.002320 | 11.46 ms | loss 0.00386 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  1.90s | valid loss 0.04334 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    32/  162 batches | lr 0.002274 | 11.99 ms | loss 0.00949 | ppl     1.01\n",
      "| epoch  39 |    64/  162 batches | lr 0.002274 | 11.00 ms | loss 0.01202 | ppl     1.01\n",
      "| epoch  39 |    96/  162 batches | lr 0.002274 | 11.23 ms | loss 0.00598 | ppl     1.01\n",
      "| epoch  39 |   128/  162 batches | lr 0.002274 | 11.08 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  39 |   160/  162 batches | lr 0.002274 | 11.02 ms | loss 0.00379 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  1.87s | valid loss 0.04364 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    32/  162 batches | lr 0.002229 | 11.34 ms | loss 0.00950 | ppl     1.01\n",
      "| epoch  40 |    64/  162 batches | lr 0.002229 | 11.05 ms | loss 0.01199 | ppl     1.01\n",
      "| epoch  40 |    96/  162 batches | lr 0.002229 | 11.03 ms | loss 0.00605 | ppl     1.01\n",
      "| epoch  40 |   128/  162 batches | lr 0.002229 | 11.86 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  40 |   160/  162 batches | lr 0.002229 | 11.88 ms | loss 0.00385 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  1.89s | valid loss 0.04339 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    32/  162 batches | lr 0.002184 | 11.48 ms | loss 0.00948 | ppl     1.01\n",
      "| epoch  41 |    64/  162 batches | lr 0.002184 | 10.95 ms | loss 0.01198 | ppl     1.01\n",
      "| epoch  41 |    96/  162 batches | lr 0.002184 | 11.53 ms | loss 0.00600 | ppl     1.01\n",
      "| epoch  41 |   128/  162 batches | lr 0.002184 | 11.18 ms | loss 0.00513 | ppl     1.01\n",
      "| epoch  41 |   160/  162 batches | lr 0.002184 | 11.03 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  1.86s | valid loss 0.04358 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    32/  162 batches | lr 0.002140 | 16.08 ms | loss 0.00948 | ppl     1.01\n",
      "| epoch  42 |    64/  162 batches | lr 0.002140 | 11.22 ms | loss 0.01195 | ppl     1.01\n",
      "| epoch  42 |    96/  162 batches | lr 0.002140 | 11.50 ms | loss 0.00595 | ppl     1.01\n",
      "| epoch  42 |   128/  162 batches | lr 0.002140 | 11.36 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  42 |   160/  162 batches | lr 0.002140 | 11.51 ms | loss 0.00380 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  2.04s | valid loss 0.04356 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    32/  162 batches | lr 0.002097 | 12.18 ms | loss 0.00946 | ppl     1.01\n",
      "| epoch  43 |    64/  162 batches | lr 0.002097 | 11.59 ms | loss 0.01190 | ppl     1.01\n",
      "| epoch  43 |    96/  162 batches | lr 0.002097 | 11.26 ms | loss 0.00596 | ppl     1.01\n",
      "| epoch  43 |   128/  162 batches | lr 0.002097 | 12.29 ms | loss 0.00513 | ppl     1.01\n",
      "| epoch  43 |   160/  162 batches | lr 0.002097 | 11.41 ms | loss 0.00382 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  1.94s | valid loss 0.04354 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    32/  162 batches | lr 0.002055 | 11.62 ms | loss 0.00946 | ppl     1.01\n",
      "| epoch  44 |    64/  162 batches | lr 0.002055 | 11.51 ms | loss 0.01186 | ppl     1.01\n",
      "| epoch  44 |    96/  162 batches | lr 0.002055 | 11.27 ms | loss 0.00595 | ppl     1.01\n",
      "| epoch  44 |   128/  162 batches | lr 0.002055 | 11.31 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  44 |   160/  162 batches | lr 0.002055 | 11.27 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  1.89s | valid loss 0.04353 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    32/  162 batches | lr 0.002014 | 11.73 ms | loss 0.00944 | ppl     1.01\n",
      "| epoch  45 |    64/  162 batches | lr 0.002014 | 11.40 ms | loss 0.01187 | ppl     1.01\n",
      "| epoch  45 |    96/  162 batches | lr 0.002014 | 11.39 ms | loss 0.00594 | ppl     1.01\n",
      "| epoch  45 |   128/  162 batches | lr 0.002014 | 11.36 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  45 |   160/  162 batches | lr 0.002014 | 11.48 ms | loss 0.00381 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  1.90s | valid loss 0.04355 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    32/  162 batches | lr 0.001974 | 11.73 ms | loss 0.00943 | ppl     1.01\n",
      "| epoch  46 |    64/  162 batches | lr 0.001974 | 13.05 ms | loss 0.01184 | ppl     1.01\n",
      "| epoch  46 |    96/  162 batches | lr 0.001974 | 12.24 ms | loss 0.00593 | ppl     1.01\n",
      "| epoch  46 |   128/  162 batches | lr 0.001974 | 10.90 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  46 |   160/  162 batches | lr 0.001974 | 10.96 ms | loss 0.00379 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  1.95s | valid loss 0.04348 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    32/  162 batches | lr 0.001935 | 11.22 ms | loss 0.00940 | ppl     1.01\n",
      "| epoch  47 |    64/  162 batches | lr 0.001935 | 10.76 ms | loss 0.01185 | ppl     1.01\n",
      "| epoch  47 |    96/  162 batches | lr 0.001935 | 10.73 ms | loss 0.00593 | ppl     1.01\n",
      "| epoch  47 |   128/  162 batches | lr 0.001935 | 10.82 ms | loss 0.00512 | ppl     1.01\n",
      "| epoch  47 |   160/  162 batches | lr 0.001935 | 11.16 ms | loss 0.00379 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  1.82s | valid loss 0.04352 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    32/  162 batches | lr 0.001896 | 11.92 ms | loss 0.00940 | ppl     1.01\n",
      "| epoch  48 |    64/  162 batches | lr 0.001896 | 11.14 ms | loss 0.01183 | ppl     1.01\n",
      "| epoch  48 |    96/  162 batches | lr 0.001896 | 10.76 ms | loss 0.00587 | ppl     1.01\n",
      "| epoch  48 |   128/  162 batches | lr 0.001896 | 10.85 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  48 |   160/  162 batches | lr 0.001896 | 11.33 ms | loss 0.00376 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  1.86s | valid loss 0.04325 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    32/  162 batches | lr 0.001858 | 11.55 ms | loss 0.00937 | ppl     1.01\n",
      "| epoch  49 |    64/  162 batches | lr 0.001858 | 11.14 ms | loss 0.01182 | ppl     1.01\n",
      "| epoch  49 |    96/  162 batches | lr 0.001858 | 10.84 ms | loss 0.00586 | ppl     1.01\n",
      "| epoch  49 |   128/  162 batches | lr 0.001858 | 10.78 ms | loss 0.00506 | ppl     1.01\n",
      "| epoch  49 |   160/  162 batches | lr 0.001858 | 11.16 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  1.84s | valid loss 0.04299 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    32/  162 batches | lr 0.001821 | 11.04 ms | loss 0.00934 | ppl     1.01\n",
      "| epoch  50 |    64/  162 batches | lr 0.001821 | 10.75 ms | loss 0.01180 | ppl     1.01\n",
      "| epoch  50 |    96/  162 batches | lr 0.001821 | 10.81 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  50 |   128/  162 batches | lr 0.001821 | 10.89 ms | loss 0.00503 | ppl     1.01\n",
      "| epoch  50 |   160/  162 batches | lr 0.001821 | 10.83 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  1.80s | valid loss 0.04283 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    32/  162 batches | lr 0.001784 | 11.13 ms | loss 0.00932 | ppl     1.01\n",
      "| epoch  51 |    64/  162 batches | lr 0.001784 | 10.79 ms | loss 0.01178 | ppl     1.01\n",
      "| epoch  51 |    96/  162 batches | lr 0.001784 | 10.91 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  51 |   128/  162 batches | lr 0.001784 | 10.80 ms | loss 0.00502 | ppl     1.01\n",
      "| epoch  51 |   160/  162 batches | lr 0.001784 | 10.97 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  1.81s | valid loss 0.04276 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    32/  162 batches | lr 0.001749 | 11.06 ms | loss 0.00931 | ppl     1.01\n",
      "| epoch  52 |    64/  162 batches | lr 0.001749 | 10.84 ms | loss 0.01177 | ppl     1.01\n",
      "| epoch  52 |    96/  162 batches | lr 0.001749 | 10.93 ms | loss 0.00584 | ppl     1.01\n",
      "| epoch  52 |   128/  162 batches | lr 0.001749 | 10.79 ms | loss 0.00502 | ppl     1.01\n",
      "| epoch  52 |   160/  162 batches | lr 0.001749 | 10.70 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  1.80s | valid loss 0.04274 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    32/  162 batches | lr 0.001714 | 11.39 ms | loss 0.00929 | ppl     1.01\n",
      "| epoch  53 |    64/  162 batches | lr 0.001714 | 11.23 ms | loss 0.01178 | ppl     1.01\n",
      "| epoch  53 |    96/  162 batches | lr 0.001714 | 10.74 ms | loss 0.00584 | ppl     1.01\n",
      "| epoch  53 |   128/  162 batches | lr 0.001714 | 10.78 ms | loss 0.00502 | ppl     1.01\n",
      "| epoch  53 |   160/  162 batches | lr 0.001714 | 10.95 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  1.82s | valid loss 0.04276 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    32/  162 batches | lr 0.001679 | 11.09 ms | loss 0.00929 | ppl     1.01\n",
      "| epoch  54 |    64/  162 batches | lr 0.001679 | 10.94 ms | loss 0.01156 | ppl     1.01\n",
      "| epoch  54 |    96/  162 batches | lr 0.001679 | 10.95 ms | loss 0.00649 | ppl     1.01\n",
      "| epoch  54 |   128/  162 batches | lr 0.001679 | 10.99 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  54 |   160/  162 batches | lr 0.001679 | 10.85 ms | loss 0.00378 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  1.82s | valid loss 0.04379 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    32/  162 batches | lr 0.001646 | 11.09 ms | loss 0.00942 | ppl     1.01\n",
      "| epoch  55 |    64/  162 batches | lr 0.001646 | 10.91 ms | loss 0.01207 | ppl     1.01\n",
      "| epoch  55 |    96/  162 batches | lr 0.001646 | 11.21 ms | loss 0.00602 | ppl     1.01\n",
      "| epoch  55 |   128/  162 batches | lr 0.001646 | 10.81 ms | loss 0.00514 | ppl     1.01\n",
      "| epoch  55 |   160/  162 batches | lr 0.001646 | 10.85 ms | loss 0.00383 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  1.82s | valid loss 0.04359 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    32/  162 batches | lr 0.001613 | 11.20 ms | loss 0.00938 | ppl     1.01\n",
      "| epoch  56 |    64/  162 batches | lr 0.001613 | 10.74 ms | loss 0.01177 | ppl     1.01\n",
      "| epoch  56 |    96/  162 batches | lr 0.001613 | 10.76 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  56 |   128/  162 batches | lr 0.001613 | 10.92 ms | loss 0.00505 | ppl     1.01\n",
      "| epoch  56 |   160/  162 batches | lr 0.001613 | 11.30 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  1.82s | valid loss 0.04285 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    32/  162 batches | lr 0.001581 | 11.06 ms | loss 0.00929 | ppl     1.01\n",
      "| epoch  57 |    64/  162 batches | lr 0.001581 | 10.89 ms | loss 0.01176 | ppl     1.01\n",
      "| epoch  57 |    96/  162 batches | lr 0.001581 | 10.96 ms | loss 0.00583 | ppl     1.01\n",
      "| epoch  57 |   128/  162 batches | lr 0.001581 | 10.89 ms | loss 0.00501 | ppl     1.01\n",
      "| epoch  57 |   160/  162 batches | lr 0.001581 | 16.74 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  2.00s | valid loss 0.04258 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    32/  162 batches | lr 0.001549 | 11.68 ms | loss 0.00926 | ppl     1.01\n",
      "| epoch  58 |    64/  162 batches | lr 0.001549 | 11.53 ms | loss 0.01172 | ppl     1.01\n",
      "| epoch  58 |    96/  162 batches | lr 0.001549 | 11.87 ms | loss 0.00582 | ppl     1.01\n",
      "| epoch  58 |   128/  162 batches | lr 0.001549 | 11.22 ms | loss 0.00499 | ppl     1.00\n",
      "| epoch  58 |   160/  162 batches | lr 0.001549 | 10.95 ms | loss 0.00374 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  1.89s | valid loss 0.04250 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    32/  162 batches | lr 0.001518 | 11.13 ms | loss 0.00924 | ppl     1.01\n",
      "| epoch  59 |    64/  162 batches | lr 0.001518 | 10.94 ms | loss 0.01171 | ppl     1.01\n",
      "| epoch  59 |    96/  162 batches | lr 0.001518 | 10.95 ms | loss 0.00581 | ppl     1.01\n",
      "| epoch  59 |   128/  162 batches | lr 0.001518 | 10.88 ms | loss 0.00498 | ppl     1.00\n",
      "| epoch  59 |   160/  162 batches | lr 0.001518 | 11.06 ms | loss 0.00372 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  1.82s | valid loss 0.04239 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    32/  162 batches | lr 0.001488 | 11.41 ms | loss 0.00914 | ppl     1.01\n",
      "| epoch  60 |    64/  162 batches | lr 0.001488 | 11.09 ms | loss 0.01047 | ppl     1.01\n",
      "| epoch  60 |    96/  162 batches | lr 0.001488 | 11.36 ms | loss 0.00656 | ppl     1.01\n",
      "| epoch  60 |   128/  162 batches | lr 0.001488 | 11.08 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  60 |   160/  162 batches | lr 0.001488 | 11.08 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  1.86s | valid loss 0.04286 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    32/  162 batches | lr 0.001458 | 11.63 ms | loss 0.00930 | ppl     1.01\n",
      "| epoch  61 |    64/  162 batches | lr 0.001458 | 11.05 ms | loss 0.01179 | ppl     1.01\n",
      "| epoch  61 |    96/  162 batches | lr 0.001458 | 11.09 ms | loss 0.00585 | ppl     1.01\n",
      "| epoch  61 |   128/  162 batches | lr 0.001458 | 11.19 ms | loss 0.00503 | ppl     1.01\n",
      "| epoch  61 |   160/  162 batches | lr 0.001458 | 11.06 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  1.86s | valid loss 0.04270 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    32/  162 batches | lr 0.001429 | 11.46 ms | loss 0.00927 | ppl     1.01\n",
      "| epoch  62 |    64/  162 batches | lr 0.001429 | 11.11 ms | loss 0.01172 | ppl     1.01\n",
      "| epoch  62 |    96/  162 batches | lr 0.001429 | 11.07 ms | loss 0.00583 | ppl     1.01\n",
      "| epoch  62 |   128/  162 batches | lr 0.001429 | 11.09 ms | loss 0.00498 | ppl     1.00\n",
      "| epoch  62 |   160/  162 batches | lr 0.001429 | 11.10 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  1.85s | valid loss 0.04248 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    32/  162 batches | lr 0.001400 | 11.39 ms | loss 0.00924 | ppl     1.01\n",
      "| epoch  63 |    64/  162 batches | lr 0.001400 | 11.12 ms | loss 0.01171 | ppl     1.01\n",
      "| epoch  63 |    96/  162 batches | lr 0.001400 | 11.07 ms | loss 0.00583 | ppl     1.01\n",
      "| epoch  63 |   128/  162 batches | lr 0.001400 | 11.07 ms | loss 0.00498 | ppl     1.00\n",
      "| epoch  63 |   160/  162 batches | lr 0.001400 | 11.06 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  1.85s | valid loss 0.04246 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    32/  162 batches | lr 0.001372 | 11.34 ms | loss 0.00923 | ppl     1.01\n",
      "| epoch  64 |    64/  162 batches | lr 0.001372 | 11.09 ms | loss 0.01170 | ppl     1.01\n",
      "| epoch  64 |    96/  162 batches | lr 0.001372 | 11.10 ms | loss 0.00583 | ppl     1.01\n",
      "| epoch  64 |   128/  162 batches | lr 0.001372 | 11.06 ms | loss 0.00497 | ppl     1.00\n",
      "| epoch  64 |   160/  162 batches | lr 0.001372 | 10.91 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  1.84s | valid loss 0.04245 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    32/  162 batches | lr 0.001345 | 11.47 ms | loss 0.00923 | ppl     1.01\n",
      "| epoch  65 |    64/  162 batches | lr 0.001345 | 11.37 ms | loss 0.01169 | ppl     1.01\n",
      "| epoch  65 |    96/  162 batches | lr 0.001345 | 11.14 ms | loss 0.00583 | ppl     1.01\n",
      "| epoch  65 |   128/  162 batches | lr 0.001345 | 11.04 ms | loss 0.00497 | ppl     1.00\n",
      "| epoch  65 |   160/  162 batches | lr 0.001345 | 11.06 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  1.86s | valid loss 0.04243 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    32/  162 batches | lr 0.001318 | 11.41 ms | loss 0.00922 | ppl     1.01\n",
      "| epoch  66 |    64/  162 batches | lr 0.001318 | 11.29 ms | loss 0.01168 | ppl     1.01\n",
      "| epoch  66 |    96/  162 batches | lr 0.001318 | 11.04 ms | loss 0.00582 | ppl     1.01\n",
      "| epoch  66 |   128/  162 batches | lr 0.001318 | 11.03 ms | loss 0.00497 | ppl     1.00\n",
      "| epoch  66 |   160/  162 batches | lr 0.001318 | 11.04 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  1.85s | valid loss 0.04242 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    32/  162 batches | lr 0.001292 | 11.42 ms | loss 0.00922 | ppl     1.01\n",
      "| epoch  67 |    64/  162 batches | lr 0.001292 | 11.11 ms | loss 0.01168 | ppl     1.01\n",
      "| epoch  67 |    96/  162 batches | lr 0.001292 | 11.08 ms | loss 0.00582 | ppl     1.01\n",
      "| epoch  67 |   128/  162 batches | lr 0.001292 | 11.01 ms | loss 0.00497 | ppl     1.00\n",
      "| epoch  67 |   160/  162 batches | lr 0.001292 | 11.09 ms | loss 0.00375 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  1.85s | valid loss 0.04241 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    32/  162 batches | lr 0.001266 | 11.63 ms | loss 0.00921 | ppl     1.01\n",
      "| epoch  68 |    64/  162 batches | lr 0.001266 | 11.05 ms | loss 0.01167 | ppl     1.01\n",
      "| epoch  68 |    96/  162 batches | lr 0.001266 | 10.93 ms | loss 0.00582 | ppl     1.01\n",
      "| epoch  68 |   128/  162 batches | lr 0.001266 | 11.05 ms | loss 0.00496 | ppl     1.00\n",
      "| epoch  68 |   160/  162 batches | lr 0.001266 | 11.08 ms | loss 0.00374 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  1.85s | valid loss 0.04238 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    32/  162 batches | lr 0.001240 | 12.65 ms | loss 0.00920 | ppl     1.01\n",
      "| epoch  69 |    64/  162 batches | lr 0.001240 | 10.97 ms | loss 0.01167 | ppl     1.01\n",
      "| epoch  69 |    96/  162 batches | lr 0.001240 | 10.98 ms | loss 0.00581 | ppl     1.01\n",
      "| epoch  69 |   128/  162 batches | lr 0.001240 | 11.10 ms | loss 0.00496 | ppl     1.00\n",
      "| epoch  69 |   160/  162 batches | lr 0.001240 | 11.09 ms | loss 0.00373 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  1.88s | valid loss 0.04234 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    32/  162 batches | lr 0.001216 | 11.55 ms | loss 0.00917 | ppl     1.01\n",
      "| epoch  70 |    64/  162 batches | lr 0.001216 | 11.05 ms | loss 0.01116 | ppl     1.01\n",
      "| epoch  70 |    96/  162 batches | lr 0.001216 | 11.10 ms | loss 0.00598 | ppl     1.01\n",
      "| epoch  70 |   128/  162 batches | lr 0.001216 | 11.10 ms | loss 0.00511 | ppl     1.01\n",
      "| epoch  70 |   160/  162 batches | lr 0.001216 | 11.13 ms | loss 0.00379 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  1.85s | valid loss 0.04368 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |    32/  162 batches | lr 0.001191 | 11.40 ms | loss 0.00938 | ppl     1.01\n",
      "| epoch  71 |    64/  162 batches | lr 0.001191 | 11.07 ms | loss 0.01184 | ppl     1.01\n",
      "| epoch  71 |    96/  162 batches | lr 0.001191 | 11.12 ms | loss 0.00588 | ppl     1.01\n",
      "| epoch  71 |   128/  162 batches | lr 0.001191 | 11.08 ms | loss 0.00504 | ppl     1.01\n",
      "| epoch  71 |   160/  162 batches | lr 0.001191 | 11.15 ms | loss 0.00374 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  1.85s | valid loss 0.04290 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    32/  162 batches | lr 0.001167 | 11.43 ms | loss 0.00925 | ppl     1.01\n",
      "| epoch  72 |    64/  162 batches | lr 0.001167 | 11.09 ms | loss 0.01168 | ppl     1.01\n",
      "| epoch  72 |    96/  162 batches | lr 0.001167 | 11.00 ms | loss 0.00579 | ppl     1.01\n",
      "| epoch  72 |   128/  162 batches | lr 0.001167 | 11.16 ms | loss 0.00496 | ppl     1.00\n",
      "| epoch  72 |   160/  162 batches | lr 0.001167 | 11.03 ms | loss 0.00370 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  1.85s | valid loss 0.04212 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |    32/  162 batches | lr 0.001144 | 11.46 ms | loss 0.00909 | ppl     1.01\n",
      "| epoch  73 |    64/  162 batches | lr 0.001144 | 11.05 ms | loss 0.01155 | ppl     1.01\n",
      "| epoch  73 |    96/  162 batches | lr 0.001144 | 11.03 ms | loss 0.00581 | ppl     1.01\n",
      "| epoch  73 |   128/  162 batches | lr 0.001144 | 11.18 ms | loss 0.00500 | ppl     1.01\n",
      "| epoch  73 |   160/  162 batches | lr 0.001144 | 11.18 ms | loss 0.00373 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  1.85s | valid loss 0.04225 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    32/  162 batches | lr 0.001121 | 11.37 ms | loss 0.00916 | ppl     1.01\n",
      "| epoch  74 |    64/  162 batches | lr 0.001121 | 11.10 ms | loss 0.01149 | ppl     1.01\n",
      "| epoch  74 |    96/  162 batches | lr 0.001121 | 10.98 ms | loss 0.00549 | ppl     1.01\n",
      "| epoch  74 |   128/  162 batches | lr 0.001121 | 11.41 ms | loss 0.00434 | ppl     1.00\n",
      "| epoch  74 |   160/  162 batches | lr 0.001121 | 11.23 ms | loss 0.00324 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  1.86s | valid loss 0.03776 | valid ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    32/  162 batches | lr 0.001099 | 11.58 ms | loss 0.00633 | ppl     1.01\n",
      "| epoch  75 |    64/  162 batches | lr 0.001099 | 11.06 ms | loss 0.00701 | ppl     1.01\n",
      "| epoch  75 |    96/  162 batches | lr 0.001099 | 11.00 ms | loss 0.00420 | ppl     1.00\n",
      "| epoch  75 |   128/  162 batches | lr 0.001099 | 11.07 ms | loss 0.00325 | ppl     1.00\n",
      "| epoch  75 |   160/  162 batches | lr 0.001099 | 11.07 ms | loss 0.00193 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time:  1.85s | valid loss 0.02237 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    32/  162 batches | lr 0.001077 | 11.43 ms | loss 0.00496 | ppl     1.00\n",
      "| epoch  76 |    64/  162 batches | lr 0.001077 | 11.13 ms | loss 0.00525 | ppl     1.01\n",
      "| epoch  76 |    96/  162 batches | lr 0.001077 | 11.07 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  76 |   128/  162 batches | lr 0.001077 | 11.28 ms | loss 0.00298 | ppl     1.00\n",
      "| epoch  76 |   160/  162 batches | lr 0.001077 | 10.98 ms | loss 0.00180 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  1.85s | valid loss 0.02174 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    32/  162 batches | lr 0.001055 | 11.42 ms | loss 0.00427 | ppl     1.00\n",
      "| epoch  77 |    64/  162 batches | lr 0.001055 | 11.08 ms | loss 0.00471 | ppl     1.00\n",
      "| epoch  77 |    96/  162 batches | lr 0.001055 | 11.06 ms | loss 0.00307 | ppl     1.00\n",
      "| epoch  77 |   128/  162 batches | lr 0.001055 | 10.97 ms | loss 0.00262 | ppl     1.00\n",
      "| epoch  77 |   160/  162 batches | lr 0.001055 | 11.39 ms | loss 0.00166 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  1.85s | valid loss 0.01968 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    32/  162 batches | lr 0.001034 | 11.45 ms | loss 0.00373 | ppl     1.00\n",
      "| epoch  78 |    64/  162 batches | lr 0.001034 | 11.05 ms | loss 0.00418 | ppl     1.00\n",
      "| epoch  78 |    96/  162 batches | lr 0.001034 | 11.09 ms | loss 0.00251 | ppl     1.00\n",
      "| epoch  78 |   128/  162 batches | lr 0.001034 | 11.16 ms | loss 0.00230 | ppl     1.00\n",
      "| epoch  78 |   160/  162 batches | lr 0.001034 | 11.94 ms | loss 0.00159 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  1.88s | valid loss 0.02019 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    32/  162 batches | lr 0.001014 | 11.37 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  79 |    64/  162 batches | lr 0.001014 | 11.09 ms | loss 0.00407 | ppl     1.00\n",
      "| epoch  79 |    96/  162 batches | lr 0.001014 | 11.12 ms | loss 0.00268 | ppl     1.00\n",
      "| epoch  79 |   128/  162 batches | lr 0.001014 | 11.07 ms | loss 0.00240 | ppl     1.00\n",
      "| epoch  79 |   160/  162 batches | lr 0.001014 | 11.07 ms | loss 0.00146 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  1.85s | valid loss 0.01637 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    32/  162 batches | lr 0.000993 | 11.36 ms | loss 0.00341 | ppl     1.00\n",
      "| epoch  80 |    64/  162 batches | lr 0.000993 | 11.15 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  80 |    96/  162 batches | lr 0.000993 | 11.05 ms | loss 0.00252 | ppl     1.00\n",
      "| epoch  80 |   128/  162 batches | lr 0.000993 | 11.04 ms | loss 0.00220 | ppl     1.00\n",
      "| epoch  80 |   160/  162 batches | lr 0.000993 | 11.06 ms | loss 0.00136 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time:  1.84s | valid loss 0.01497 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    32/  162 batches | lr 0.000973 | 11.27 ms | loss 0.00318 | ppl     1.00\n",
      "| epoch  81 |    64/  162 batches | lr 0.000973 | 11.30 ms | loss 0.00358 | ppl     1.00\n",
      "| epoch  81 |    96/  162 batches | lr 0.000973 | 11.10 ms | loss 0.00240 | ppl     1.00\n",
      "| epoch  81 |   128/  162 batches | lr 0.000973 | 11.04 ms | loss 0.00201 | ppl     1.00\n",
      "| epoch  81 |   160/  162 batches | lr 0.000973 | 11.45 ms | loss 0.00129 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  1.86s | valid loss 0.01519 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    32/  162 batches | lr 0.000954 | 11.34 ms | loss 0.00300 | ppl     1.00\n",
      "| epoch  82 |    64/  162 batches | lr 0.000954 | 11.11 ms | loss 0.00343 | ppl     1.00\n",
      "| epoch  82 |    96/  162 batches | lr 0.000954 | 11.12 ms | loss 0.00227 | ppl     1.00\n",
      "| epoch  82 |   128/  162 batches | lr 0.000954 | 11.16 ms | loss 0.00193 | ppl     1.00\n",
      "| epoch  82 |   160/  162 batches | lr 0.000954 | 11.08 ms | loss 0.00124 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  1.85s | valid loss 0.01503 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    32/  162 batches | lr 0.000935 | 11.38 ms | loss 0.00287 | ppl     1.00\n",
      "| epoch  83 |    64/  162 batches | lr 0.000935 | 11.07 ms | loss 0.00324 | ppl     1.00\n",
      "| epoch  83 |    96/  162 batches | lr 0.000935 | 11.10 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch  83 |   128/  162 batches | lr 0.000935 | 11.09 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch  83 |   160/  162 batches | lr 0.000935 | 11.46 ms | loss 0.00120 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  1.86s | valid loss 0.01473 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    32/  162 batches | lr 0.000916 | 11.38 ms | loss 0.00275 | ppl     1.00\n",
      "| epoch  84 |    64/  162 batches | lr 0.000916 | 11.02 ms | loss 0.00320 | ppl     1.00\n",
      "| epoch  84 |    96/  162 batches | lr 0.000916 | 11.07 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch  84 |   128/  162 batches | lr 0.000916 | 11.03 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch  84 |   160/  162 batches | lr 0.000916 | 11.08 ms | loss 0.00117 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  1.84s | valid loss 0.01491 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |    32/  162 batches | lr 0.000898 | 11.41 ms | loss 0.00262 | ppl     1.00\n",
      "| epoch  85 |    64/  162 batches | lr 0.000898 | 11.22 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  85 |    96/  162 batches | lr 0.000898 | 11.06 ms | loss 0.00195 | ppl     1.00\n",
      "| epoch  85 |   128/  162 batches | lr 0.000898 | 10.99 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch  85 |   160/  162 batches | lr 0.000898 | 11.08 ms | loss 0.00119 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time:  1.85s | valid loss 0.01380 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    32/  162 batches | lr 0.000880 | 11.34 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch  86 |    64/  162 batches | lr 0.000880 | 11.11 ms | loss 0.00301 | ppl     1.00\n",
      "| epoch  86 |    96/  162 batches | lr 0.000880 | 11.10 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch  86 |   128/  162 batches | lr 0.000880 | 11.08 ms | loss 0.00169 | ppl     1.00\n",
      "| epoch  86 |   160/  162 batches | lr 0.000880 | 11.49 ms | loss 0.00112 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  1.86s | valid loss 0.01457 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    32/  162 batches | lr 0.000862 | 11.43 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch  87 |    64/  162 batches | lr 0.000862 | 11.36 ms | loss 0.00298 | ppl     1.00\n",
      "| epoch  87 |    96/  162 batches | lr 0.000862 | 11.17 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch  87 |   128/  162 batches | lr 0.000862 | 11.12 ms | loss 0.00166 | ppl     1.00\n",
      "| epoch  87 |   160/  162 batches | lr 0.000862 | 11.44 ms | loss 0.00108 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  1.87s | valid loss 0.01374 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    32/  162 batches | lr 0.000845 | 11.49 ms | loss 0.00245 | ppl     1.00\n",
      "| epoch  88 |    64/  162 batches | lr 0.000845 | 11.33 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  88 |    96/  162 batches | lr 0.000845 | 11.17 ms | loss 0.00174 | ppl     1.00\n",
      "| epoch  88 |   128/  162 batches | lr 0.000845 | 11.08 ms | loss 0.00163 | ppl     1.00\n",
      "| epoch  88 |   160/  162 batches | lr 0.000845 | 11.05 ms | loss 0.00107 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  1.86s | valid loss 0.01464 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    32/  162 batches | lr 0.000828 | 11.64 ms | loss 0.00240 | ppl     1.00\n",
      "| epoch  89 |    64/  162 batches | lr 0.000828 | 11.25 ms | loss 0.00287 | ppl     1.00\n",
      "| epoch  89 |    96/  162 batches | lr 0.000828 | 11.35 ms | loss 0.00171 | ppl     1.00\n",
      "| epoch  89 |   128/  162 batches | lr 0.000828 | 11.33 ms | loss 0.00162 | ppl     1.00\n",
      "| epoch  89 |   160/  162 batches | lr 0.000828 | 11.54 ms | loss 0.00105 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  1.89s | valid loss 0.01328 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    32/  162 batches | lr 0.000812 | 11.44 ms | loss 0.00237 | ppl     1.00\n",
      "| epoch  90 |    64/  162 batches | lr 0.000812 | 11.22 ms | loss 0.00279 | ppl     1.00\n",
      "| epoch  90 |    96/  162 batches | lr 0.000812 | 11.40 ms | loss 0.00165 | ppl     1.00\n",
      "| epoch  90 |   128/  162 batches | lr 0.000812 | 11.31 ms | loss 0.00159 | ppl     1.00\n",
      "| epoch  90 |   160/  162 batches | lr 0.000812 | 11.03 ms | loss 0.00103 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time:  1.87s | valid loss 0.01429 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    32/  162 batches | lr 0.000795 | 11.32 ms | loss 0.00233 | ppl     1.00\n",
      "| epoch  91 |    64/  162 batches | lr 0.000795 | 11.02 ms | loss 0.00279 | ppl     1.00\n",
      "| epoch  91 |    96/  162 batches | lr 0.000795 | 11.03 ms | loss 0.00163 | ppl     1.00\n",
      "| epoch  91 |   128/  162 batches | lr 0.000795 | 11.01 ms | loss 0.00157 | ppl     1.00\n",
      "| epoch  91 |   160/  162 batches | lr 0.000795 | 11.00 ms | loss 0.00103 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  1.83s | valid loss 0.01283 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    32/  162 batches | lr 0.000779 | 11.41 ms | loss 0.00230 | ppl     1.00\n",
      "| epoch  92 |    64/  162 batches | lr 0.000779 | 11.03 ms | loss 0.00269 | ppl     1.00\n",
      "| epoch  92 |    96/  162 batches | lr 0.000779 | 11.09 ms | loss 0.00159 | ppl     1.00\n",
      "| epoch  92 |   128/  162 batches | lr 0.000779 | 11.25 ms | loss 0.00154 | ppl     1.00\n",
      "| epoch  92 |   160/  162 batches | lr 0.000779 | 11.04 ms | loss 0.00102 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  1.85s | valid loss 0.01398 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    32/  162 batches | lr 0.000764 | 11.40 ms | loss 0.00226 | ppl     1.00\n",
      "| epoch  93 |    64/  162 batches | lr 0.000764 | 11.10 ms | loss 0.00269 | ppl     1.00\n",
      "| epoch  93 |    96/  162 batches | lr 0.000764 | 11.04 ms | loss 0.00158 | ppl     1.00\n",
      "| epoch  93 |   128/  162 batches | lr 0.000764 | 11.02 ms | loss 0.00153 | ppl     1.00\n",
      "| epoch  93 |   160/  162 batches | lr 0.000764 | 11.09 ms | loss 0.00100 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  1.84s | valid loss 0.01293 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    32/  162 batches | lr 0.000749 | 11.40 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch  94 |    64/  162 batches | lr 0.000749 | 11.00 ms | loss 0.00260 | ppl     1.00\n",
      "| epoch  94 |    96/  162 batches | lr 0.000749 | 11.04 ms | loss 0.00153 | ppl     1.00\n",
      "| epoch  94 |   128/  162 batches | lr 0.000749 | 12.11 ms | loss 0.00151 | ppl     1.00\n",
      "| epoch  94 |   160/  162 batches | lr 0.000749 | 11.13 ms | loss 0.00099 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  1.88s | valid loss 0.01367 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    32/  162 batches | lr 0.000734 | 11.36 ms | loss 0.00221 | ppl     1.00\n",
      "| epoch  95 |    64/  162 batches | lr 0.000734 | 11.05 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch  95 |    96/  162 batches | lr 0.000734 | 11.03 ms | loss 0.00154 | ppl     1.00\n",
      "| epoch  95 |   128/  162 batches | lr 0.000734 | 11.09 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  95 |   160/  162 batches | lr 0.000734 | 11.55 ms | loss 0.00098 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time:  1.86s | valid loss 0.01309 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    32/  162 batches | lr 0.000719 | 12.46 ms | loss 0.00220 | ppl     1.00\n",
      "| epoch  96 |    64/  162 batches | lr 0.000719 | 11.57 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch  96 |    96/  162 batches | lr 0.000719 | 11.74 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  96 |   128/  162 batches | lr 0.000719 | 11.82 ms | loss 0.00148 | ppl     1.00\n",
      "| epoch  96 |   160/  162 batches | lr 0.000719 | 11.60 ms | loss 0.00097 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  1.96s | valid loss 0.01369 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |    32/  162 batches | lr 0.000705 | 11.63 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch  97 |    64/  162 batches | lr 0.000705 | 11.27 ms | loss 0.00254 | ppl     1.00\n",
      "| epoch  97 |    96/  162 batches | lr 0.000705 | 11.58 ms | loss 0.00150 | ppl     1.00\n",
      "| epoch  97 |   128/  162 batches | lr 0.000705 | 11.57 ms | loss 0.00146 | ppl     1.00\n",
      "| epoch  97 |   160/  162 batches | lr 0.000705 | 11.29 ms | loss 0.00097 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  1.90s | valid loss 0.01323 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    32/  162 batches | lr 0.000690 | 11.34 ms | loss 0.00215 | ppl     1.00\n",
      "| epoch  98 |    64/  162 batches | lr 0.000690 | 10.99 ms | loss 0.00248 | ppl     1.00\n",
      "| epoch  98 |    96/  162 batches | lr 0.000690 | 11.13 ms | loss 0.00145 | ppl     1.00\n",
      "| epoch  98 |   128/  162 batches | lr 0.000690 | 11.13 ms | loss 0.00145 | ppl     1.00\n",
      "| epoch  98 |   160/  162 batches | lr 0.000690 | 11.51 ms | loss 0.00096 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  1.86s | valid loss 0.01364 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    32/  162 batches | lr 0.000677 | 11.88 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch  99 |    64/  162 batches | lr 0.000677 | 11.71 ms | loss 0.00247 | ppl     1.00\n",
      "| epoch  99 |    96/  162 batches | lr 0.000677 | 11.96 ms | loss 0.00147 | ppl     1.00\n",
      "| epoch  99 |   128/  162 batches | lr 0.000677 | 11.73 ms | loss 0.00144 | ppl     1.00\n",
      "| epoch  99 |   160/  162 batches | lr 0.000677 | 11.91 ms | loss 0.00095 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  1.96s | valid loss 0.01326 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    32/  162 batches | lr 0.000663 | 11.87 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 100 |    64/  162 batches | lr 0.000663 | 11.86 ms | loss 0.00244 | ppl     1.00\n",
      "| epoch 100 |    96/  162 batches | lr 0.000663 | 11.61 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch 100 |   128/  162 batches | lr 0.000663 | 11.82 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch 100 |   160/  162 batches | lr 0.000663 | 11.44 ms | loss 0.00095 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  1.94s | valid loss 0.01394 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train_data, val_data = get_data()\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# lr = 0.005 \n",
    "# #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "prev_loss = np.Inf\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    prev_loss = train_model(train_data, prev_loss)\n",
    "    \n",
    "    \n",
    "    # if(epoch % 10 == 0):\n",
    "        #val_loss = plot_and_loss(model, val_data,epoch)\n",
    "\n",
    "        # predict_future(model, val_data,200)\n",
    "        \n",
    "    # else:\n",
    "    val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tranformer_models/state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransAm(\n",
       "  (t2v): Time2Vector()\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.0, inplace=False)\n",
       "    (dropout2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4971],\n",
      "        [0.4846],\n",
      "        [0.4814],\n",
      "        [0.4780],\n",
      "        [0.4632],\n",
      "        [0.4597],\n",
      "        [0.4582],\n",
      "        [0.4630],\n",
      "        [0.4663],\n",
      "        [0.4679],\n",
      "        [0.4760],\n",
      "        [0.4728],\n",
      "        [0.4891],\n",
      "        [0.5003],\n",
      "        [0.5085],\n",
      "        [0.5031],\n",
      "        [0.4985],\n",
      "        [0.4871],\n",
      "        [0.4763],\n",
      "        [0.4510],\n",
      "        [0.4519],\n",
      "        [0.4574],\n",
      "        [0.4595],\n",
      "        [0.4621],\n",
      "        [0.4651],\n",
      "        [0.4703],\n",
      "        [0.4720],\n",
      "        [0.4751],\n",
      "        [0.4762],\n",
      "        [0.4781],\n",
      "        [0.4829],\n",
      "        [0.4799]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4765],\n",
      "        [0.4814],\n",
      "        [0.4827],\n",
      "        [0.4826],\n",
      "        [0.4875],\n",
      "        [0.4973],\n",
      "        [0.5079],\n",
      "        [0.5033],\n",
      "        [0.5004],\n",
      "        [0.4991],\n",
      "        [0.4967],\n",
      "        [0.4898],\n",
      "        [0.4919],\n",
      "        [0.4961],\n",
      "        [0.4952],\n",
      "        [0.4852],\n",
      "        [0.4769],\n",
      "        [0.4772],\n",
      "        [0.4703],\n",
      "        [0.4683],\n",
      "        [0.4712],\n",
      "        [0.4736],\n",
      "        [0.4914],\n",
      "        [0.4988],\n",
      "        [0.5059],\n",
      "        [0.5226],\n",
      "        [0.5291],\n",
      "        [0.5567],\n",
      "        [0.5754],\n",
      "        [0.5696],\n",
      "        [0.5612],\n",
      "        [0.5452]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5144],\n",
      "        [0.4972],\n",
      "        [0.4813],\n",
      "        [0.4698],\n",
      "        [0.4674],\n",
      "        [0.4524],\n",
      "        [0.4521],\n",
      "        [0.4623],\n",
      "        [0.4742],\n",
      "        [0.4799],\n",
      "        [0.4805],\n",
      "        [0.4847],\n",
      "        [0.4870],\n",
      "        [0.4836],\n",
      "        [0.4710],\n",
      "        [0.4742],\n",
      "        [0.4668],\n",
      "        [0.4577],\n",
      "        [0.4532],\n",
      "        [0.4370],\n",
      "        [0.4410],\n",
      "        [0.4504],\n",
      "        [0.4900],\n",
      "        [0.5072],\n",
      "        [0.5161],\n",
      "        [0.5291],\n",
      "        [0.5387],\n",
      "        [0.5448],\n",
      "        [0.5457],\n",
      "        [0.5448],\n",
      "        [0.5391],\n",
      "        [0.5203]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4974],\n",
      "        [0.4807],\n",
      "        [0.4704],\n",
      "        [0.4612],\n",
      "        [0.4610],\n",
      "        [0.4630],\n",
      "        [0.4592],\n",
      "        [0.4376],\n",
      "        [0.4324],\n",
      "        [0.4396],\n",
      "        [0.4600],\n",
      "        [0.4646],\n",
      "        [0.4674],\n",
      "        [0.4732],\n",
      "        [0.4430],\n",
      "        [0.4382],\n",
      "        [0.4365],\n",
      "        [0.4261],\n",
      "        [0.4245],\n",
      "        [0.4366],\n",
      "        [0.4327],\n",
      "        [0.4327],\n",
      "        [0.4500],\n",
      "        [0.4501],\n",
      "        [0.4523],\n",
      "        [0.4456],\n",
      "        [0.4274],\n",
      "        [0.4004],\n",
      "        [0.3965],\n",
      "        [0.3952],\n",
      "        [0.3878],\n",
      "        [0.3792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3818],\n",
      "        [0.3881],\n",
      "        [0.4111],\n",
      "        [0.4634],\n",
      "        [0.4958],\n",
      "        [0.5204],\n",
      "        [0.5257],\n",
      "        [0.5194],\n",
      "        [0.5091],\n",
      "        [0.5150],\n",
      "        [0.5144],\n",
      "        [0.5071],\n",
      "        [0.4851],\n",
      "        [0.4731],\n",
      "        [0.4752],\n",
      "        [0.4738],\n",
      "        [0.4726],\n",
      "        [0.4678],\n",
      "        [0.4668],\n",
      "        [0.4547],\n",
      "        [0.4391],\n",
      "        [0.4356],\n",
      "        [0.4334],\n",
      "        [0.4742],\n",
      "        [0.4889],\n",
      "        [0.5013],\n",
      "        [0.5135],\n",
      "        [0.5361],\n",
      "        [0.5420],\n",
      "        [0.5436],\n",
      "        [0.5523],\n",
      "        [0.5477]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5269],\n",
      "        [0.4983],\n",
      "        [0.4839],\n",
      "        [0.4705],\n",
      "        [0.4513],\n",
      "        [0.4448],\n",
      "        [0.4521],\n",
      "        [0.4577],\n",
      "        [0.4586],\n",
      "        [0.4619],\n",
      "        [0.4608],\n",
      "        [0.4586],\n",
      "        [0.4647],\n",
      "        [0.4696],\n",
      "        [0.4797],\n",
      "        [0.4863],\n",
      "        [0.4720],\n",
      "        [0.4663],\n",
      "        [0.4603],\n",
      "        [0.4463],\n",
      "        [0.4175],\n",
      "        [0.4064],\n",
      "        [0.3920],\n",
      "        [0.3933],\n",
      "        [0.3919],\n",
      "        [0.3920],\n",
      "        [0.3927],\n",
      "        [0.4052],\n",
      "        [0.4231],\n",
      "        [0.4344],\n",
      "        [0.4444],\n",
      "        [0.4771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5017],\n",
      "        [0.5090],\n",
      "        [0.5150],\n",
      "        [0.5188],\n",
      "        [0.5272],\n",
      "        [0.5203],\n",
      "        [0.4976],\n",
      "        [0.4738],\n",
      "        [0.4725],\n",
      "        [0.4465],\n",
      "        [0.4247],\n",
      "        [0.4076],\n",
      "        [0.3914],\n",
      "        [0.3896],\n",
      "        [0.4016],\n",
      "        [0.4330],\n",
      "        [0.4739],\n",
      "        [0.4901],\n",
      "        [0.5043],\n",
      "        [0.5209],\n",
      "        [0.5283],\n",
      "        [0.5282],\n",
      "        [0.5271],\n",
      "        [0.5058],\n",
      "        [0.4868],\n",
      "        [0.4627],\n",
      "        [0.4369],\n",
      "        [0.4241],\n",
      "        [0.3992],\n",
      "        [0.3728],\n",
      "        [0.3533],\n",
      "        [0.3423]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3440],\n",
      "        [0.3471],\n",
      "        [0.3697],\n",
      "        [0.3921],\n",
      "        [0.4282],\n",
      "        [0.4296],\n",
      "        [0.4508],\n",
      "        [0.4714],\n",
      "        [0.4829],\n",
      "        [0.4843],\n",
      "        [0.4818],\n",
      "        [0.4814],\n",
      "        [0.4364],\n",
      "        [0.4032],\n",
      "        [0.3885],\n",
      "        [0.3659],\n",
      "        [0.3348],\n",
      "        [0.3489],\n",
      "        [0.3566],\n",
      "        [0.3828],\n",
      "        [0.4021],\n",
      "        [0.4266],\n",
      "        [0.4790],\n",
      "        [0.5200],\n",
      "        [0.6402],\n",
      "        [0.6827],\n",
      "        [0.6876],\n",
      "        [0.6763],\n",
      "        [0.6571],\n",
      "        [0.6280],\n",
      "        [0.5812],\n",
      "        [0.5472]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5028],\n",
      "        [0.4567],\n",
      "        [0.3782],\n",
      "        [0.3483],\n",
      "        [0.3346],\n",
      "        [0.3551],\n",
      "        [0.3720],\n",
      "        [0.3766],\n",
      "        [0.3834],\n",
      "        [0.3801],\n",
      "        [0.3974],\n",
      "        [0.4261],\n",
      "        [0.4245],\n",
      "        [0.4221],\n",
      "        [0.4165],\n",
      "        [0.4280],\n",
      "        [0.4480],\n",
      "        [0.4595],\n",
      "        [0.4854],\n",
      "        [0.5063],\n",
      "        [0.5069],\n",
      "        [0.4799],\n",
      "        [0.4881],\n",
      "        [0.4816],\n",
      "        [0.4737],\n",
      "        [0.4322],\n",
      "        [0.4015],\n",
      "        [0.3900],\n",
      "        [0.3936],\n",
      "        [0.3989],\n",
      "        [0.4199],\n",
      "        [0.4424]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4438],\n",
      "        [0.4539],\n",
      "        [0.4802],\n",
      "        [0.5165],\n",
      "        [0.5466],\n",
      "        [0.5422],\n",
      "        [0.5389],\n",
      "        [0.5382],\n",
      "        [0.5342],\n",
      "        [0.5352],\n",
      "        [0.5533],\n",
      "        [0.5797],\n",
      "        [0.5690],\n",
      "        [0.5546],\n",
      "        [0.5399],\n",
      "        [0.5369],\n",
      "        [0.5295],\n",
      "        [0.5161],\n",
      "        [0.4924],\n",
      "        [0.4779],\n",
      "        [0.4657],\n",
      "        [0.4635],\n",
      "        [0.4687],\n",
      "        [0.4557],\n",
      "        [0.4382],\n",
      "        [0.4068],\n",
      "        [0.3986],\n",
      "        [0.4007],\n",
      "        [0.3760],\n",
      "        [0.3664],\n",
      "        [0.3573],\n",
      "        [0.3515]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3503],\n",
      "        [0.3610],\n",
      "        [0.3747],\n",
      "        [0.3859],\n",
      "        [0.4003],\n",
      "        [0.4164],\n",
      "        [0.4427],\n",
      "        [0.4394],\n",
      "        [0.4472],\n",
      "        [0.4438],\n",
      "        [0.4373],\n",
      "        [0.4293],\n",
      "        [0.3945],\n",
      "        [0.3366],\n",
      "        [0.3148],\n",
      "        [0.2960],\n",
      "        [0.3033],\n",
      "        [0.3281],\n",
      "        [0.3730],\n",
      "        [0.4045],\n",
      "        [0.4421],\n",
      "        [0.4318],\n",
      "        [0.4279],\n",
      "        [0.4304],\n",
      "        [0.4488],\n",
      "        [0.4610],\n",
      "        [0.4591],\n",
      "        [0.4714],\n",
      "        [0.4690],\n",
      "        [0.4508],\n",
      "        [0.4524],\n",
      "        [0.4532]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4669],\n",
      "        [0.4572],\n",
      "        [0.4532],\n",
      "        [0.4376],\n",
      "        [0.4251],\n",
      "        [0.4279],\n",
      "        [0.4080],\n",
      "        [0.3820],\n",
      "        [0.3651],\n",
      "        [0.3841],\n",
      "        [0.3813],\n",
      "        [0.4092],\n",
      "        [0.4405],\n",
      "        [0.4847],\n",
      "        [0.5285],\n",
      "        [0.5666],\n",
      "        [0.5947],\n",
      "        [0.6216],\n",
      "        [0.6339],\n",
      "        [0.5853],\n",
      "        [0.5569],\n",
      "        [0.5421],\n",
      "        [0.5205],\n",
      "        [0.5060],\n",
      "        [0.4581],\n",
      "        [0.4275],\n",
      "        [0.3923],\n",
      "        [0.3589],\n",
      "        [0.3466],\n",
      "        [0.3565],\n",
      "        [0.3336],\n",
      "        [0.3344]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3410],\n",
      "        [0.3570],\n",
      "        [0.3877],\n",
      "        [0.4395],\n",
      "        [0.4779],\n",
      "        [0.5308],\n",
      "        [0.6031],\n",
      "        [0.6346],\n",
      "        [0.6677],\n",
      "        [0.6666],\n",
      "        [0.6244],\n",
      "        [0.5686],\n",
      "        [0.5237],\n",
      "        [0.4691],\n",
      "        [0.4206],\n",
      "        [0.3967],\n",
      "        [0.3605],\n",
      "        [0.3584],\n",
      "        [0.3895],\n",
      "        [0.4373],\n",
      "        [0.4903],\n",
      "        [0.5436],\n",
      "        [0.6000],\n",
      "        [0.6305],\n",
      "        [0.6376],\n",
      "        [0.6023],\n",
      "        [0.5519],\n",
      "        [0.5195],\n",
      "        [0.4729],\n",
      "        [0.4164],\n",
      "        [0.3821],\n",
      "        [0.3723]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3744],\n",
      "        [0.3954],\n",
      "        [0.4041],\n",
      "        [0.4236],\n",
      "        [0.4530],\n",
      "        [0.4481],\n",
      "        [0.4314],\n",
      "        [0.4482],\n",
      "        [0.4729],\n",
      "        [0.4833],\n",
      "        [0.4638],\n",
      "        [0.4721],\n",
      "        [0.4990],\n",
      "        [0.4968],\n",
      "        [0.4940],\n",
      "        [0.5120],\n",
      "        [0.5216],\n",
      "        [0.4758],\n",
      "        [0.4645],\n",
      "        [0.4493],\n",
      "        [0.4260],\n",
      "        [0.4417],\n",
      "        [0.4562],\n",
      "        [0.4809],\n",
      "        [0.4705],\n",
      "        [0.4744],\n",
      "        [0.4855],\n",
      "        [0.4907],\n",
      "        [0.5213],\n",
      "        [0.5260],\n",
      "        [0.5133],\n",
      "        [0.4863]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.4722],\n",
      "        [0.4448],\n",
      "        [0.4213],\n",
      "        [0.3944],\n",
      "        [0.3894],\n",
      "        [0.3702],\n",
      "        [0.3787],\n",
      "        [0.3804],\n",
      "        [0.3664],\n",
      "        [0.3669],\n",
      "        [0.3727],\n",
      "        [0.3880],\n",
      "        [0.3933],\n",
      "        [0.4028],\n",
      "        [0.4034],\n",
      "        [0.4273],\n",
      "        [0.4228],\n",
      "        [0.4321],\n",
      "        [0.4585],\n",
      "        [0.4779],\n",
      "        [0.4978],\n",
      "        [0.5105],\n",
      "        [0.5046],\n",
      "        [0.4793],\n",
      "        [0.4539],\n",
      "        [0.4208],\n",
      "        [0.4122],\n",
      "        [0.3986],\n",
      "        [0.3899],\n",
      "        [0.3763],\n",
      "        [0.3637],\n",
      "        [0.3520]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3348],\n",
      "        [0.3823],\n",
      "        [0.4070],\n",
      "        [0.4299],\n",
      "        [0.4466],\n",
      "        [0.4530],\n",
      "        [0.4648],\n",
      "        [0.4512],\n",
      "        [0.4369],\n",
      "        [0.4275],\n",
      "        [0.4177],\n",
      "        [0.4247],\n",
      "        [0.4532],\n",
      "        [0.4651],\n",
      "        [0.4857],\n",
      "        [0.4853],\n",
      "        [0.4840],\n",
      "        [0.4872],\n",
      "        [0.4909],\n",
      "        [0.4850],\n",
      "        [0.4754],\n",
      "        [0.4617],\n",
      "        [0.4469],\n",
      "        [0.4486],\n",
      "        [0.4512],\n",
      "        [0.4577],\n",
      "        [0.4694],\n",
      "        [0.4737],\n",
      "        [0.4945],\n",
      "        [0.5081],\n",
      "        [0.5131],\n",
      "        [0.5126]], grad_fn=<AddmmBackward0>)\n",
      "Test loss: 0.008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "test_preds = []\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "for (combo_data_3) in test_loader:\n",
    "    inputs, labels = combo_data_3['inputs'], combo_data_3['targets'].unsqueeze(1)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # output, h = model(inputs, h)\n",
    "    output = model(inputs)\n",
    "    print(output)\n",
    "    test_preds.append(output.cpu().detach().numpy())\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze()) #rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "        \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4971323 ],\n",
       "       [0.48460066],\n",
       "       [0.48137152],\n",
       "       [0.47802132],\n",
       "       [0.4631648 ],\n",
       "       [0.4597229 ],\n",
       "       [0.4581977 ],\n",
       "       [0.4630381 ],\n",
       "       [0.4662603 ],\n",
       "       [0.46791887],\n",
       "       [0.4759804 ],\n",
       "       [0.47281235],\n",
       "       [0.4891148 ],\n",
       "       [0.5002863 ],\n",
       "       [0.5084597 ],\n",
       "       [0.5030651 ],\n",
       "       [0.49854293],\n",
       "       [0.48710716],\n",
       "       [0.4762887 ],\n",
       "       [0.45101678],\n",
       "       [0.45186478],\n",
       "       [0.45742166],\n",
       "       [0.45951396],\n",
       "       [0.46212152],\n",
       "       [0.4650805 ],\n",
       "       [0.47031727],\n",
       "       [0.47195488],\n",
       "       [0.47507492],\n",
       "       [0.47624922],\n",
       "       [0.4781336 ],\n",
       "       [0.48294306],\n",
       "       [0.47986436],\n",
       "       [0.47647268],\n",
       "       [0.48142305],\n",
       "       [0.48267022],\n",
       "       [0.4826189 ],\n",
       "       [0.48751202],\n",
       "       [0.4973306 ],\n",
       "       [0.50787795],\n",
       "       [0.50331146],\n",
       "       [0.5004382 ],\n",
       "       [0.49909985],\n",
       "       [0.49670053],\n",
       "       [0.4898007 ],\n",
       "       [0.4918503 ],\n",
       "       [0.49608096],\n",
       "       [0.4952265 ],\n",
       "       [0.4851619 ],\n",
       "       [0.47693202],\n",
       "       [0.47716796],\n",
       "       [0.47034675],\n",
       "       [0.4683445 ],\n",
       "       [0.47121447],\n",
       "       [0.47358853],\n",
       "       [0.4914187 ],\n",
       "       [0.49883667],\n",
       "       [0.50587916],\n",
       "       [0.5225755 ],\n",
       "       [0.52914894],\n",
       "       [0.55668926],\n",
       "       [0.5753881 ],\n",
       "       [0.56964225],\n",
       "       [0.5612296 ],\n",
       "       [0.5452347 ],\n",
       "       [0.51438713],\n",
       "       [0.4972407 ],\n",
       "       [0.48126984],\n",
       "       [0.46979654],\n",
       "       [0.4674468 ],\n",
       "       [0.45244682],\n",
       "       [0.45214045],\n",
       "       [0.46226132],\n",
       "       [0.47416148],\n",
       "       [0.4798976 ],\n",
       "       [0.4805454 ],\n",
       "       [0.48468822],\n",
       "       [0.48695943],\n",
       "       [0.48363903],\n",
       "       [0.47098607],\n",
       "       [0.47416544],\n",
       "       [0.4668222 ],\n",
       "       [0.45769888],\n",
       "       [0.45322862],\n",
       "       [0.43701214],\n",
       "       [0.44102556],\n",
       "       [0.45041913],\n",
       "       [0.48997962],\n",
       "       [0.5071881 ],\n",
       "       [0.5161235 ],\n",
       "       [0.5290551 ],\n",
       "       [0.5386826 ],\n",
       "       [0.54484385],\n",
       "       [0.54574573],\n",
       "       [0.5448307 ],\n",
       "       [0.53909874],\n",
       "       [0.5203248 ],\n",
       "       [0.49738848],\n",
       "       [0.48066902],\n",
       "       [0.4704153 ],\n",
       "       [0.4611525 ],\n",
       "       [0.46102875],\n",
       "       [0.4629656 ],\n",
       "       [0.4591649 ],\n",
       "       [0.43762565],\n",
       "       [0.43240437],\n",
       "       [0.43961233],\n",
       "       [0.45995665],\n",
       "       [0.46458298],\n",
       "       [0.46735534],\n",
       "       [0.47322172],\n",
       "       [0.4429719 ],\n",
       "       [0.4382115 ],\n",
       "       [0.43651995],\n",
       "       [0.42613745],\n",
       "       [0.4244937 ],\n",
       "       [0.4365672 ],\n",
       "       [0.43266034],\n",
       "       [0.43265617],\n",
       "       [0.44995505],\n",
       "       [0.45014668],\n",
       "       [0.45233533],\n",
       "       [0.44555628],\n",
       "       [0.42740333],\n",
       "       [0.4003573 ],\n",
       "       [0.39648768],\n",
       "       [0.39518   ],\n",
       "       [0.3877835 ],\n",
       "       [0.3792359 ],\n",
       "       [0.3817991 ],\n",
       "       [0.38813502],\n",
       "       [0.41113728],\n",
       "       [0.46340764],\n",
       "       [0.49579594],\n",
       "       [0.5203569 ],\n",
       "       [0.525717  ],\n",
       "       [0.51942044],\n",
       "       [0.5091419 ],\n",
       "       [0.5150247 ],\n",
       "       [0.5143646 ],\n",
       "       [0.5071086 ],\n",
       "       [0.48508054],\n",
       "       [0.47312856],\n",
       "       [0.4752066 ],\n",
       "       [0.47379637],\n",
       "       [0.47263446],\n",
       "       [0.46784848],\n",
       "       [0.46683064],\n",
       "       [0.45467418],\n",
       "       [0.43911174],\n",
       "       [0.43564728],\n",
       "       [0.43342102],\n",
       "       [0.47419757],\n",
       "       [0.48894972],\n",
       "       [0.5012883 ],\n",
       "       [0.513494  ],\n",
       "       [0.5361084 ],\n",
       "       [0.5419706 ],\n",
       "       [0.54364955],\n",
       "       [0.5522671 ],\n",
       "       [0.5477173 ],\n",
       "       [0.52685344],\n",
       "       [0.49829486],\n",
       "       [0.48392415],\n",
       "       [0.47053   ],\n",
       "       [0.45132214],\n",
       "       [0.44478726],\n",
       "       [0.45209694],\n",
       "       [0.4577285 ],\n",
       "       [0.45855272],\n",
       "       [0.46194917],\n",
       "       [0.4608423 ],\n",
       "       [0.45855373],\n",
       "       [0.46465763],\n",
       "       [0.4696147 ],\n",
       "       [0.47965917],\n",
       "       [0.48631862],\n",
       "       [0.47202668],\n",
       "       [0.4663353 ],\n",
       "       [0.46032596],\n",
       "       [0.44625586],\n",
       "       [0.41754612],\n",
       "       [0.40637347],\n",
       "       [0.39200947],\n",
       "       [0.3932538 ],\n",
       "       [0.391865  ],\n",
       "       [0.39199102],\n",
       "       [0.39266628],\n",
       "       [0.40520644],\n",
       "       [0.4231425 ],\n",
       "       [0.43435758],\n",
       "       [0.44440487],\n",
       "       [0.47713542],\n",
       "       [0.5017213 ],\n",
       "       [0.5090049 ],\n",
       "       [0.51496446],\n",
       "       [0.51880884],\n",
       "       [0.5272485 ],\n",
       "       [0.52031493],\n",
       "       [0.4975634 ],\n",
       "       [0.47378683],\n",
       "       [0.47248918],\n",
       "       [0.44650257],\n",
       "       [0.42470518],\n",
       "       [0.40755597],\n",
       "       [0.39140645],\n",
       "       [0.3895792 ],\n",
       "       [0.4016367 ],\n",
       "       [0.43300667],\n",
       "       [0.47392976],\n",
       "       [0.4900599 ],\n",
       "       [0.5043039 ],\n",
       "       [0.5209496 ],\n",
       "       [0.5283392 ],\n",
       "       [0.5282107 ],\n",
       "       [0.5271131 ],\n",
       "       [0.5058445 ],\n",
       "       [0.4867924 ],\n",
       "       [0.46272388],\n",
       "       [0.43694186],\n",
       "       [0.42405608],\n",
       "       [0.39916125],\n",
       "       [0.37279713],\n",
       "       [0.35328668],\n",
       "       [0.3422752 ],\n",
       "       [0.34399557],\n",
       "       [0.3470909 ],\n",
       "       [0.36969876],\n",
       "       [0.39205053],\n",
       "       [0.42822105],\n",
       "       [0.42964578],\n",
       "       [0.4507975 ],\n",
       "       [0.4713685 ],\n",
       "       [0.4828834 ],\n",
       "       [0.4843107 ],\n",
       "       [0.4817505 ],\n",
       "       [0.48135048],\n",
       "       [0.436361  ],\n",
       "       [0.40319842],\n",
       "       [0.38854244],\n",
       "       [0.36588466],\n",
       "       [0.33480525],\n",
       "       [0.34885046],\n",
       "       [0.35657427],\n",
       "       [0.38283277],\n",
       "       [0.40205362],\n",
       "       [0.4265841 ],\n",
       "       [0.47902223],\n",
       "       [0.5200248 ],\n",
       "       [0.6402388 ],\n",
       "       [0.68265533],\n",
       "       [0.6876232 ],\n",
       "       [0.6763091 ],\n",
       "       [0.65707827],\n",
       "       [0.62800133],\n",
       "       [0.58119047],\n",
       "       [0.5471764 ],\n",
       "       [0.5027823 ],\n",
       "       [0.45671043],\n",
       "       [0.37822014],\n",
       "       [0.3482789 ],\n",
       "       [0.33455342],\n",
       "       [0.355096  ],\n",
       "       [0.372008  ],\n",
       "       [0.37663016],\n",
       "       [0.38338864],\n",
       "       [0.38014787],\n",
       "       [0.39740825],\n",
       "       [0.42606044],\n",
       "       [0.424473  ],\n",
       "       [0.42212024],\n",
       "       [0.41648194],\n",
       "       [0.42800516],\n",
       "       [0.44799113],\n",
       "       [0.45945117],\n",
       "       [0.48535937],\n",
       "       [0.5062863 ],\n",
       "       [0.5068758 ],\n",
       "       [0.4798886 ],\n",
       "       [0.4880889 ],\n",
       "       [0.48162568],\n",
       "       [0.47374833],\n",
       "       [0.43215847],\n",
       "       [0.40153104],\n",
       "       [0.38995734],\n",
       "       [0.39358208],\n",
       "       [0.39889425],\n",
       "       [0.41992098],\n",
       "       [0.44239163],\n",
       "       [0.44377983],\n",
       "       [0.45385185],\n",
       "       [0.480187  ],\n",
       "       [0.5164578 ],\n",
       "       [0.5465623 ],\n",
       "       [0.54223466],\n",
       "       [0.53893507],\n",
       "       [0.5381535 ],\n",
       "       [0.53419465],\n",
       "       [0.5352493 ],\n",
       "       [0.5532976 ],\n",
       "       [0.5796736 ],\n",
       "       [0.56898963],\n",
       "       [0.5546205 ],\n",
       "       [0.5398903 ],\n",
       "       [0.53694993],\n",
       "       [0.5295341 ],\n",
       "       [0.51613265],\n",
       "       [0.4924476 ],\n",
       "       [0.4779398 ],\n",
       "       [0.4656955 ],\n",
       "       [0.4635312 ],\n",
       "       [0.46872857],\n",
       "       [0.45565766],\n",
       "       [0.43820822],\n",
       "       [0.40676713],\n",
       "       [0.39857572],\n",
       "       [0.4007111 ],\n",
       "       [0.37600884],\n",
       "       [0.36639377],\n",
       "       [0.35734287],\n",
       "       [0.35145825],\n",
       "       [0.35032466],\n",
       "       [0.36102936],\n",
       "       [0.37468764],\n",
       "       [0.38589728],\n",
       "       [0.40026075],\n",
       "       [0.41639757],\n",
       "       [0.44267347],\n",
       "       [0.43936488],\n",
       "       [0.44716695],\n",
       "       [0.44375953],\n",
       "       [0.43727732],\n",
       "       [0.42927998],\n",
       "       [0.39451134],\n",
       "       [0.33661646],\n",
       "       [0.31476754],\n",
       "       [0.29601568],\n",
       "       [0.30328283],\n",
       "       [0.3280778 ],\n",
       "       [0.37301528],\n",
       "       [0.40446317],\n",
       "       [0.44214016],\n",
       "       [0.4317532 ],\n",
       "       [0.42792153],\n",
       "       [0.4304027 ],\n",
       "       [0.4487661 ],\n",
       "       [0.46104532],\n",
       "       [0.4591064 ],\n",
       "       [0.47142506],\n",
       "       [0.46901494],\n",
       "       [0.4507796 ],\n",
       "       [0.452408  ],\n",
       "       [0.45323223],\n",
       "       [0.46687055],\n",
       "       [0.4571968 ],\n",
       "       [0.4531725 ],\n",
       "       [0.4375744 ],\n",
       "       [0.42511594],\n",
       "       [0.4279166 ],\n",
       "       [0.4080249 ],\n",
       "       [0.3820196 ],\n",
       "       [0.3650752 ],\n",
       "       [0.384136  ],\n",
       "       [0.3813481 ],\n",
       "       [0.40920937],\n",
       "       [0.4404553 ],\n",
       "       [0.48470294],\n",
       "       [0.52846646],\n",
       "       [0.56656617],\n",
       "       [0.59470963],\n",
       "       [0.62162876],\n",
       "       [0.63394785],\n",
       "       [0.58526826],\n",
       "       [0.5568738 ],\n",
       "       [0.542061  ],\n",
       "       [0.5204922 ],\n",
       "       [0.5059881 ],\n",
       "       [0.45811465],\n",
       "       [0.42746568],\n",
       "       [0.39226234],\n",
       "       [0.35894242],\n",
       "       [0.34658548],\n",
       "       [0.35648736],\n",
       "       [0.33357748],\n",
       "       [0.3343926 ],\n",
       "       [0.3409892 ],\n",
       "       [0.35699445],\n",
       "       [0.3876986 ],\n",
       "       [0.43947142],\n",
       "       [0.47790304],\n",
       "       [0.530831  ],\n",
       "       [0.60314846],\n",
       "       [0.6346261 ],\n",
       "       [0.6677128 ],\n",
       "       [0.6665906 ],\n",
       "       [0.6243589 ],\n",
       "       [0.5686352 ],\n",
       "       [0.52367264],\n",
       "       [0.46905893],\n",
       "       [0.42059407],\n",
       "       [0.39670038],\n",
       "       [0.3605488 ],\n",
       "       [0.35841116],\n",
       "       [0.3895171 ],\n",
       "       [0.43727785],\n",
       "       [0.49031287],\n",
       "       [0.5435651 ],\n",
       "       [0.6000417 ],\n",
       "       [0.63045233],\n",
       "       [0.63757944],\n",
       "       [0.60226464],\n",
       "       [0.55194366],\n",
       "       [0.51950526],\n",
       "       [0.47288114],\n",
       "       [0.41641435],\n",
       "       [0.38208288],\n",
       "       [0.37232512],\n",
       "       [0.37438214],\n",
       "       [0.39535552],\n",
       "       [0.40414575],\n",
       "       [0.4235805 ],\n",
       "       [0.45298037],\n",
       "       [0.44810507],\n",
       "       [0.4313631 ],\n",
       "       [0.44820964],\n",
       "       [0.47286564],\n",
       "       [0.48329037],\n",
       "       [0.46378684],\n",
       "       [0.47214258],\n",
       "       [0.49903283],\n",
       "       [0.49676237],\n",
       "       [0.49404207],\n",
       "       [0.51198983],\n",
       "       [0.5215961 ],\n",
       "       [0.47583383],\n",
       "       [0.46445397],\n",
       "       [0.44932908],\n",
       "       [0.42604703],\n",
       "       [0.4416502 ],\n",
       "       [0.4562201 ],\n",
       "       [0.48089156],\n",
       "       [0.47048703],\n",
       "       [0.47435492],\n",
       "       [0.48550922],\n",
       "       [0.4907055 ],\n",
       "       [0.52126783],\n",
       "       [0.52597785],\n",
       "       [0.5132595 ],\n",
       "       [0.48628402],\n",
       "       [0.47218844],\n",
       "       [0.444835  ],\n",
       "       [0.42129153],\n",
       "       [0.3944274 ],\n",
       "       [0.38940156],\n",
       "       [0.3701937 ],\n",
       "       [0.37869367],\n",
       "       [0.38039508],\n",
       "       [0.3663922 ],\n",
       "       [0.36687872],\n",
       "       [0.37272522],\n",
       "       [0.38797152],\n",
       "       [0.39332205],\n",
       "       [0.40281597],\n",
       "       [0.4033644 ],\n",
       "       [0.42728746],\n",
       "       [0.42281932],\n",
       "       [0.43208358],\n",
       "       [0.4584919 ],\n",
       "       [0.47794294],\n",
       "       [0.49782652],\n",
       "       [0.5104885 ],\n",
       "       [0.5045766 ],\n",
       "       [0.479326  ],\n",
       "       [0.45393002],\n",
       "       [0.42084014],\n",
       "       [0.41216007],\n",
       "       [0.39855075],\n",
       "       [0.3898746 ],\n",
       "       [0.37625906],\n",
       "       [0.3637145 ],\n",
       "       [0.35197648],\n",
       "       [0.33478504],\n",
       "       [0.38225085],\n",
       "       [0.40700123],\n",
       "       [0.42989457],\n",
       "       [0.4465951 ],\n",
       "       [0.45295578],\n",
       "       [0.4647564 ],\n",
       "       [0.45116103],\n",
       "       [0.43693843],\n",
       "       [0.4274588 ],\n",
       "       [0.41765296],\n",
       "       [0.42467427],\n",
       "       [0.4531725 ],\n",
       "       [0.46514785],\n",
       "       [0.48570734],\n",
       "       [0.4852854 ],\n",
       "       [0.4840196 ],\n",
       "       [0.4872151 ],\n",
       "       [0.49091756],\n",
       "       [0.48499045],\n",
       "       [0.4754217 ],\n",
       "       [0.46166104],\n",
       "       [0.44688043],\n",
       "       [0.4486479 ],\n",
       "       [0.45119333],\n",
       "       [0.4577297 ],\n",
       "       [0.46942526],\n",
       "       [0.47366476],\n",
       "       [0.49452865],\n",
       "       [0.50810707],\n",
       "       [0.5130719 ],\n",
       "       [0.5126288 ]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(test_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.concatenate(test_preds, axis=0), label='test predict')\n",
    "plt.plot(test_loader.dataset[:]['targets'], label= 'real predict', alpha =0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4971],\n",
      "        [0.4846],\n",
      "        [0.4814],\n",
      "        [0.4780],\n",
      "        [0.4632],\n",
      "        [0.4597],\n",
      "        [0.4582],\n",
      "        [0.4630],\n",
      "        [0.4663],\n",
      "        [0.4679],\n",
      "        [0.4760],\n",
      "        [0.4728],\n",
      "        [0.4891],\n",
      "        [0.5003],\n",
      "        [0.5085],\n",
      "        [0.5031],\n",
      "        [0.4985],\n",
      "        [0.4871],\n",
      "        [0.4763],\n",
      "        [0.4510],\n",
      "        [0.4519],\n",
      "        [0.4574],\n",
      "        [0.4595],\n",
      "        [0.4621],\n",
      "        [0.4651],\n",
      "        [0.4703],\n",
      "        [0.4720],\n",
      "        [0.4751],\n",
      "        [0.4762],\n",
      "        [0.4781],\n",
      "        [0.4829],\n",
      "        [0.4799]])\n",
      "tensor([[0.4765],\n",
      "        [0.4814],\n",
      "        [0.4827],\n",
      "        [0.4826],\n",
      "        [0.4875],\n",
      "        [0.4973],\n",
      "        [0.5079],\n",
      "        [0.5033],\n",
      "        [0.5004],\n",
      "        [0.4991],\n",
      "        [0.4967],\n",
      "        [0.4898],\n",
      "        [0.4919],\n",
      "        [0.4961],\n",
      "        [0.4952],\n",
      "        [0.4852],\n",
      "        [0.4769],\n",
      "        [0.4772],\n",
      "        [0.4703],\n",
      "        [0.4683],\n",
      "        [0.4712],\n",
      "        [0.4736],\n",
      "        [0.4914],\n",
      "        [0.4988],\n",
      "        [0.5059],\n",
      "        [0.5226],\n",
      "        [0.5291],\n",
      "        [0.5567],\n",
      "        [0.5754],\n",
      "        [0.5696],\n",
      "        [0.5612],\n",
      "        [0.5452]])\n",
      "tensor([[0.5144],\n",
      "        [0.4972],\n",
      "        [0.4813],\n",
      "        [0.4698],\n",
      "        [0.4674],\n",
      "        [0.4524],\n",
      "        [0.4521],\n",
      "        [0.4623],\n",
      "        [0.4742],\n",
      "        [0.4799],\n",
      "        [0.4805],\n",
      "        [0.4847],\n",
      "        [0.4870],\n",
      "        [0.4836],\n",
      "        [0.4710],\n",
      "        [0.4742],\n",
      "        [0.4668],\n",
      "        [0.4577],\n",
      "        [0.4532],\n",
      "        [0.4370],\n",
      "        [0.4410],\n",
      "        [0.4504],\n",
      "        [0.4900],\n",
      "        [0.5072],\n",
      "        [0.5161],\n",
      "        [0.5291],\n",
      "        [0.5387],\n",
      "        [0.5448],\n",
      "        [0.5457],\n",
      "        [0.5448],\n",
      "        [0.5391],\n",
      "        [0.5203]])\n",
      "tensor([[0.4974],\n",
      "        [0.4807],\n",
      "        [0.4704],\n",
      "        [0.4612],\n",
      "        [0.4610],\n",
      "        [0.4630],\n",
      "        [0.4592],\n",
      "        [0.4376],\n",
      "        [0.4324],\n",
      "        [0.4396],\n",
      "        [0.4600],\n",
      "        [0.4646],\n",
      "        [0.4674],\n",
      "        [0.4732],\n",
      "        [0.4430],\n",
      "        [0.4382],\n",
      "        [0.4365],\n",
      "        [0.4261],\n",
      "        [0.4245],\n",
      "        [0.4366],\n",
      "        [0.4327],\n",
      "        [0.4327],\n",
      "        [0.4500],\n",
      "        [0.4501],\n",
      "        [0.4523],\n",
      "        [0.4456],\n",
      "        [0.4274],\n",
      "        [0.4004],\n",
      "        [0.3965],\n",
      "        [0.3952],\n",
      "        [0.3878],\n",
      "        [0.3792]])\n",
      "tensor([[0.3818],\n",
      "        [0.3881],\n",
      "        [0.4111],\n",
      "        [0.4634],\n",
      "        [0.4958],\n",
      "        [0.5204],\n",
      "        [0.5257],\n",
      "        [0.5194],\n",
      "        [0.5091],\n",
      "        [0.5150],\n",
      "        [0.5144],\n",
      "        [0.5071],\n",
      "        [0.4851],\n",
      "        [0.4731],\n",
      "        [0.4752],\n",
      "        [0.4738],\n",
      "        [0.4726],\n",
      "        [0.4678],\n",
      "        [0.4668],\n",
      "        [0.4547],\n",
      "        [0.4391],\n",
      "        [0.4356],\n",
      "        [0.4334],\n",
      "        [0.4742],\n",
      "        [0.4889],\n",
      "        [0.5013],\n",
      "        [0.5135],\n",
      "        [0.5361],\n",
      "        [0.5420],\n",
      "        [0.5436],\n",
      "        [0.5523],\n",
      "        [0.5477]])\n",
      "tensor([[0.5269],\n",
      "        [0.4983],\n",
      "        [0.4839],\n",
      "        [0.4705],\n",
      "        [0.4513],\n",
      "        [0.4448],\n",
      "        [0.4521],\n",
      "        [0.4577],\n",
      "        [0.4586],\n",
      "        [0.4619],\n",
      "        [0.4608],\n",
      "        [0.4586],\n",
      "        [0.4647],\n",
      "        [0.4696],\n",
      "        [0.4797],\n",
      "        [0.4863],\n",
      "        [0.4720],\n",
      "        [0.4663],\n",
      "        [0.4603],\n",
      "        [0.4463],\n",
      "        [0.4175],\n",
      "        [0.4064],\n",
      "        [0.3920],\n",
      "        [0.3933],\n",
      "        [0.3919],\n",
      "        [0.3920],\n",
      "        [0.3927],\n",
      "        [0.4052],\n",
      "        [0.4231],\n",
      "        [0.4344],\n",
      "        [0.4444],\n",
      "        [0.4771]])\n",
      "tensor([[0.5017],\n",
      "        [0.5090],\n",
      "        [0.5150],\n",
      "        [0.5188],\n",
      "        [0.5272],\n",
      "        [0.5203],\n",
      "        [0.4976],\n",
      "        [0.4738],\n",
      "        [0.4725],\n",
      "        [0.4465],\n",
      "        [0.4247],\n",
      "        [0.4076],\n",
      "        [0.3914],\n",
      "        [0.3896],\n",
      "        [0.4016],\n",
      "        [0.4330],\n",
      "        [0.4739],\n",
      "        [0.4901],\n",
      "        [0.5043],\n",
      "        [0.5209],\n",
      "        [0.5283],\n",
      "        [0.5282],\n",
      "        [0.5271],\n",
      "        [0.5058],\n",
      "        [0.4868],\n",
      "        [0.4627],\n",
      "        [0.4369],\n",
      "        [0.4241],\n",
      "        [0.3992],\n",
      "        [0.3728],\n",
      "        [0.3533],\n",
      "        [0.3423]])\n",
      "tensor([[0.3440],\n",
      "        [0.3471],\n",
      "        [0.3697],\n",
      "        [0.3921],\n",
      "        [0.4282],\n",
      "        [0.4296],\n",
      "        [0.4508],\n",
      "        [0.4714],\n",
      "        [0.4829],\n",
      "        [0.4843],\n",
      "        [0.4818],\n",
      "        [0.4814],\n",
      "        [0.4364],\n",
      "        [0.4032],\n",
      "        [0.3885],\n",
      "        [0.3659],\n",
      "        [0.3348],\n",
      "        [0.3489],\n",
      "        [0.3566],\n",
      "        [0.3828],\n",
      "        [0.4021],\n",
      "        [0.4266],\n",
      "        [0.4790],\n",
      "        [0.5200],\n",
      "        [0.6402],\n",
      "        [0.6827],\n",
      "        [0.6876],\n",
      "        [0.6763],\n",
      "        [0.6571],\n",
      "        [0.6280],\n",
      "        [0.5812],\n",
      "        [0.5472]])\n",
      "tensor([[0.5028],\n",
      "        [0.4567],\n",
      "        [0.3782],\n",
      "        [0.3483],\n",
      "        [0.3346],\n",
      "        [0.3551],\n",
      "        [0.3720],\n",
      "        [0.3766],\n",
      "        [0.3834],\n",
      "        [0.3801],\n",
      "        [0.3974],\n",
      "        [0.4261],\n",
      "        [0.4245],\n",
      "        [0.4221],\n",
      "        [0.4165],\n",
      "        [0.4280],\n",
      "        [0.4480],\n",
      "        [0.4595],\n",
      "        [0.4854],\n",
      "        [0.5063],\n",
      "        [0.5069],\n",
      "        [0.4799],\n",
      "        [0.4881],\n",
      "        [0.4816],\n",
      "        [0.4737],\n",
      "        [0.4322],\n",
      "        [0.4015],\n",
      "        [0.3900],\n",
      "        [0.3936],\n",
      "        [0.3989],\n",
      "        [0.4199],\n",
      "        [0.4424]])\n",
      "tensor([[0.4438],\n",
      "        [0.4539],\n",
      "        [0.4802],\n",
      "        [0.5165],\n",
      "        [0.5466],\n",
      "        [0.5422],\n",
      "        [0.5389],\n",
      "        [0.5382],\n",
      "        [0.5342],\n",
      "        [0.5352],\n",
      "        [0.5533],\n",
      "        [0.5797],\n",
      "        [0.5690],\n",
      "        [0.5546],\n",
      "        [0.5399],\n",
      "        [0.5369],\n",
      "        [0.5295],\n",
      "        [0.5161],\n",
      "        [0.4924],\n",
      "        [0.4779],\n",
      "        [0.4657],\n",
      "        [0.4635],\n",
      "        [0.4687],\n",
      "        [0.4557],\n",
      "        [0.4382],\n",
      "        [0.4068],\n",
      "        [0.3986],\n",
      "        [0.4007],\n",
      "        [0.3760],\n",
      "        [0.3664],\n",
      "        [0.3573],\n",
      "        [0.3515]])\n",
      "tensor([[0.3503],\n",
      "        [0.3610],\n",
      "        [0.3747],\n",
      "        [0.3859],\n",
      "        [0.4003],\n",
      "        [0.4164],\n",
      "        [0.4427],\n",
      "        [0.4394],\n",
      "        [0.4472],\n",
      "        [0.4438],\n",
      "        [0.4373],\n",
      "        [0.4293],\n",
      "        [0.3945],\n",
      "        [0.3366],\n",
      "        [0.3148],\n",
      "        [0.2960],\n",
      "        [0.3033],\n",
      "        [0.3281],\n",
      "        [0.3730],\n",
      "        [0.4045],\n",
      "        [0.4421],\n",
      "        [0.4318],\n",
      "        [0.4279],\n",
      "        [0.4304],\n",
      "        [0.4488],\n",
      "        [0.4610],\n",
      "        [0.4591],\n",
      "        [0.4714],\n",
      "        [0.4690],\n",
      "        [0.4508],\n",
      "        [0.4524],\n",
      "        [0.4532]])\n",
      "tensor([[0.4669],\n",
      "        [0.4572],\n",
      "        [0.4532],\n",
      "        [0.4376],\n",
      "        [0.4251],\n",
      "        [0.4279],\n",
      "        [0.4080],\n",
      "        [0.3820],\n",
      "        [0.3651],\n",
      "        [0.3841],\n",
      "        [0.3813],\n",
      "        [0.4092],\n",
      "        [0.4405],\n",
      "        [0.4847],\n",
      "        [0.5285],\n",
      "        [0.5666],\n",
      "        [0.5947],\n",
      "        [0.6216],\n",
      "        [0.6339],\n",
      "        [0.5853],\n",
      "        [0.5569],\n",
      "        [0.5421],\n",
      "        [0.5205],\n",
      "        [0.5060],\n",
      "        [0.4581],\n",
      "        [0.4275],\n",
      "        [0.3923],\n",
      "        [0.3589],\n",
      "        [0.3466],\n",
      "        [0.3565],\n",
      "        [0.3336],\n",
      "        [0.3344]])\n",
      "tensor([[0.3410],\n",
      "        [0.3570],\n",
      "        [0.3877],\n",
      "        [0.4395],\n",
      "        [0.4779],\n",
      "        [0.5308],\n",
      "        [0.6031],\n",
      "        [0.6346],\n",
      "        [0.6677],\n",
      "        [0.6666],\n",
      "        [0.6244],\n",
      "        [0.5686],\n",
      "        [0.5237],\n",
      "        [0.4691],\n",
      "        [0.4206],\n",
      "        [0.3967],\n",
      "        [0.3605],\n",
      "        [0.3584],\n",
      "        [0.3895],\n",
      "        [0.4373],\n",
      "        [0.4903],\n",
      "        [0.5436],\n",
      "        [0.6000],\n",
      "        [0.6305],\n",
      "        [0.6376],\n",
      "        [0.6023],\n",
      "        [0.5519],\n",
      "        [0.5195],\n",
      "        [0.4729],\n",
      "        [0.4164],\n",
      "        [0.3821],\n",
      "        [0.3723]])\n",
      "tensor([[0.3744],\n",
      "        [0.3954],\n",
      "        [0.4041],\n",
      "        [0.4236],\n",
      "        [0.4530],\n",
      "        [0.4481],\n",
      "        [0.4314],\n",
      "        [0.4482],\n",
      "        [0.4729],\n",
      "        [0.4833],\n",
      "        [0.4638],\n",
      "        [0.4721],\n",
      "        [0.4990],\n",
      "        [0.4968],\n",
      "        [0.4940],\n",
      "        [0.5120],\n",
      "        [0.5216],\n",
      "        [0.4758],\n",
      "        [0.4645],\n",
      "        [0.4493],\n",
      "        [0.4260],\n",
      "        [0.4417],\n",
      "        [0.4562],\n",
      "        [0.4809],\n",
      "        [0.4705],\n",
      "        [0.4744],\n",
      "        [0.4855],\n",
      "        [0.4907],\n",
      "        [0.5213],\n",
      "        [0.5260],\n",
      "        [0.5133],\n",
      "        [0.4863]])\n",
      "tensor([[0.4722],\n",
      "        [0.4448],\n",
      "        [0.4213],\n",
      "        [0.3944],\n",
      "        [0.3894],\n",
      "        [0.3702],\n",
      "        [0.3787],\n",
      "        [0.3804],\n",
      "        [0.3664],\n",
      "        [0.3669],\n",
      "        [0.3727],\n",
      "        [0.3880],\n",
      "        [0.3933],\n",
      "        [0.4028],\n",
      "        [0.4034],\n",
      "        [0.4273],\n",
      "        [0.4228],\n",
      "        [0.4321],\n",
      "        [0.4585],\n",
      "        [0.4779],\n",
      "        [0.4978],\n",
      "        [0.5105],\n",
      "        [0.5046],\n",
      "        [0.4793],\n",
      "        [0.4539],\n",
      "        [0.4208],\n",
      "        [0.4122],\n",
      "        [0.3986],\n",
      "        [0.3899],\n",
      "        [0.3763],\n",
      "        [0.3637],\n",
      "        [0.3520]])\n",
      "tensor([[0.3348],\n",
      "        [0.3823],\n",
      "        [0.4070],\n",
      "        [0.4299],\n",
      "        [0.4466],\n",
      "        [0.4530],\n",
      "        [0.4648],\n",
      "        [0.4512],\n",
      "        [0.4369],\n",
      "        [0.4275],\n",
      "        [0.4177],\n",
      "        [0.4247],\n",
      "        [0.4532],\n",
      "        [0.4651],\n",
      "        [0.4857],\n",
      "        [0.4853],\n",
      "        [0.4840],\n",
      "        [0.4872],\n",
      "        [0.4909],\n",
      "        [0.4850],\n",
      "        [0.4754],\n",
      "        [0.4617],\n",
      "        [0.4469],\n",
      "        [0.4486],\n",
      "        [0.4512],\n",
      "        [0.4577],\n",
      "        [0.4694],\n",
      "        [0.4737],\n",
      "        [0.4945],\n",
      "        [0.5081],\n",
      "        [0.5131],\n",
      "        [0.5126]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        data, targets = batch[\"inputs\"], batch[\"targets\"]\n",
    "\n",
    "        preds = model(data)\n",
    "        print(preds)\n",
    "#   y_val = model(inputCatColumns, inputNumColumns)\n",
    "#   preds.append(y_val > 0.) # if y_val are logits\n",
    "#   loss = loss_function(y_val, test_outputs)\n",
    "#   print(f'Loss: {loss:.8f}')\n",
    "# preds = torch.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesDataset(object):\n",
    "#     def __init__(self, data, categorical_cols, target_col, seq_length, prediction_window=1):\n",
    "#         '''\n",
    "#         :param data: dataset of type pandas.DataFrame\n",
    "#         :param categorical_cols: name of the categorical columns, if None pass empty list\n",
    "#         :param target_col: name of the targeted column\n",
    "#         :param seq_length: window length to use\n",
    "#         :param prediction_window: window length to predict\n",
    "#         '''\n",
    "#         self.data = data\n",
    "#         self.categorical_cols = categorical_cols\n",
    "#         self.numerical_cols = list(set(data.columns) - set(categorical_cols) - set(target_col))\n",
    "#         self.target_col = target_col\n",
    "#         self.seq_length = seq_length\n",
    "#         self.prediction_window = prediction_window\n",
    "#         self.preprocessor = None\n",
    "\n",
    "#     def preprocess_data(self):\n",
    "#         '''Preprocessing function'''\n",
    "#         X = self.data.drop(self.target_col, axis=1)\n",
    "#         y = self.data[self.target_col]\n",
    "\n",
    "#         self.preprocess = ColumnTransformer(\n",
    "#             [(\"scaler\", StandardScaler(), self.numerical_cols),\n",
    "#              (\"encoder\", OneHotEncoder(), self.categorical_cols)],\n",
    "#             remainder=\"passthrough\"\n",
    "#         )\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "#         X_train = self.preprocessor.fit_transform(X_train)\n",
    "#         X_test = self.preprocessor.transform(X_test)\n",
    "\n",
    "#         if self.target_col:\n",
    "#             return X_train, X_test, y_train.values, y_test.values\n",
    "#         return X_train, X_test\n",
    "\n",
    "#     def frame_series(self, X, y=None):\n",
    "#         '''\n",
    "#         Function used to prepare the data for time series prediction\n",
    "#         :param X: set of features\n",
    "#         :param y: targeted value to predict\n",
    "#         :return: TensorDataset\n",
    "#         '''\n",
    "#         nb_obs, nb_features = X.shape\n",
    "#         features, target, y_hist = [], [], []\n",
    "\n",
    "#         for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#             features.append(torch.FloatTensor(X[i:i + self.seq_length, :]).unsqueeze(0))\n",
    "\n",
    "#         features_var = torch.cat(features)\n",
    "\n",
    "#         if y is not None:\n",
    "#             for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#                 target.append(\n",
    "#                     torch.tensor(y[i + self.seq_length:i + self.seq_length + self.prediction_window]))\n",
    "#                 y_hist.append(\n",
    "#                     torch.tensor(y[i + self.seq_length - 1:i + self.seq_length + self.prediction_window - 1]))\n",
    "#             target_var, y_hist_var = torch.cat(target), torch.cat(y_hist)\n",
    "#             return TensorDataset(features_var, target_var, y_hist_var)\n",
    "#         return TensorDataset(features_var)\n",
    "\n",
    "#     def get_loaders(self, batch_size: int):\n",
    "#         '''\n",
    "#         Preprocess and frame the dataset\n",
    "#         :param batch_size: batch size\n",
    "#         :return: DataLoaders associated to training and testing data\n",
    "#         '''\n",
    "#         X_train, X_test, y_train, y_test = self.preprocess_data()\n",
    "\n",
    "#         train_dataset = self.frame_series(X_train, y_train)\n",
    "#         test_dataset = self.frame_series(X_test, y_test)\n",
    "\n",
    "#         train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model.eval() # Turn on the evaluation mode\n",
    "# # total_loss = 0.\n",
    "# # eval_batch_size = 1000\n",
    "# preds = []\n",
    "# # with torch.no_grad():\n",
    "# preds_batch = []\n",
    "# for _, batch in enumerate(test_loader):\n",
    "#     # print(batch[\"targets\"])\n",
    "#     data, targets = batch['inputs'].to(device), batch[\"targets\"].to(device).unsqueeze(1)\n",
    "#     print(data, targets)\n",
    "#     # print(data.size())\n",
    "#     output = model(data)\n",
    "#     # preds_batch.append(output)\n",
    "#     print(output)\n",
    "#     break\n",
    "#     # loss = criterion(output, targets).cpu().item()\n",
    "#     # print(f'Loss: {loss:.8f}')\n",
    "#     # # if calculate_loss_over_all_values:\n",
    "#     # #     total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "#     # # else:                                \n",
    "#     # #     total_loss += len(data[0])* criterion(output[-1:], targets[-1:]).cpu().item() \n",
    "               \n",
    "# # return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The first input argument needs to be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer copy.ipynb Cell 51\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer%20copy.ipynb#X66sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m (np\u001b[39m.\u001b[39;49mconcatenate(preds, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The first input argument needs to be a sequence"
     ]
    }
   ],
   "source": [
    "# (np.concatenate(preds, axis=0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# preds = []\n",
    "# with torch.no_grad():\n",
    "#   y_val = model(inputCatColumns, inputNumColumns)\n",
    "#   preds.append(y_val > 0.) # if y_val are logits\n",
    "#   loss = loss_function(y_val, test_outputs)\n",
    "#   print(f'Loss: {loss:.8f}')\n",
    "# preds = torch.stack(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('TAAC_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f251cf0b22a23ca88a79bdc1e3e56992e90498b36dfcdde46ef40d449004d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
