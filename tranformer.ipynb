{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tdqm import tdqm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" \\\n",
    "Our transformer model will have an input size of (32, 128, 5) \\\n",
    "32 -> batch size \\\n",
    "128 -> sequence length \\\n",
    "5 -> number of features (Open, Low, High, Close) \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are separated into sequences of 128 days, 4 price features and 1 volume feature\n",
    "seq_len = 128\n",
    "# target_len = 3\n",
    "\n",
    "# during a single step the model receives 32 sequences\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock for the deep learning\n",
    "ticker = \"GOOGL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset (use the function in data/data.py)\n",
    "df = pd.read_csv(\n",
    "    \"data/data.csv\", usecols=[\"Date\", \"High\", \"Low\", \"Open\", \"Close\", \"Volume\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>96.790001</td>\n",
       "      <td>94.510002</td>\n",
       "      <td>95.089996</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>30179500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>100.139999</td>\n",
       "      <td>96.709999</td>\n",
       "      <td>98.260002</td>\n",
       "      <td>98.440002</td>\n",
       "      <td>41640800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>99.639999</td>\n",
       "      <td>97.639999</td>\n",
       "      <td>97.900002</td>\n",
       "      <td>98.849998</td>\n",
       "      <td>29105200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>99.279999</td>\n",
       "      <td>96.790001</td>\n",
       "      <td>96.970001</td>\n",
       "      <td>98.360001</td>\n",
       "      <td>26052600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>98.900002</td>\n",
       "      <td>96.370003</td>\n",
       "      <td>98.769997</td>\n",
       "      <td>97.430000</td>\n",
       "      <td>28328800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        High        Low       Open      Close      Volume\n",
       "6661  2022-11-14   96.790001  94.510002  95.089996  95.699997  30179500.0\n",
       "6662  2022-11-15  100.139999  96.709999  98.260002  98.440002  41640800.0\n",
       "6663  2022-11-16   99.639999  97.639999  97.900002  98.849998  29105200.0\n",
       "6664  2022-11-17   99.279999  96.790001  96.970001  98.360001  26052600.0\n",
       "6665  2022-11-18   98.900002  96.370003  98.769997  97.430000  28328800.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid dividing by 0\n",
    "df[\"Volume\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n",
    "\n",
    "# Sort the values based on date\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open\"] = df[\"Open\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"High\"] = df[\"High\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Low\"] = df[\"Low\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Close\"] = df[\"Close\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Volume\"] = df[\"Volume\"].pct_change()\n",
    "\n",
    "# Drop the rows with the NaN created by the percentage change\n",
    "df.dropna(how=\"any\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values to create the separation of the dataset\n",
    "times = sorted(df.index.values)\n",
    "last_10pct = sorted(df.index.values)[-int(0.1 * len(times))]\n",
    "last_20pct = sorted(df.index.values)[-int(0.2 * len(times))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max price columns\n",
    "min_return = min(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].min(axis=0)\n",
    ")\n",
    "max_return = max(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].max(axis=0)\n",
    ")\n",
    "\n",
    "# Min-max normalize price columns (0-1 range)\n",
    "df[\"Open\"] = (df[\"Open\"] - min_return) / (max_return - min_return)\n",
    "df[\"High\"] = (df[\"High\"] - min_return) / (max_return - min_return)\n",
    "df[\"Low\"] = (df[\"Low\"] - min_return) / (max_return - min_return)\n",
    "df[\"Close\"] = (df[\"Close\"] - min_return) / (max_return - min_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max volume column\n",
    "min_volume = df[(df.index < last_20pct)][\"Volume\"].min(axis=0)\n",
    "max_volume = df[(df.index < last_20pct)][\"Volume\"].max(axis=0)\n",
    "\n",
    "# Min-max normalize volume columns (0-1 range)\n",
    "df[\"Volume\"] = (df[\"Volume\"] - min_volume) / (max_volume - min_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5332, 5)\n",
      "Validation data shape: (667, 5)\n",
      "Test data shape: (666, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_13557/2609535530.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_13557/2609535530.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_13557/2609535530.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop(columns=[\"Date\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519737</td>\n",
       "      <td>0.516795</td>\n",
       "      <td>0.399106</td>\n",
       "      <td>0.618376</td>\n",
       "      <td>0.055513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.149419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.149419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.494738</td>\n",
       "      <td>0.636230</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.398992</td>\n",
       "      <td>0.110893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.314767</td>\n",
       "      <td>0.208230</td>\n",
       "      <td>0.381155</td>\n",
       "      <td>0.236202</td>\n",
       "      <td>0.117750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       High       Low      Open     Close    Volume\n",
       "1  0.519737  0.516795  0.399106  0.618376  0.055513\n",
       "2  0.367163  0.367163  0.367163  0.367163  0.149419\n",
       "3  0.367163  0.367163  0.367163  0.367163  0.149419\n",
       "4  0.494738  0.636230  0.672131  0.398992  0.110893\n",
       "5  0.314767  0.208230  0.381155  0.236202  0.117750"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
    "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
    "df_test = df[(df.index >= last_10pct)]\n",
    "\n",
    "# Drop the date column from the splitted datasets\n",
    "df_train.drop(columns=[\"Date\"], inplace=True)\n",
    "df_val.drop(columns=[\"Date\"], inplace=True)\n",
    "df_test.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Train data into arrays np.ndarray\n",
    "train_data = df_train.values\n",
    "val_data = df_val.values\n",
    "test_data = df_test.values\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.T#[:,0:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "# src = torch.rand(10, 5, 128)\n",
    "# out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerData(Dataset):\n",
    "    def __init__(self, data: np.ndarray, seq_len: int) -> None:\n",
    "        \"\"\"Init function of dataset class\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): data from the dataframe to numpy\n",
    "            seq_len (int): len of values to base for the prediction\n",
    "        \"\"\"\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for i in range(seq_len, len(data)):\n",
    "            # Chunks of  data with a length of 128 df-rows\n",
    "            self.inputs.append(data[i-seq_len:i])\n",
    "            \n",
    "            # Value of 4th column (Close Price) of df-row 128+1\n",
    "            self.targets.append(data[:, 3][i])\n",
    "        \n",
    "        self.inputs, self.targets = torch.FloatTensor(np.array(self.inputs)), torch.FloatTensor(np.array(self.targets))\n",
    "        # print(self.inputs)\n",
    "        # print(self.inputs.shape)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Get item at a certain index\n",
    "\n",
    "        Args:\n",
    "            idx (int): index to get the value\n",
    "\n",
    "        Returns:\n",
    "            dict: returns the input and the target\n",
    "        \"\"\"\n",
    "        return {'inputs': self.inputs[idx],\n",
    "                'targets': self.targets[idx]}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Length function \n",
    "\n",
    "        Returns:\n",
    "            int: length of the Dataset\n",
    "        \"\"\"\n",
    "        return min(len(self.inputs), len(self.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TickerData(data=train_data, seq_len=seq_len)\n",
    "val_data = TickerData(data=val_data, seq_len=seq_len)\n",
    "test_data = TickerData(data=test_data, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5204"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _,batch in enumerate(train_loader):\n",
    "#     x = batch[\"inputs\"]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "# input_window = 100\n",
    "# output_window = 5\n",
    "# batch_size = 10 # batch size\n",
    "device = torch.device(\"cpu\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()       \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         #pe.requires_grad = False\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # Add extra dimension to self.pe\n",
    "#         self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(x)\n",
    "#         # print(x.size())\n",
    "#         print(self.pe)\n",
    "#         # print(self.pe.size())\n",
    "#         # self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "  def __init__(self, seq_len):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    # Initialize weights and biases with shape (seq_len)\n",
    "    self.weights_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.bias_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.weights_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.bias_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Calculate linear and periodic time features'''\n",
    "    # print(x[:,:,:4])\n",
    "    x = torch.mean(x[:,:,:4], dim=-1) \n",
    "    # print(x)\n",
    "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "    time_linear = time_linear.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "    time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)\n",
    "    time_periodic = time_periodic.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "    return torch.nan_to_num(torch.cat([time_linear, time_periodic], dim=-1).swapaxes(1,2)) # shape = (batch, seq_len, 2)\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=128,num_layers=4,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        # self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.t2v = Time2Vector(128)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)     \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        \n",
    "        if self.src_mask is None or self.src_mask.size(0) != 5:\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(5).to(device)\n",
    "            self.src_mask = mask\n",
    "        # print(src.size())\n",
    "        # src = self.pos_encoder(src)\n",
    "        # time2vector = self.t2v(src) \n",
    "        # print(torch.count_nonzero(torch.isnan(time2vector)), time2vector)\n",
    "        # print(torch.count_nonzero(torch.isnan(src.swapaxes(1,2))),src.swapaxes(1,2))\n",
    "        # src = torch.concatenate((self.t2v(src), src.swapaxes(1,2)), axis=1)\n",
    "        # print(src.size()) \n",
    "        # print(src.size())\n",
    "        output = self.transformer_encoder(src.swapaxes(1,2),self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# src = torch.rand(32, 5, 128)\n",
    "# out = transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.005 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # data, targets = get_batch(train_data, i,batch_size)\n",
    "        data, targets = batch['inputs'].to(device), batch['targets'].to(device).squeeze(0)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.size())\n",
    "        output = model(data)  \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output)\n",
    "        else:\n",
    "            loss = criterion(output[-1:], targets[-1:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            data, target = batch['inputs'].to(device), batch[\"targets\"].to(device)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-1:], target[-1:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    plt.plot(test_result,color=\"red\")\n",
    "    plt.plot(truth[:500],color=\"blue\")\n",
    "    plt.plot(test_result-truth,color=\"green\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    plt.close()\n",
    "    \n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([32, 128, 5])\n",
      "torch.Size([27, 128, 5])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(val_loader):\n",
    "    # print(batch[\"targets\"])\n",
    "    data, targets = batch['inputs'], batch[\"targets\"]\n",
    "    print(data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_future(eval_model, data_source,steps):\n",
    "#     eval_model.eval() \n",
    "#     total_loss = 0.\n",
    "#     test_result = torch.Tensor(0)    \n",
    "#     truth = torch.Tensor(0)\n",
    "#     _ , data = get_batch(data_source, 0,1)\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0, steps,1):\n",
    "#             input = torch.clone(data[-input_window:])\n",
    "#             input[-output_window:] = 0     \n",
    "#             output = eval_model(data[-input_window:])                        \n",
    "#             data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "#     data = data.cpu().view(-1)\n",
    "    \n",
    "#     plt.plot(data,color=\"red\")       \n",
    "#     plt.plot(data[:input_window],color=\"blue\")\n",
    "#     plt.grid(True, which='both')\n",
    "#     plt.axhline(y=0, color='k')\n",
    "#     plt.savefig('graph/transformer-future%d.png'%steps)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_loader):\n",
    "            # print(batch[\"targets\"])\n",
    "            data, targets = batch['inputs'].to(device), batch[\"targets\"].to(device)\n",
    "            # print(data.size())\n",
    "            output = eval_model(data)           \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-1:], targets[-1:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    32/  162 batches | lr 0.005000 | 45.24 ms | loss 1.97321 | ppl     7.19\n",
      "| epoch   1 |    64/  162 batches | lr 0.005000 | 40.81 ms | loss 0.02146 | ppl     1.02\n",
      "| epoch   1 |    96/  162 batches | lr 0.005000 | 39.73 ms | loss 0.01349 | ppl     1.01\n",
      "| epoch   1 |   128/  162 batches | lr 0.005000 | 39.63 ms | loss 0.01526 | ppl     1.02\n",
      "| epoch   1 |   160/  162 batches | lr 0.005000 | 39.43 ms | loss 0.00754 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.72s | valid loss 0.03273 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    32/  162 batches | lr 0.004802 | 40.77 ms | loss 0.00623 | ppl     1.01\n",
      "| epoch   2 |    64/  162 batches | lr 0.004802 | 39.44 ms | loss 0.00924 | ppl     1.01\n",
      "| epoch   2 |    96/  162 batches | lr 0.004802 | 39.49 ms | loss 0.01165 | ppl     1.01\n",
      "| epoch   2 |   128/  162 batches | lr 0.004802 | 39.80 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch   2 |   160/  162 batches | lr 0.004802 | 39.74 ms | loss 0.00439 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.54s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    32/  162 batches | lr 0.004706 | 40.91 ms | loss 0.00531 | ppl     1.01\n",
      "| epoch   3 |    64/  162 batches | lr 0.004706 | 39.72 ms | loss 0.00828 | ppl     1.01\n",
      "| epoch   3 |    96/  162 batches | lr 0.004706 | 39.67 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch   3 |   128/  162 batches | lr 0.004706 | 39.59 ms | loss 0.00317 | ppl     1.00\n",
      "| epoch   3 |   160/  162 batches | lr 0.004706 | 39.57 ms | loss 0.00360 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.55s | valid loss 0.02124 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    32/  162 batches | lr 0.004612 | 41.13 ms | loss 0.00494 | ppl     1.00\n",
      "| epoch   4 |    64/  162 batches | lr 0.004612 | 39.70 ms | loss 0.00582 | ppl     1.01\n",
      "| epoch   4 |    96/  162 batches | lr 0.004612 | 39.61 ms | loss 0.00391 | ppl     1.00\n",
      "| epoch   4 |   128/  162 batches | lr 0.004612 | 39.65 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch   4 |   160/  162 batches | lr 0.004612 | 39.70 ms | loss 0.00268 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.56s | valid loss 0.02271 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    32/  162 batches | lr 0.004520 | 40.92 ms | loss 0.00435 | ppl     1.00\n",
      "| epoch   5 |    64/  162 batches | lr 0.004520 | 40.48 ms | loss 0.00603 | ppl     1.01\n",
      "| epoch   5 |    96/  162 batches | lr 0.004520 | 39.33 ms | loss 0.00253 | ppl     1.00\n",
      "| epoch   5 |   128/  162 batches | lr 0.004520 | 40.07 ms | loss 0.00231 | ppl     1.00\n",
      "| epoch   5 |   160/  162 batches | lr 0.004520 | 39.68 ms | loss 0.00295 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  6.58s | valid loss 0.02621 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    32/  162 batches | lr 0.004429 | 40.79 ms | loss 0.00413 | ppl     1.00\n",
      "| epoch   6 |    64/  162 batches | lr 0.004429 | 39.56 ms | loss 0.00674 | ppl     1.01\n",
      "| epoch   6 |    96/  162 batches | lr 0.004429 | 39.61 ms | loss 0.00436 | ppl     1.00\n",
      "| epoch   6 |   128/  162 batches | lr 0.004429 | 39.41 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch   6 |   160/  162 batches | lr 0.004429 | 39.49 ms | loss 0.00264 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  6.53s | valid loss 0.02611 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    32/  162 batches | lr 0.004341 | 40.70 ms | loss 0.00388 | ppl     1.00\n",
      "| epoch   7 |    64/  162 batches | lr 0.004341 | 39.70 ms | loss 0.00572 | ppl     1.01\n",
      "| epoch   7 |    96/  162 batches | lr 0.004341 | 39.53 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch   7 |   128/  162 batches | lr 0.004341 | 39.59 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch   7 |   160/  162 batches | lr 0.004341 | 39.59 ms | loss 0.00248 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  6.53s | valid loss 0.02612 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    32/  162 batches | lr 0.004254 | 40.92 ms | loss 0.00380 | ppl     1.00\n",
      "| epoch   8 |    64/  162 batches | lr 0.004254 | 39.55 ms | loss 0.00552 | ppl     1.01\n",
      "| epoch   8 |    96/  162 batches | lr 0.004254 | 39.47 ms | loss 0.00308 | ppl     1.00\n",
      "| epoch   8 |   128/  162 batches | lr 0.004254 | 39.80 ms | loss 0.00157 | ppl     1.00\n",
      "| epoch   8 |   160/  162 batches | lr 0.004254 | 39.62 ms | loss 0.00255 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.54s | valid loss 0.02623 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    32/  162 batches | lr 0.004169 | 40.72 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch   9 |    64/  162 batches | lr 0.004169 | 39.39 ms | loss 0.00504 | ppl     1.01\n",
      "| epoch   9 |    96/  162 batches | lr 0.004169 | 39.50 ms | loss 0.00256 | ppl     1.00\n",
      "| epoch   9 |   128/  162 batches | lr 0.004169 | 39.87 ms | loss 0.00151 | ppl     1.00\n",
      "| epoch   9 |   160/  162 batches | lr 0.004169 | 41.72 ms | loss 0.00203 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.61s | valid loss 0.02173 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    32/  162 batches | lr 0.004085 | 41.60 ms | loss 0.00362 | ppl     1.00\n",
      "| epoch  10 |    64/  162 batches | lr 0.004085 | 44.14 ms | loss 0.00545 | ppl     1.01\n",
      "| epoch  10 |    96/  162 batches | lr 0.004085 | 41.30 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch  10 |   128/  162 batches | lr 0.004085 | 44.66 ms | loss 0.00116 | ppl     1.00\n",
      "| epoch  10 |   160/  162 batches | lr 0.004085 | 42.43 ms | loss 0.00214 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.03s | valid loss 0.02068 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    32/  162 batches | lr 0.004004 | 44.13 ms | loss 0.00398 | ppl     1.00\n",
      "| epoch  11 |    64/  162 batches | lr 0.004004 | 41.80 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  11 |    96/  162 batches | lr 0.004004 | 41.33 ms | loss 0.00241 | ppl     1.00\n",
      "| epoch  11 |   128/  162 batches | lr 0.004004 | 42.43 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  11 |   160/  162 batches | lr 0.004004 | 41.52 ms | loss 0.00210 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  6.94s | valid loss 0.01961 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    32/  162 batches | lr 0.003924 | 42.30 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  12 |    64/  162 batches | lr 0.003924 | 41.41 ms | loss 0.00476 | ppl     1.00\n",
      "| epoch  12 |    96/  162 batches | lr 0.003924 | 42.21 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch  12 |   128/  162 batches | lr 0.003924 | 41.92 ms | loss 0.00112 | ppl     1.00\n",
      "| epoch  12 |   160/  162 batches | lr 0.003924 | 41.82 ms | loss 0.00215 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  6.87s | valid loss 0.01909 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    32/  162 batches | lr 0.003845 | 42.56 ms | loss 0.00370 | ppl     1.00\n",
      "| epoch  13 |    64/  162 batches | lr 0.003845 | 41.40 ms | loss 0.00508 | ppl     1.01\n",
      "| epoch  13 |    96/  162 batches | lr 0.003845 | 41.54 ms | loss 0.00257 | ppl     1.00\n",
      "| epoch  13 |   128/  162 batches | lr 0.003845 | 41.80 ms | loss 0.00136 | ppl     1.00\n",
      "| epoch  13 |   160/  162 batches | lr 0.003845 | 40.21 ms | loss 0.00213 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  6.80s | valid loss 0.01892 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    32/  162 batches | lr 0.003768 | 46.33 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  14 |    64/  162 batches | lr 0.003768 | 41.81 ms | loss 0.00462 | ppl     1.00\n",
      "| epoch  14 |    96/  162 batches | lr 0.003768 | 41.95 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch  14 |   128/  162 batches | lr 0.003768 | 41.24 ms | loss 0.00136 | ppl     1.00\n",
      "| epoch  14 |   160/  162 batches | lr 0.003768 | 40.96 ms | loss 0.00206 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  6.96s | valid loss 0.01863 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    32/  162 batches | lr 0.003693 | 42.67 ms | loss 0.00362 | ppl     1.00\n",
      "| epoch  15 |    64/  162 batches | lr 0.003693 | 41.13 ms | loss 0.00487 | ppl     1.00\n",
      "| epoch  15 |    96/  162 batches | lr 0.003693 | 40.31 ms | loss 0.00196 | ppl     1.00\n",
      "| epoch  15 |   128/  162 batches | lr 0.003693 | 42.53 ms | loss 0.00124 | ppl     1.00\n",
      "| epoch  15 |   160/  162 batches | lr 0.003693 | 41.62 ms | loss 0.00193 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  6.85s | valid loss 0.01861 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    32/  162 batches | lr 0.003619 | 44.13 ms | loss 0.00365 | ppl     1.00\n",
      "| epoch  16 |    64/  162 batches | lr 0.003619 | 41.18 ms | loss 0.00444 | ppl     1.00\n",
      "| epoch  16 |    96/  162 batches | lr 0.003619 | 41.26 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch  16 |   128/  162 batches | lr 0.003619 | 39.88 ms | loss 0.00146 | ppl     1.00\n",
      "| epoch  16 |   160/  162 batches | lr 0.003619 | 39.73 ms | loss 0.00190 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  6.76s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    32/  162 batches | lr 0.003547 | 41.25 ms | loss 0.00367 | ppl     1.00\n",
      "| epoch  17 |    64/  162 batches | lr 0.003547 | 39.96 ms | loss 0.00441 | ppl     1.00\n",
      "| epoch  17 |    96/  162 batches | lr 0.003547 | 47.39 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch  17 |   128/  162 batches | lr 0.003547 | 41.06 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  17 |   160/  162 batches | lr 0.003547 | 41.94 ms | loss 0.00190 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  6.94s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    32/  162 batches | lr 0.003476 | 46.26 ms | loss 0.00362 | ppl     1.00\n",
      "| epoch  18 |    64/  162 batches | lr 0.003476 | 39.71 ms | loss 0.00453 | ppl     1.00\n",
      "| epoch  18 |    96/  162 batches | lr 0.003476 | 39.62 ms | loss 0.00172 | ppl     1.00\n",
      "| epoch  18 |   128/  162 batches | lr 0.003476 | 39.55 ms | loss 0.00117 | ppl     1.00\n",
      "| epoch  18 |   160/  162 batches | lr 0.003476 | 39.42 ms | loss 0.00188 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  6.71s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    32/  162 batches | lr 0.003406 | 40.64 ms | loss 0.00369 | ppl     1.00\n",
      "| epoch  19 |    64/  162 batches | lr 0.003406 | 39.90 ms | loss 0.00434 | ppl     1.00\n",
      "| epoch  19 |    96/  162 batches | lr 0.003406 | 40.65 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch  19 |   128/  162 batches | lr 0.003406 | 39.47 ms | loss 0.00113 | ppl     1.00\n",
      "| epoch  19 |   160/  162 batches | lr 0.003406 | 39.08 ms | loss 0.00180 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  6.55s | valid loss 0.01861 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    32/  162 batches | lr 0.003338 | 39.84 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  20 |    64/  162 batches | lr 0.003338 | 38.99 ms | loss 0.00464 | ppl     1.00\n",
      "| epoch  20 |    96/  162 batches | lr 0.003338 | 39.03 ms | loss 0.00174 | ppl     1.00\n",
      "| epoch  20 |   128/  162 batches | lr 0.003338 | 39.04 ms | loss 0.00114 | ppl     1.00\n",
      "| epoch  20 |   160/  162 batches | lr 0.003338 | 39.10 ms | loss 0.00177 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  6.43s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    32/  162 batches | lr 0.003271 | 40.18 ms | loss 0.00375 | ppl     1.00\n",
      "| epoch  21 |    64/  162 batches | lr 0.003271 | 39.01 ms | loss 0.00434 | ppl     1.00\n",
      "| epoch  21 |    96/  162 batches | lr 0.003271 | 39.01 ms | loss 0.00161 | ppl     1.00\n",
      "| epoch  21 |   128/  162 batches | lr 0.003271 | 39.02 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  21 |   160/  162 batches | lr 0.003271 | 39.06 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  6.44s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    32/  162 batches | lr 0.003206 | 40.10 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  22 |    64/  162 batches | lr 0.003206 | 38.99 ms | loss 0.00433 | ppl     1.00\n",
      "| epoch  22 |    96/  162 batches | lr 0.003206 | 39.39 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  22 |   128/  162 batches | lr 0.003206 | 39.39 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  22 |   160/  162 batches | lr 0.003206 | 39.00 ms | loss 0.00166 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  6.46s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    32/  162 batches | lr 0.003142 | 40.11 ms | loss 0.00368 | ppl     1.00\n",
      "| epoch  23 |    64/  162 batches | lr 0.003142 | 39.05 ms | loss 0.00435 | ppl     1.00\n",
      "| epoch  23 |    96/  162 batches | lr 0.003142 | 38.99 ms | loss 0.00162 | ppl     1.00\n",
      "| epoch  23 |   128/  162 batches | lr 0.003142 | 39.05 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  23 |   160/  162 batches | lr 0.003142 | 39.09 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  6.44s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    32/  162 batches | lr 0.003079 | 40.22 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  24 |    64/  162 batches | lr 0.003079 | 38.81 ms | loss 0.00438 | ppl     1.00\n",
      "| epoch  24 |    96/  162 batches | lr 0.003079 | 38.95 ms | loss 0.00130 | ppl     1.00\n",
      "| epoch  24 |   128/  162 batches | lr 0.003079 | 38.90 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  24 |   160/  162 batches | lr 0.003079 | 38.89 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  6.42s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    32/  162 batches | lr 0.003017 | 40.14 ms | loss 0.00363 | ppl     1.00\n",
      "| epoch  25 |    64/  162 batches | lr 0.003017 | 39.33 ms | loss 0.00432 | ppl     1.00\n",
      "| epoch  25 |    96/  162 batches | lr 0.003017 | 39.23 ms | loss 0.00130 | ppl     1.00\n",
      "| epoch  25 |   128/  162 batches | lr 0.003017 | 38.98 ms | loss 0.00117 | ppl     1.00\n",
      "| epoch  25 |   160/  162 batches | lr 0.003017 | 38.82 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  6.45s | valid loss 0.01865 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    32/  162 batches | lr 0.002957 | 39.98 ms | loss 0.00364 | ppl     1.00\n",
      "| epoch  26 |    64/  162 batches | lr 0.002957 | 38.76 ms | loss 0.00431 | ppl     1.00\n",
      "| epoch  26 |    96/  162 batches | lr 0.002957 | 39.07 ms | loss 0.00133 | ppl     1.00\n",
      "| epoch  26 |   128/  162 batches | lr 0.002957 | 39.32 ms | loss 0.00112 | ppl     1.00\n",
      "| epoch  26 |   160/  162 batches | lr 0.002957 | 39.08 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  6.44s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    32/  162 batches | lr 0.002898 | 40.13 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  27 |    64/  162 batches | lr 0.002898 | 39.10 ms | loss 0.00421 | ppl     1.00\n",
      "| epoch  27 |    96/  162 batches | lr 0.002898 | 39.10 ms | loss 0.00132 | ppl     1.00\n",
      "| epoch  27 |   128/  162 batches | lr 0.002898 | 38.99 ms | loss 0.00111 | ppl     1.00\n",
      "| epoch  27 |   160/  162 batches | lr 0.002898 | 39.10 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  6.44s | valid loss 0.01863 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    32/  162 batches | lr 0.002840 | 40.19 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  28 |    64/  162 batches | lr 0.002840 | 39.07 ms | loss 0.00413 | ppl     1.00\n",
      "| epoch  28 |    96/  162 batches | lr 0.002840 | 39.01 ms | loss 0.00141 | ppl     1.00\n",
      "| epoch  28 |   128/  162 batches | lr 0.002840 | 38.97 ms | loss 0.00115 | ppl     1.00\n",
      "| epoch  28 |   160/  162 batches | lr 0.002840 | 38.83 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  6.43s | valid loss 0.01882 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    32/  162 batches | lr 0.002783 | 40.40 ms | loss 0.00367 | ppl     1.00\n",
      "| epoch  29 |    64/  162 batches | lr 0.002783 | 38.67 ms | loss 0.00405 | ppl     1.00\n",
      "| epoch  29 |    96/  162 batches | lr 0.002783 | 38.99 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  29 |   128/  162 batches | lr 0.002783 | 38.88 ms | loss 0.00109 | ppl     1.00\n",
      "| epoch  29 |   160/  162 batches | lr 0.002783 | 38.79 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  6.42s | valid loss 0.01876 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    32/  162 batches | lr 0.002727 | 40.04 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  30 |    64/  162 batches | lr 0.002727 | 38.94 ms | loss 0.00405 | ppl     1.00\n",
      "| epoch  30 |    96/  162 batches | lr 0.002727 | 38.94 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch  30 |   128/  162 batches | lr 0.002727 | 38.93 ms | loss 0.00108 | ppl     1.00\n",
      "| epoch  30 |   160/  162 batches | lr 0.002727 | 38.96 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  6.42s | valid loss 0.01876 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    32/  162 batches | lr 0.002673 | 40.16 ms | loss 0.00352 | ppl     1.00\n",
      "| epoch  31 |    64/  162 batches | lr 0.002673 | 38.81 ms | loss 0.00413 | ppl     1.00\n",
      "| epoch  31 |    96/  162 batches | lr 0.002673 | 39.04 ms | loss 0.00132 | ppl     1.00\n",
      "| epoch  31 |   128/  162 batches | lr 0.002673 | 38.89 ms | loss 0.00112 | ppl     1.00\n",
      "| epoch  31 |   160/  162 batches | lr 0.002673 | 39.21 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  6.43s | valid loss 0.01871 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    32/  162 batches | lr 0.002619 | 40.00 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  32 |    64/  162 batches | lr 0.002619 | 39.00 ms | loss 0.00400 | ppl     1.00\n",
      "| epoch  32 |    96/  162 batches | lr 0.002619 | 38.94 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch  32 |   128/  162 batches | lr 0.002619 | 39.07 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  32 |   160/  162 batches | lr 0.002619 | 38.88 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  6.43s | valid loss 0.01871 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    32/  162 batches | lr 0.002567 | 40.71 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  33 |    64/  162 batches | lr 0.002567 | 39.07 ms | loss 0.00395 | ppl     1.00\n",
      "| epoch  33 |    96/  162 batches | lr 0.002567 | 39.02 ms | loss 0.00144 | ppl     1.00\n",
      "| epoch  33 |   128/  162 batches | lr 0.002567 | 38.83 ms | loss 0.00110 | ppl     1.00\n",
      "| epoch  33 |   160/  162 batches | lr 0.002567 | 39.44 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  6.46s | valid loss 0.01876 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    32/  162 batches | lr 0.002516 | 40.20 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  34 |    64/  162 batches | lr 0.002516 | 38.95 ms | loss 0.00407 | ppl     1.00\n",
      "| epoch  34 |    96/  162 batches | lr 0.002516 | 38.94 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch  34 |   128/  162 batches | lr 0.002516 | 39.44 ms | loss 0.00107 | ppl     1.00\n",
      "| epoch  34 |   160/  162 batches | lr 0.002516 | 38.90 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  6.44s | valid loss 0.01875 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    32/  162 batches | lr 0.002465 | 40.25 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  35 |    64/  162 batches | lr 0.002465 | 38.87 ms | loss 0.00396 | ppl     1.00\n",
      "| epoch  35 |    96/  162 batches | lr 0.002465 | 38.90 ms | loss 0.00145 | ppl     1.00\n",
      "| epoch  35 |   128/  162 batches | lr 0.002465 | 39.07 ms | loss 0.00110 | ppl     1.00\n",
      "| epoch  35 |   160/  162 batches | lr 0.002465 | 39.15 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  6.44s | valid loss 0.01880 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    32/  162 batches | lr 0.002416 | 40.22 ms | loss 0.00348 | ppl     1.00\n",
      "| epoch  36 |    64/  162 batches | lr 0.002416 | 39.10 ms | loss 0.00402 | ppl     1.00\n",
      "| epoch  36 |    96/  162 batches | lr 0.002416 | 38.85 ms | loss 0.00152 | ppl     1.00\n",
      "| epoch  36 |   128/  162 batches | lr 0.002416 | 39.03 ms | loss 0.00108 | ppl     1.00\n",
      "| epoch  36 |   160/  162 batches | lr 0.002416 | 39.03 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  6.44s | valid loss 0.01875 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    32/  162 batches | lr 0.002368 | 40.17 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  37 |    64/  162 batches | lr 0.002368 | 38.90 ms | loss 0.00402 | ppl     1.00\n",
      "| epoch  37 |    96/  162 batches | lr 0.002368 | 38.89 ms | loss 0.00138 | ppl     1.00\n",
      "| epoch  37 |   128/  162 batches | lr 0.002368 | 38.99 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  37 |   160/  162 batches | lr 0.002368 | 38.85 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  6.42s | valid loss 0.01886 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    32/  162 batches | lr 0.002320 | 40.12 ms | loss 0.00362 | ppl     1.00\n",
      "| epoch  38 |    64/  162 batches | lr 0.002320 | 38.97 ms | loss 0.00405 | ppl     1.00\n",
      "| epoch  38 |    96/  162 batches | lr 0.002320 | 39.03 ms | loss 0.00143 | ppl     1.00\n",
      "| epoch  38 |   128/  162 batches | lr 0.002320 | 38.74 ms | loss 0.00107 | ppl     1.00\n",
      "| epoch  38 |   160/  162 batches | lr 0.002320 | 39.02 ms | loss 0.00182 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  6.43s | valid loss 0.01874 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    32/  162 batches | lr 0.002274 | 40.10 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  39 |    64/  162 batches | lr 0.002274 | 38.91 ms | loss 0.00393 | ppl     1.00\n",
      "| epoch  39 |    96/  162 batches | lr 0.002274 | 38.84 ms | loss 0.00150 | ppl     1.00\n",
      "| epoch  39 |   128/  162 batches | lr 0.002274 | 39.22 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  39 |   160/  162 batches | lr 0.002274 | 38.98 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  6.43s | valid loss 0.01885 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    32/  162 batches | lr 0.002229 | 40.46 ms | loss 0.00371 | ppl     1.00\n",
      "| epoch  40 |    64/  162 batches | lr 0.002229 | 48.53 ms | loss 0.00400 | ppl     1.00\n",
      "| epoch  40 |    96/  162 batches | lr 0.002229 | 44.58 ms | loss 0.00144 | ppl     1.00\n",
      "| epoch  40 |   128/  162 batches | lr 0.002229 | 40.03 ms | loss 0.00108 | ppl     1.00\n",
      "| epoch  40 |   160/  162 batches | lr 0.002229 | 39.69 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time:  6.99s | valid loss 0.01874 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    32/  162 batches | lr 0.002184 | 40.97 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  41 |    64/  162 batches | lr 0.002184 | 39.79 ms | loss 0.00397 | ppl     1.00\n",
      "| epoch  41 |    96/  162 batches | lr 0.002184 | 39.57 ms | loss 0.00152 | ppl     1.00\n",
      "| epoch  41 |   128/  162 batches | lr 0.002184 | 41.27 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  41 |   160/  162 batches | lr 0.002184 | 43.24 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  6.71s | valid loss 0.01897 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    32/  162 batches | lr 0.002140 | 40.55 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  42 |    64/  162 batches | lr 0.002140 | 38.99 ms | loss 0.00397 | ppl     1.00\n",
      "| epoch  42 |    96/  162 batches | lr 0.002140 | 39.22 ms | loss 0.00139 | ppl     1.00\n",
      "| epoch  42 |   128/  162 batches | lr 0.002140 | 39.16 ms | loss 0.00104 | ppl     1.00\n",
      "| epoch  42 |   160/  162 batches | lr 0.002140 | 38.73 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  6.45s | valid loss 0.01881 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    32/  162 batches | lr 0.002097 | 40.03 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  43 |    64/  162 batches | lr 0.002097 | 38.88 ms | loss 0.00397 | ppl     1.00\n",
      "| epoch  43 |    96/  162 batches | lr 0.002097 | 38.96 ms | loss 0.00147 | ppl     1.00\n",
      "| epoch  43 |   128/  162 batches | lr 0.002097 | 38.82 ms | loss 0.00106 | ppl     1.00\n",
      "| epoch  43 |   160/  162 batches | lr 0.002097 | 38.97 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  6.42s | valid loss 0.01883 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    32/  162 batches | lr 0.002055 | 40.24 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  44 |    64/  162 batches | lr 0.002055 | 39.04 ms | loss 0.00396 | ppl     1.00\n",
      "| epoch  44 |    96/  162 batches | lr 0.002055 | 39.14 ms | loss 0.00146 | ppl     1.00\n",
      "| epoch  44 |   128/  162 batches | lr 0.002055 | 38.87 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  44 |   160/  162 batches | lr 0.002055 | 38.99 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  6.44s | valid loss 0.01873 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    32/  162 batches | lr 0.002014 | 40.30 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  45 |    64/  162 batches | lr 0.002014 | 39.03 ms | loss 0.00389 | ppl     1.00\n",
      "| epoch  45 |    96/  162 batches | lr 0.002014 | 39.02 ms | loss 0.00143 | ppl     1.00\n",
      "| epoch  45 |   128/  162 batches | lr 0.002014 | 39.73 ms | loss 0.00104 | ppl     1.00\n",
      "| epoch  45 |   160/  162 batches | lr 0.002014 | 39.33 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time:  6.48s | valid loss 0.01869 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    32/  162 batches | lr 0.001974 | 40.48 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  46 |    64/  162 batches | lr 0.001974 | 39.18 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  46 |    96/  162 batches | lr 0.001974 | 39.00 ms | loss 0.00138 | ppl     1.00\n",
      "| epoch  46 |   128/  162 batches | lr 0.001974 | 39.12 ms | loss 0.00104 | ppl     1.00\n",
      "| epoch  46 |   160/  162 batches | lr 0.001974 | 39.21 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  6.46s | valid loss 0.01882 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    32/  162 batches | lr 0.001935 | 40.25 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  47 |    64/  162 batches | lr 0.001935 | 39.09 ms | loss 0.00377 | ppl     1.00\n",
      "| epoch  47 |    96/  162 batches | lr 0.001935 | 38.86 ms | loss 0.00148 | ppl     1.00\n",
      "| epoch  47 |   128/  162 batches | lr 0.001935 | 39.56 ms | loss 0.00107 | ppl     1.00\n",
      "| epoch  47 |   160/  162 batches | lr 0.001935 | 38.91 ms | loss 0.00178 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  6.45s | valid loss 0.01874 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    32/  162 batches | lr 0.001896 | 40.20 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  48 |    64/  162 batches | lr 0.001896 | 39.25 ms | loss 0.00394 | ppl     1.00\n",
      "| epoch  48 |    96/  162 batches | lr 0.001896 | 39.16 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  48 |   128/  162 batches | lr 0.001896 | 39.12 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  48 |   160/  162 batches | lr 0.001896 | 39.28 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  6.46s | valid loss 0.01875 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    32/  162 batches | lr 0.001858 | 40.32 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  49 |    64/  162 batches | lr 0.001858 | 39.13 ms | loss 0.00381 | ppl     1.00\n",
      "| epoch  49 |    96/  162 batches | lr 0.001858 | 39.19 ms | loss 0.00139 | ppl     1.00\n",
      "| epoch  49 |   128/  162 batches | lr 0.001858 | 39.12 ms | loss 0.00104 | ppl     1.00\n",
      "| epoch  49 |   160/  162 batches | lr 0.001858 | 39.16 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  6.46s | valid loss 0.01878 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    32/  162 batches | lr 0.001821 | 40.43 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  50 |    64/  162 batches | lr 0.001821 | 39.32 ms | loss 0.00386 | ppl     1.00\n",
      "| epoch  50 |    96/  162 batches | lr 0.001821 | 39.17 ms | loss 0.00141 | ppl     1.00\n",
      "| epoch  50 |   128/  162 batches | lr 0.001821 | 39.09 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  50 |   160/  162 batches | lr 0.001821 | 39.12 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  6.47s | valid loss 0.01875 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    32/  162 batches | lr 0.001784 | 40.32 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  51 |    64/  162 batches | lr 0.001784 | 38.98 ms | loss 0.00373 | ppl     1.00\n",
      "| epoch  51 |    96/  162 batches | lr 0.001784 | 39.02 ms | loss 0.00141 | ppl     1.00\n",
      "| epoch  51 |   128/  162 batches | lr 0.001784 | 39.11 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  51 |   160/  162 batches | lr 0.001784 | 39.23 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  6.45s | valid loss 0.01870 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    32/  162 batches | lr 0.001749 | 40.28 ms | loss 0.00358 | ppl     1.00\n",
      "| epoch  52 |    64/  162 batches | lr 0.001749 | 49.66 ms | loss 0.00373 | ppl     1.00\n",
      "| epoch  52 |    96/  162 batches | lr 0.001749 | 40.37 ms | loss 0.00133 | ppl     1.00\n",
      "| epoch  52 |   128/  162 batches | lr 0.001749 | 40.12 ms | loss 0.00105 | ppl     1.00\n",
      "| epoch  52 |   160/  162 batches | lr 0.001749 | 39.81 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  6.89s | valid loss 0.01872 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    32/  162 batches | lr 0.001714 | 40.72 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  53 |    64/  162 batches | lr 0.001714 | 39.79 ms | loss 0.00374 | ppl     1.00\n",
      "| epoch  53 |    96/  162 batches | lr 0.001714 | 40.19 ms | loss 0.00139 | ppl     1.00\n",
      "| epoch  53 |   128/  162 batches | lr 0.001714 | 39.91 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  53 |   160/  162 batches | lr 0.001714 | 40.95 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  6.61s | valid loss 0.01871 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    32/  162 batches | lr 0.001679 | 40.41 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  54 |    64/  162 batches | lr 0.001679 | 39.15 ms | loss 0.00379 | ppl     1.00\n",
      "| epoch  54 |    96/  162 batches | lr 0.001679 | 39.09 ms | loss 0.00137 | ppl     1.00\n",
      "| epoch  54 |   128/  162 batches | lr 0.001679 | 39.18 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  54 |   160/  162 batches | lr 0.001679 | 39.14 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  6.46s | valid loss 0.01868 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    32/  162 batches | lr 0.001646 | 40.21 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  55 |    64/  162 batches | lr 0.001646 | 39.04 ms | loss 0.00372 | ppl     1.00\n",
      "| epoch  55 |    96/  162 batches | lr 0.001646 | 39.14 ms | loss 0.00130 | ppl     1.00\n",
      "| epoch  55 |   128/  162 batches | lr 0.001646 | 39.15 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  55 |   160/  162 batches | lr 0.001646 | 39.23 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time:  6.46s | valid loss 0.01870 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    32/  162 batches | lr 0.001613 | 40.28 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  56 |    64/  162 batches | lr 0.001613 | 38.98 ms | loss 0.00369 | ppl     1.00\n",
      "| epoch  56 |    96/  162 batches | lr 0.001613 | 39.34 ms | loss 0.00139 | ppl     1.00\n",
      "| epoch  56 |   128/  162 batches | lr 0.001613 | 38.66 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  56 |   160/  162 batches | lr 0.001613 | 38.84 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  6.43s | valid loss 0.01870 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    32/  162 batches | lr 0.001581 | 39.95 ms | loss 0.00352 | ppl     1.00\n",
      "| epoch  57 |    64/  162 batches | lr 0.001581 | 39.02 ms | loss 0.00373 | ppl     1.00\n",
      "| epoch  57 |    96/  162 batches | lr 0.001581 | 38.73 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  57 |   128/  162 batches | lr 0.001581 | 38.91 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  57 |   160/  162 batches | lr 0.001581 | 44.51 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  6.61s | valid loss 0.01868 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    32/  162 batches | lr 0.001549 | 47.02 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  58 |    64/  162 batches | lr 0.001549 | 39.58 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  58 |    96/  162 batches | lr 0.001549 | 39.70 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  58 |   128/  162 batches | lr 0.001549 | 46.08 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  58 |   160/  162 batches | lr 0.001549 | 41.30 ms | loss 0.00178 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  7.01s | valid loss 0.01866 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    32/  162 batches | lr 0.001518 | 42.11 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  59 |    64/  162 batches | lr 0.001518 | 41.15 ms | loss 0.00377 | ppl     1.00\n",
      "| epoch  59 |    96/  162 batches | lr 0.001518 | 40.60 ms | loss 0.00130 | ppl     1.00\n",
      "| epoch  59 |   128/  162 batches | lr 0.001518 | 40.48 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  59 |   160/  162 batches | lr 0.001518 | 41.56 ms | loss 0.00179 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  6.80s | valid loss 0.01875 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    32/  162 batches | lr 0.001488 | 41.79 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  60 |    64/  162 batches | lr 0.001488 | 41.04 ms | loss 0.00366 | ppl     1.00\n",
      "| epoch  60 |    96/  162 batches | lr 0.001488 | 40.42 ms | loss 0.00135 | ppl     1.00\n",
      "| epoch  60 |   128/  162 batches | lr 0.001488 | 39.86 ms | loss 0.00105 | ppl     1.00\n",
      "| epoch  60 |   160/  162 batches | lr 0.001488 | 40.30 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time:  6.68s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    32/  162 batches | lr 0.001458 | 42.27 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  61 |    64/  162 batches | lr 0.001458 | 40.22 ms | loss 0.00362 | ppl     1.00\n",
      "| epoch  61 |    96/  162 batches | lr 0.001458 | 39.93 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  61 |   128/  162 batches | lr 0.001458 | 39.60 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  61 |   160/  162 batches | lr 0.001458 | 40.47 ms | loss 0.00179 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  6.64s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    32/  162 batches | lr 0.001429 | 41.44 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  62 |    64/  162 batches | lr 0.001429 | 40.12 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  62 |    96/  162 batches | lr 0.001429 | 40.15 ms | loss 0.00131 | ppl     1.00\n",
      "| epoch  62 |   128/  162 batches | lr 0.001429 | 40.28 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  62 |   160/  162 batches | lr 0.001429 | 40.34 ms | loss 0.00179 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  6.66s | valid loss 0.01863 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    32/  162 batches | lr 0.001400 | 41.78 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch  63 |    64/  162 batches | lr 0.001400 | 40.52 ms | loss 0.00364 | ppl     1.00\n",
      "| epoch  63 |    96/  162 batches | lr 0.001400 | 39.89 ms | loss 0.00129 | ppl     1.00\n",
      "| epoch  63 |   128/  162 batches | lr 0.001400 | 39.95 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  63 |   160/  162 batches | lr 0.001400 | 40.03 ms | loss 0.00178 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  6.63s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    32/  162 batches | lr 0.001372 | 41.58 ms | loss 0.00361 | ppl     1.00\n",
      "| epoch  64 |    64/  162 batches | lr 0.001372 | 40.77 ms | loss 0.00364 | ppl     1.00\n",
      "| epoch  64 |    96/  162 batches | lr 0.001372 | 40.07 ms | loss 0.00131 | ppl     1.00\n",
      "| epoch  64 |   128/  162 batches | lr 0.001372 | 39.72 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  64 |   160/  162 batches | lr 0.001372 | 41.75 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  6.69s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    32/  162 batches | lr 0.001345 | 43.25 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  65 |    64/  162 batches | lr 0.001345 | 42.05 ms | loss 0.00361 | ppl     1.00\n",
      "| epoch  65 |    96/  162 batches | lr 0.001345 | 40.85 ms | loss 0.00126 | ppl     1.00\n",
      "| epoch  65 |   128/  162 batches | lr 0.001345 | 42.27 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  65 |   160/  162 batches | lr 0.001345 | 39.83 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time:  6.83s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    32/  162 batches | lr 0.001318 | 41.34 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  66 |    64/  162 batches | lr 0.001318 | 40.14 ms | loss 0.00363 | ppl     1.00\n",
      "| epoch  66 |    96/  162 batches | lr 0.001318 | 40.40 ms | loss 0.00125 | ppl     1.00\n",
      "| epoch  66 |   128/  162 batches | lr 0.001318 | 40.28 ms | loss 0.00101 | ppl     1.00\n",
      "| epoch  66 |   160/  162 batches | lr 0.001318 | 40.89 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  6.67s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    32/  162 batches | lr 0.001292 | 41.35 ms | loss 0.00347 | ppl     1.00\n",
      "| epoch  67 |    64/  162 batches | lr 0.001292 | 40.08 ms | loss 0.00363 | ppl     1.00\n",
      "| epoch  67 |    96/  162 batches | lr 0.001292 | 40.12 ms | loss 0.00128 | ppl     1.00\n",
      "| epoch  67 |   128/  162 batches | lr 0.001292 | 40.41 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  67 |   160/  162 batches | lr 0.001292 | 40.28 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  6.64s | valid loss 0.01863 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    32/  162 batches | lr 0.001266 | 41.92 ms | loss 0.00347 | ppl     1.00\n",
      "| epoch  68 |    64/  162 batches | lr 0.001266 | 40.16 ms | loss 0.00364 | ppl     1.00\n",
      "| epoch  68 |    96/  162 batches | lr 0.001266 | 40.21 ms | loss 0.00137 | ppl     1.00\n",
      "| epoch  68 |   128/  162 batches | lr 0.001266 | 40.20 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  68 |   160/  162 batches | lr 0.001266 | 43.36 ms | loss 0.00176 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  6.76s | valid loss 0.01860 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    32/  162 batches | lr 0.001240 | 42.56 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch  69 |    64/  162 batches | lr 0.001240 | 41.17 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  69 |    96/  162 batches | lr 0.001240 | 40.86 ms | loss 0.00129 | ppl     1.00\n",
      "| epoch  69 |   128/  162 batches | lr 0.001240 | 40.45 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  69 |   160/  162 batches | lr 0.001240 | 41.57 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  6.77s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    32/  162 batches | lr 0.001216 | 41.03 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  70 |    64/  162 batches | lr 0.001216 | 39.81 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  70 |    96/  162 batches | lr 0.001216 | 42.16 ms | loss 0.00127 | ppl     1.00\n",
      "| epoch  70 |   128/  162 batches | lr 0.001216 | 40.54 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  70 |   160/  162 batches | lr 0.001216 | 41.57 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time:  6.73s | valid loss 0.01864 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |    32/  162 batches | lr 0.001191 | 41.19 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  71 |    64/  162 batches | lr 0.001191 | 40.89 ms | loss 0.00361 | ppl     1.00\n",
      "| epoch  71 |    96/  162 batches | lr 0.001191 | 39.68 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  71 |   128/  162 batches | lr 0.001191 | 40.22 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  71 |   160/  162 batches | lr 0.001191 | 40.38 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  6.64s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    32/  162 batches | lr 0.001167 | 41.72 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  72 |    64/  162 batches | lr 0.001167 | 43.43 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  72 |    96/  162 batches | lr 0.001167 | 39.80 ms | loss 0.00129 | ppl     1.00\n",
      "| epoch  72 |   128/  162 batches | lr 0.001167 | 41.38 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  72 |   160/  162 batches | lr 0.001167 | 39.61 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  6.75s | valid loss 0.01862 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |    32/  162 batches | lr 0.001144 | 40.93 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch  73 |    64/  162 batches | lr 0.001144 | 40.02 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  73 |    96/  162 batches | lr 0.001144 | 39.92 ms | loss 0.00127 | ppl     1.00\n",
      "| epoch  73 |   128/  162 batches | lr 0.001144 | 39.86 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  73 |   160/  162 batches | lr 0.001144 | 39.99 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  6.59s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    32/  162 batches | lr 0.001121 | 41.10 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch  74 |    64/  162 batches | lr 0.001121 | 39.93 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  74 |    96/  162 batches | lr 0.001121 | 39.97 ms | loss 0.00125 | ppl     1.00\n",
      "| epoch  74 |   128/  162 batches | lr 0.001121 | 40.76 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  74 |   160/  162 batches | lr 0.001121 | 40.60 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  6.64s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    32/  162 batches | lr 0.001099 | 40.79 ms | loss 0.00339 | ppl     1.00\n",
      "| epoch  75 |    64/  162 batches | lr 0.001099 | 39.73 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  75 |    96/  162 batches | lr 0.001099 | 39.83 ms | loss 0.00131 | ppl     1.00\n",
      "| epoch  75 |   128/  162 batches | lr 0.001099 | 39.80 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  75 |   160/  162 batches | lr 0.001099 | 39.80 ms | loss 0.00175 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time:  6.56s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    32/  162 batches | lr 0.001077 | 41.18 ms | loss 0.00343 | ppl     1.00\n",
      "| epoch  76 |    64/  162 batches | lr 0.001077 | 39.86 ms | loss 0.00359 | ppl     1.00\n",
      "| epoch  76 |    96/  162 batches | lr 0.001077 | 39.96 ms | loss 0.00125 | ppl     1.00\n",
      "| epoch  76 |   128/  162 batches | lr 0.001077 | 40.09 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  76 |   160/  162 batches | lr 0.001077 | 39.80 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  6.59s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    32/  162 batches | lr 0.001055 | 41.17 ms | loss 0.00343 | ppl     1.00\n",
      "| epoch  77 |    64/  162 batches | lr 0.001055 | 42.19 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  77 |    96/  162 batches | lr 0.001055 | 41.00 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  77 |   128/  162 batches | lr 0.001055 | 40.02 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  77 |   160/  162 batches | lr 0.001055 | 39.80 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  6.70s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    32/  162 batches | lr 0.001034 | 41.37 ms | loss 0.00348 | ppl     1.00\n",
      "| epoch  78 |    64/  162 batches | lr 0.001034 | 39.60 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch  78 |    96/  162 batches | lr 0.001034 | 43.19 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  78 |   128/  162 batches | lr 0.001034 | 39.53 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  78 |   160/  162 batches | lr 0.001034 | 40.00 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  6.68s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    32/  162 batches | lr 0.001014 | 40.79 ms | loss 0.00342 | ppl     1.00\n",
      "| epoch  79 |    64/  162 batches | lr 0.001014 | 41.04 ms | loss 0.00360 | ppl     1.00\n",
      "| epoch  79 |    96/  162 batches | lr 0.001014 | 39.54 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  79 |   128/  162 batches | lr 0.001014 | 39.94 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  79 |   160/  162 batches | lr 0.001014 | 39.88 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  6.60s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    32/  162 batches | lr 0.000993 | 41.53 ms | loss 0.00342 | ppl     1.00\n",
      "| epoch  80 |    64/  162 batches | lr 0.000993 | 39.83 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  80 |    96/  162 batches | lr 0.000993 | 39.92 ms | loss 0.00124 | ppl     1.00\n",
      "| epoch  80 |   128/  162 batches | lr 0.000993 | 40.12 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  80 |   160/  162 batches | lr 0.000993 | 40.01 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time:  6.62s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    32/  162 batches | lr 0.000973 | 41.54 ms | loss 0.00342 | ppl     1.00\n",
      "| epoch  81 |    64/  162 batches | lr 0.000973 | 40.14 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  81 |    96/  162 batches | lr 0.000973 | 39.96 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  81 |   128/  162 batches | lr 0.000973 | 40.04 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  81 |   160/  162 batches | lr 0.000973 | 39.89 ms | loss 0.00174 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  6.61s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    32/  162 batches | lr 0.000954 | 41.32 ms | loss 0.00342 | ppl     1.00\n",
      "| epoch  82 |    64/  162 batches | lr 0.000954 | 40.15 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  82 |    96/  162 batches | lr 0.000954 | 40.65 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  82 |   128/  162 batches | lr 0.000954 | 42.08 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  82 |   160/  162 batches | lr 0.000954 | 40.96 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  6.74s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    32/  162 batches | lr 0.000935 | 44.31 ms | loss 0.00341 | ppl     1.00\n",
      "| epoch  83 |    64/  162 batches | lr 0.000935 | 40.11 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  83 |    96/  162 batches | lr 0.000935 | 41.65 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  83 |   128/  162 batches | lr 0.000935 | 40.94 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  83 |   160/  162 batches | lr 0.000935 | 40.60 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  6.81s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    32/  162 batches | lr 0.000916 | 41.02 ms | loss 0.00343 | ppl     1.00\n",
      "| epoch  84 |    64/  162 batches | lr 0.000916 | 41.74 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  84 |    96/  162 batches | lr 0.000916 | 41.32 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  84 |   128/  162 batches | lr 0.000916 | 43.34 ms | loss 0.00097 | ppl     1.00\n",
      "| epoch  84 |   160/  162 batches | lr 0.000916 | 40.69 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  6.82s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |    32/  162 batches | lr 0.000898 | 41.01 ms | loss 0.00339 | ppl     1.00\n",
      "| epoch  85 |    64/  162 batches | lr 0.000898 | 40.09 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  85 |    96/  162 batches | lr 0.000898 | 40.19 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  85 |   128/  162 batches | lr 0.000898 | 39.64 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  85 |   160/  162 batches | lr 0.000898 | 39.91 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time:  6.59s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    32/  162 batches | lr 0.000880 | 40.99 ms | loss 0.00337 | ppl     1.00\n",
      "| epoch  86 |    64/  162 batches | lr 0.000880 | 39.47 ms | loss 0.00358 | ppl     1.00\n",
      "| epoch  86 |    96/  162 batches | lr 0.000880 | 40.86 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  86 |   128/  162 batches | lr 0.000880 | 40.10 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  86 |   160/  162 batches | lr 0.000880 | 39.73 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  6.60s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    32/  162 batches | lr 0.000862 | 41.08 ms | loss 0.00346 | ppl     1.00\n",
      "| epoch  87 |    64/  162 batches | lr 0.000862 | 39.69 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  87 |    96/  162 batches | lr 0.000862 | 40.05 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  87 |   128/  162 batches | lr 0.000862 | 39.67 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  87 |   160/  162 batches | lr 0.000862 | 39.87 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  6.57s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    32/  162 batches | lr 0.000845 | 40.78 ms | loss 0.00341 | ppl     1.00\n",
      "| epoch  88 |    64/  162 batches | lr 0.000845 | 39.62 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  88 |    96/  162 batches | lr 0.000845 | 41.03 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  88 |   128/  162 batches | lr 0.000845 | 41.00 ms | loss 0.00094 | ppl     1.00\n",
      "| epoch  88 |   160/  162 batches | lr 0.000845 | 43.66 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  6.75s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    32/  162 batches | lr 0.000828 | 40.44 ms | loss 0.00337 | ppl     1.00\n",
      "| epoch  89 |    64/  162 batches | lr 0.000828 | 39.26 ms | loss 0.00356 | ppl     1.00\n",
      "| epoch  89 |    96/  162 batches | lr 0.000828 | 39.01 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  89 |   128/  162 batches | lr 0.000828 | 38.78 ms | loss 0.00098 | ppl     1.00\n",
      "| epoch  89 |   160/  162 batches | lr 0.000828 | 38.93 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  6.44s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    32/  162 batches | lr 0.000812 | 50.32 ms | loss 0.00342 | ppl     1.00\n",
      "| epoch  90 |    64/  162 batches | lr 0.000812 | 42.20 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  90 |    96/  162 batches | lr 0.000812 | 43.23 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  90 |   128/  162 batches | lr 0.000812 | 42.46 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  90 |   160/  162 batches | lr 0.000812 | 46.07 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time:  7.36s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    32/  162 batches | lr 0.000795 | 42.66 ms | loss 0.00338 | ppl     1.00\n",
      "| epoch  91 |    64/  162 batches | lr 0.000795 | 42.15 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  91 |    96/  162 batches | lr 0.000795 | 41.56 ms | loss 0.00124 | ppl     1.00\n",
      "| epoch  91 |   128/  162 batches | lr 0.000795 | 41.81 ms | loss 0.00097 | ppl     1.00\n",
      "| epoch  91 |   160/  162 batches | lr 0.000795 | 41.22 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  6.87s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    32/  162 batches | lr 0.000779 | 41.28 ms | loss 0.00335 | ppl     1.00\n",
      "| epoch  92 |    64/  162 batches | lr 0.000779 | 42.59 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  92 |    96/  162 batches | lr 0.000779 | 40.91 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  92 |   128/  162 batches | lr 0.000779 | 45.34 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  92 |   160/  162 batches | lr 0.000779 | 45.49 ms | loss 0.00173 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  7.08s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    32/  162 batches | lr 0.000764 | 43.81 ms | loss 0.00338 | ppl     1.00\n",
      "| epoch  93 |    64/  162 batches | lr 0.000764 | 40.08 ms | loss 0.00350 | ppl     1.00\n",
      "| epoch  93 |    96/  162 batches | lr 0.000764 | 40.42 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  93 |   128/  162 batches | lr 0.000764 | 41.82 ms | loss 0.00097 | ppl     1.00\n",
      "| epoch  93 |   160/  162 batches | lr 0.000764 | 42.07 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  6.84s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    32/  162 batches | lr 0.000749 | 44.64 ms | loss 0.00337 | ppl     1.00\n",
      "| epoch  94 |    64/  162 batches | lr 0.000749 | 41.78 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch  94 |    96/  162 batches | lr 0.000749 | 43.63 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  94 |   128/  162 batches | lr 0.000749 | 42.30 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  94 |   160/  162 batches | lr 0.000749 | 43.37 ms | loss 0.00170 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  7.15s | valid loss 0.01859 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    32/  162 batches | lr 0.000734 | 40.98 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch  95 |    64/  162 batches | lr 0.000734 | 39.64 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  95 |    96/  162 batches | lr 0.000734 | 39.07 ms | loss 0.00118 | ppl     1.00\n",
      "| epoch  95 |   128/  162 batches | lr 0.000734 | 39.29 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  95 |   160/  162 batches | lr 0.000734 | 39.30 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time:  6.50s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    32/  162 batches | lr 0.000719 | 40.73 ms | loss 0.00339 | ppl     1.00\n",
      "| epoch  96 |    64/  162 batches | lr 0.000719 | 39.05 ms | loss 0.00353 | ppl     1.00\n",
      "| epoch  96 |    96/  162 batches | lr 0.000719 | 39.20 ms | loss 0.00124 | ppl     1.00\n",
      "| epoch  96 |   128/  162 batches | lr 0.000719 | 39.13 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  96 |   160/  162 batches | lr 0.000719 | 39.31 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  6.48s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |    32/  162 batches | lr 0.000705 | 40.29 ms | loss 0.00336 | ppl     1.00\n",
      "| epoch  97 |    64/  162 batches | lr 0.000705 | 38.76 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch  97 |    96/  162 batches | lr 0.000705 | 39.05 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  97 |   128/  162 batches | lr 0.000705 | 39.01 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  97 |   160/  162 batches | lr 0.000705 | 39.03 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  6.44s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    32/  162 batches | lr 0.000690 | 41.94 ms | loss 0.00340 | ppl     1.00\n",
      "| epoch  98 |    64/  162 batches | lr 0.000690 | 40.57 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch  98 |    96/  162 batches | lr 0.000690 | 39.17 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  98 |   128/  162 batches | lr 0.000690 | 39.31 ms | loss 0.00094 | ppl     1.00\n",
      "| epoch  98 |   160/  162 batches | lr 0.000690 | 39.19 ms | loss 0.00172 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  6.57s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    32/  162 batches | lr 0.000677 | 40.24 ms | loss 0.00336 | ppl     1.00\n",
      "| epoch  99 |    64/  162 batches | lr 0.000677 | 39.17 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch  99 |    96/  162 batches | lr 0.000677 | 39.18 ms | loss 0.00119 | ppl     1.00\n",
      "| epoch  99 |   128/  162 batches | lr 0.000677 | 39.21 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  99 |   160/  162 batches | lr 0.000677 | 39.07 ms | loss 0.00169 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  6.46s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    32/  162 batches | lr 0.000663 | 40.45 ms | loss 0.00338 | ppl     1.00\n",
      "| epoch 100 |    64/  162 batches | lr 0.000663 | 39.16 ms | loss 0.00354 | ppl     1.00\n",
      "| epoch 100 |    96/  162 batches | lr 0.000663 | 39.33 ms | loss 0.00119 | ppl     1.00\n",
      "| epoch 100 |   128/  162 batches | lr 0.000663 | 39.02 ms | loss 0.00093 | ppl     1.00\n",
      "| epoch 100 |   160/  162 batches | lr 0.000663 | 39.13 ms | loss 0.00171 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  6.47s | valid loss 0.01858 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train_data, val_data = get_data()\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# lr = 0.005 \n",
    "# #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_model(train_data)\n",
    "    \n",
    "    \n",
    "    # if(epoch % 10 == 0):\n",
    "        #val_loss = plot_and_loss(model, val_data,epoch)\n",
    "\n",
    "        # predict_future(model, val_data,200)\n",
    "        \n",
    "    # else:\n",
    "    val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesDataset(object):\n",
    "#     def __init__(self, data, categorical_cols, target_col, seq_length, prediction_window=1):\n",
    "#         '''\n",
    "#         :param data: dataset of type pandas.DataFrame\n",
    "#         :param categorical_cols: name of the categorical columns, if None pass empty list\n",
    "#         :param target_col: name of the targeted column\n",
    "#         :param seq_length: window length to use\n",
    "#         :param prediction_window: window length to predict\n",
    "#         '''\n",
    "#         self.data = data\n",
    "#         self.categorical_cols = categorical_cols\n",
    "#         self.numerical_cols = list(set(data.columns) - set(categorical_cols) - set(target_col))\n",
    "#         self.target_col = target_col\n",
    "#         self.seq_length = seq_length\n",
    "#         self.prediction_window = prediction_window\n",
    "#         self.preprocessor = None\n",
    "\n",
    "#     def preprocess_data(self):\n",
    "#         '''Preprocessing function'''\n",
    "#         X = self.data.drop(self.target_col, axis=1)\n",
    "#         y = self.data[self.target_col]\n",
    "\n",
    "#         self.preprocess = ColumnTransformer(\n",
    "#             [(\"scaler\", StandardScaler(), self.numerical_cols),\n",
    "#              (\"encoder\", OneHotEncoder(), self.categorical_cols)],\n",
    "#             remainder=\"passthrough\"\n",
    "#         )\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "#         X_train = self.preprocessor.fit_transform(X_train)\n",
    "#         X_test = self.preprocessor.transform(X_test)\n",
    "\n",
    "#         if self.target_col:\n",
    "#             return X_train, X_test, y_train.values, y_test.values\n",
    "#         return X_train, X_test\n",
    "\n",
    "#     def frame_series(self, X, y=None):\n",
    "#         '''\n",
    "#         Function used to prepare the data for time series prediction\n",
    "#         :param X: set of features\n",
    "#         :param y: targeted value to predict\n",
    "#         :return: TensorDataset\n",
    "#         '''\n",
    "#         nb_obs, nb_features = X.shape\n",
    "#         features, target, y_hist = [], [], []\n",
    "\n",
    "#         for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#             features.append(torch.FloatTensor(X[i:i + self.seq_length, :]).unsqueeze(0))\n",
    "\n",
    "#         features_var = torch.cat(features)\n",
    "\n",
    "#         if y is not None:\n",
    "#             for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#                 target.append(\n",
    "#                     torch.tensor(y[i + self.seq_length:i + self.seq_length + self.prediction_window]))\n",
    "#                 y_hist.append(\n",
    "#                     torch.tensor(y[i + self.seq_length - 1:i + self.seq_length + self.prediction_window - 1]))\n",
    "#             target_var, y_hist_var = torch.cat(target), torch.cat(y_hist)\n",
    "#             return TensorDataset(features_var, target_var, y_hist_var)\n",
    "#         return TensorDataset(features_var)\n",
    "\n",
    "#     def get_loaders(self, batch_size: int):\n",
    "#         '''\n",
    "#         Preprocess and frame the dataset\n",
    "#         :param batch_size: batch size\n",
    "#         :return: DataLoaders associated to training and testing data\n",
    "#         '''\n",
    "#         X_train, X_test, y_train, y_test = self.preprocess_data()\n",
    "\n",
    "#         train_dataset = self.frame_series(X_train, y_train)\n",
    "#         test_dataset = self.frame_series(X_test, y_test)\n",
    "\n",
    "#         train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         return train_iter, test_iter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('TAAC_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f251cf0b22a23ca88a79bdc1e3e56992e90498b36dfcdde46ef40d449004d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
