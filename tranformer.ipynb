{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from tdqm import tdqm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" \\\n",
    "Our transformer model will have an input size of (32, 128, 5) \\\n",
    "32 -> batch size \\\n",
    "128 -> sequence length \\\n",
    "5 -> number of features (Open, Low, High, Close) \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features are separated into sequences of 128 days, 4 price features and 1 volume feature\n",
    "seq_len = 128\n",
    "# target_len = 3\n",
    "\n",
    "# during a single step the model receives 32 sequences\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock for the deep learning\n",
    "ticker = \"GOOGL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset (use the function in data/data.py)\n",
    "df = pd.read_csv(\n",
    "    \"data/data.csv\", usecols=[\"Date\", \"High\", \"Low\", \"Open\", \"Close\", \"Volume\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6661</th>\n",
       "      <td>2022-11-14</td>\n",
       "      <td>96.790001</td>\n",
       "      <td>94.510002</td>\n",
       "      <td>95.089996</td>\n",
       "      <td>95.699997</td>\n",
       "      <td>30179500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6662</th>\n",
       "      <td>2022-11-15</td>\n",
       "      <td>100.139999</td>\n",
       "      <td>96.709999</td>\n",
       "      <td>98.260002</td>\n",
       "      <td>98.440002</td>\n",
       "      <td>41640800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6663</th>\n",
       "      <td>2022-11-16</td>\n",
       "      <td>99.639999</td>\n",
       "      <td>97.639999</td>\n",
       "      <td>97.900002</td>\n",
       "      <td>98.849998</td>\n",
       "      <td>29105200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>2022-11-17</td>\n",
       "      <td>99.279999</td>\n",
       "      <td>96.790001</td>\n",
       "      <td>96.970001</td>\n",
       "      <td>98.360001</td>\n",
       "      <td>26052600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>2022-11-18</td>\n",
       "      <td>98.900002</td>\n",
       "      <td>96.370003</td>\n",
       "      <td>98.769997</td>\n",
       "      <td>97.430000</td>\n",
       "      <td>28328800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        High        Low       Open      Close      Volume\n",
       "6661  2022-11-14   96.790001  94.510002  95.089996  95.699997  30179500.0\n",
       "6662  2022-11-15  100.139999  96.709999  98.260002  98.440002  41640800.0\n",
       "6663  2022-11-16   99.639999  97.639999  97.900002  98.849998  29105200.0\n",
       "6664  2022-11-17   99.279999  96.790001  96.970001  98.360001  26052600.0\n",
       "6665  2022-11-18   98.900002  96.370003  98.769997  97.430000  28328800.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid dividing by 0\n",
    "df[\"Volume\"].replace(to_replace=0, method=\"ffill\", inplace=True)\n",
    "\n",
    "# Sort the values based on date\n",
    "df.sort_values(\"Date\", inplace=True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open\"] = df[\"Open\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"High\"] = df[\"High\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Low\"] = df[\"Low\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Close\"] = df[\"Close\"].pct_change()  # Create arithmetic returns column\n",
    "df[\"Volume\"] = df[\"Volume\"].pct_change()\n",
    "\n",
    "# Drop the rows with the NaN created by the percentage change\n",
    "df.dropna(how=\"any\", axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values to create the separation of the dataset\n",
    "times = sorted(df.index.values)\n",
    "last_10pct = sorted(df.index.values)[-int(0.1 * len(times))]\n",
    "last_20pct = sorted(df.index.values)[-int(0.2 * len(times))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max price columns\n",
    "min_return = min(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].min(axis=0)\n",
    ")\n",
    "max_return = max(\n",
    "    df[(df.index < last_20pct)][[\"Open\", \"High\", \"Low\", \"Close\"]].max(axis=0)\n",
    ")\n",
    "\n",
    "# Min-max normalize price columns (0-1 range)\n",
    "df[\"Open\"] = (df[\"Open\"] - min_return) / (max_return - min_return)\n",
    "df[\"High\"] = (df[\"High\"] - min_return) / (max_return - min_return)\n",
    "df[\"Low\"] = (df[\"Low\"] - min_return) / (max_return - min_return)\n",
    "df[\"Close\"] = (df[\"Close\"] - min_return) / (max_return - min_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max volume column\n",
    "min_volume = df[(df.index < last_20pct)][\"Volume\"].min(axis=0)\n",
    "max_volume = df[(df.index < last_20pct)][\"Volume\"].max(axis=0)\n",
    "\n",
    "# Min-max normalize volume columns (0-1 range)\n",
    "df[\"Volume\"] = (df[\"Volume\"] - min_volume) / (max_volume - min_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5332, 5)\n",
      "Validation data shape: (667, 5)\n",
      "Test data shape: (666, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_7104/2609535530.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_7104/2609535530.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val.drop(columns=[\"Date\"], inplace=True)\n",
      "/var/folders/r4/wcp88kdx6r9g4_1wtlb27h2r0000gn/T/ipykernel_7104/2609535530.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.drop(columns=[\"Date\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519737</td>\n",
       "      <td>0.516795</td>\n",
       "      <td>0.399106</td>\n",
       "      <td>0.618376</td>\n",
       "      <td>0.055513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.149419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.367163</td>\n",
       "      <td>0.149419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.494738</td>\n",
       "      <td>0.636230</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.398992</td>\n",
       "      <td>0.110893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.314767</td>\n",
       "      <td>0.208230</td>\n",
       "      <td>0.381155</td>\n",
       "      <td>0.236202</td>\n",
       "      <td>0.117750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       High       Low      Open     Close    Volume\n",
       "1  0.519737  0.516795  0.399106  0.618376  0.055513\n",
       "2  0.367163  0.367163  0.367163  0.367163  0.149419\n",
       "3  0.367163  0.367163  0.367163  0.367163  0.149419\n",
       "4  0.494738  0.636230  0.672131  0.398992  0.110893\n",
       "5  0.314767  0.208230  0.381155  0.236202  0.117750"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n",
    "df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n",
    "df_test = df[(df.index >= last_10pct)]\n",
    "\n",
    "# Drop the date column from the splitted datasets\n",
    "df_train.drop(columns=[\"Date\"], inplace=True)\n",
    "df_val.drop(columns=[\"Date\"], inplace=True)\n",
    "df_test.drop(columns=[\"Date\"], inplace=True)\n",
    "\n",
    "# Train data into arrays np.ndarray\n",
    "train_data = df_train.values\n",
    "val_data = df_val.values\n",
    "test_data = df_test.values\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.T#[:,0:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
    "# src = torch.rand(10, 5, 128)\n",
    "# out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerData(Dataset):\n",
    "    def __init__(self, data: np.ndarray, seq_len: int) -> None:\n",
    "        \"\"\"Init function of dataset class\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): data from the dataframe to numpy\n",
    "            seq_len (int): len of values to base for the prediction\n",
    "        \"\"\"\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for i in range(seq_len, len(data)):\n",
    "            # Chunks of  data with a length of 128 df-rows\n",
    "            self.inputs.append(data[i-seq_len:i].T)\n",
    "            \n",
    "            # Value of 4th column (Close Price) of df-row 128+1\n",
    "            self.targets.append(data[:, 3][i])\n",
    "        \n",
    "        self.inputs, self.targets = torch.FloatTensor(np.array(self.inputs)), torch.FloatTensor(np.array(self.targets))\n",
    "        # print(self.inputs)\n",
    "        # print(self.inputs.shape)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        \"\"\"Get item at a certain index\n",
    "\n",
    "        Args:\n",
    "            idx (int): index to get the value\n",
    "\n",
    "        Returns:\n",
    "            dict: returns the input and the target\n",
    "        \"\"\"\n",
    "        return {'inputs': self.inputs[idx],\n",
    "                'targets': self.targets[idx]}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Length function \n",
    "\n",
    "        Returns:\n",
    "            int: length of the Dataset\n",
    "        \"\"\"\n",
    "        return min(len(self.inputs), len(self.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TickerData(data=train_data, seq_len=seq_len)\n",
    "val_data = TickerData(data=val_data, seq_len=seq_len)\n",
    "test_data = TickerData(data=test_data, seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5204"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n",
      "torch.Size([32, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "for _,batch in enumerate(train_loader):\n",
    "    print(batch[\"inputs\"].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "#\n",
    "#print(out)\n",
    "\n",
    "# input_window = 100\n",
    "# output_window = 5\n",
    "# batch_size = 10 # batch size\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()       \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         #pe.requires_grad = False\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def call(self, x):\n",
    "#         # Add extra dimension to self.pe\n",
    "#         self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(x)\n",
    "#         # print(x.size())\n",
    "#         print(self.pe)\n",
    "#         # print(self.pe.size())\n",
    "#         # self.pe = self.pe.unsqueeze(0)\n",
    "#         return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class Time2Vector(nn.Module):\n",
    "  def __init__(self, seq_len):\n",
    "    super(Time2Vector, self).__init__()\n",
    "    self.seq_len = seq_len\n",
    "\n",
    "    # Initialize weights and biases with shape (seq_len)\n",
    "    self.weights_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.bias_linear = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.weights_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "    self.bias_periodic = nn.Parameter(torch.Tensor(seq_len))\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''Calculate linear and periodic time features'''\n",
    "    x = torch.mean(x[:,:,:4], dim=-1) \n",
    "    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "    time_linear = time_linear.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "    time_periodic = torch.sin(x * self.weights_periodic + self.bias_periodic)\n",
    "    time_periodic = time_periodic.unsqueeze(-1) # Add dimension (batch, seq_len, 1)\n",
    "    return torch.cat([time_linear, time_periodic], dim=-1) # shape = (batch, seq_len, 2)\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=128,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        # self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.t2v = Time2Vector(128)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)     \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        \n",
    "        if self.src_mask is None or self.src_mask.size(0) != 5:\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(5).to(device)\n",
    "            self.src_mask = mask\n",
    "        # print(src.size())\n",
    "        # src = self.pos_encoder(src)\n",
    "        src = self.t2v(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8, batch_first=True)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "# src = torch.rand(32, 5, 128)\n",
    "# out = transformer_encoder(src)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "lr = 0.005 \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data):\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # data, targets = get_batch(train_data, i,batch_size)\n",
    "        data, targets = batch['inputs'].to(device), batch['targets'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  \n",
    "\n",
    "        if calculate_loss_over_all_values:\n",
    "            loss = criterion(output, targets)\n",
    "        else:\n",
    "            loss = criterion(output[-1:], targets[-1:])\n",
    "    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, i, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_data):\n",
    "            data, target = batch['inputs'].to(device), batch[\"targets\"].to(device)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    plt.plot(test_result,color=\"red\")\n",
    "    plt.plot(truth[:500],color=\"blue\")\n",
    "    plt.plot(test_result-truth,color=\"green\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    plt.close()\n",
    "    \n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_future(eval_model, data_source,steps):\n",
    "#     eval_model.eval() \n",
    "#     total_loss = 0.\n",
    "#     test_result = torch.Tensor(0)    \n",
    "#     truth = torch.Tensor(0)\n",
    "#     _ , data = get_batch(data_source, 0,1)\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(0, steps,1):\n",
    "#             input = torch.clone(data[-input_window:])\n",
    "#             input[-output_window:] = 0     \n",
    "#             output = eval_model(data[-input_window:])                        \n",
    "#             data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "#     data = data.cpu().view(-1)\n",
    "    \n",
    "#     plt.plot(data,color=\"red\")       \n",
    "#     plt.plot(data[:input_window],color=\"blue\")\n",
    "#     plt.grid(True, which='both')\n",
    "#     plt.axhline(y=0, color='k')\n",
    "#     plt.savefig('graph/transformer-future%d.png'%steps)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(val_data):\n",
    "            data, targets = batch['inputs'].to(device), batch[\"targets\"].to(device)\n",
    "            output = eval_model(data)            \n",
    "            if calculate_loss_over_all_values:\n",
    "                total_loss += len(data[0])* criterion(output, targets).cpu().item()\n",
    "            else:                                \n",
    "                total_loss += len(data[0])* criterion(output[-1:], targets[-1:]).cpu().item()            \n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_model(train_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mif\u001b[39;00m(epoch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         val_loss \u001b[39m=\u001b[39m plot_and_loss(model, val_data,epoch)\n",
      "\u001b[1;32m/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb Cell 35\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m data, targets \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), batch[\u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m calculate_loss_over_all_values:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, targets)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb Cell 35\u001b[0m in \u001b[0;36mTransAm.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_mask \u001b[39m=\u001b[39m mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# print(src.size())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# src = self.pos_encoder(src)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt2v(src)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder(src,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_mask)\u001b[39m#, self.src_mask)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/TAAC_project/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb Cell 35\u001b[0m in \u001b[0;36mTime2Vector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m'''Calculate linear and periodic time features'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x[:,:,:\u001b[39m4\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m time_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights_linear \u001b[39m*\u001b[39;49m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_linear \u001b[39m# Linear time feature\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m time_linear \u001b[39m=\u001b[39m time_linear\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# Add dimension (batch, seq_len, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/filipebarnabe/Documents/mecd_masters/TAAC/project/deeplearning_tradingBot/tranformer.ipynb#X36sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m time_periodic \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_periodic \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_periodic)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# train_data, val_data = get_data()\n",
    "\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# lr = 0.005 \n",
    "# #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_model(train_data)\n",
    "    \n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        # predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeSeriesDataset(object):\n",
    "#     def __init__(self, data, categorical_cols, target_col, seq_length, prediction_window=1):\n",
    "#         '''\n",
    "#         :param data: dataset of type pandas.DataFrame\n",
    "#         :param categorical_cols: name of the categorical columns, if None pass empty list\n",
    "#         :param target_col: name of the targeted column\n",
    "#         :param seq_length: window length to use\n",
    "#         :param prediction_window: window length to predict\n",
    "#         '''\n",
    "#         self.data = data\n",
    "#         self.categorical_cols = categorical_cols\n",
    "#         self.numerical_cols = list(set(data.columns) - set(categorical_cols) - set(target_col))\n",
    "#         self.target_col = target_col\n",
    "#         self.seq_length = seq_length\n",
    "#         self.prediction_window = prediction_window\n",
    "#         self.preprocessor = None\n",
    "\n",
    "#     def preprocess_data(self):\n",
    "#         '''Preprocessing function'''\n",
    "#         X = self.data.drop(self.target_col, axis=1)\n",
    "#         y = self.data[self.target_col]\n",
    "\n",
    "#         self.preprocess = ColumnTransformer(\n",
    "#             [(\"scaler\", StandardScaler(), self.numerical_cols),\n",
    "#              (\"encoder\", OneHotEncoder(), self.categorical_cols)],\n",
    "#             remainder=\"passthrough\"\n",
    "#         )\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "#         X_train = self.preprocessor.fit_transform(X_train)\n",
    "#         X_test = self.preprocessor.transform(X_test)\n",
    "\n",
    "#         if self.target_col:\n",
    "#             return X_train, X_test, y_train.values, y_test.values\n",
    "#         return X_train, X_test\n",
    "\n",
    "#     def frame_series(self, X, y=None):\n",
    "#         '''\n",
    "#         Function used to prepare the data for time series prediction\n",
    "#         :param X: set of features\n",
    "#         :param y: targeted value to predict\n",
    "#         :return: TensorDataset\n",
    "#         '''\n",
    "#         nb_obs, nb_features = X.shape\n",
    "#         features, target, y_hist = [], [], []\n",
    "\n",
    "#         for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#             features.append(torch.FloatTensor(X[i:i + self.seq_length, :]).unsqueeze(0))\n",
    "\n",
    "#         features_var = torch.cat(features)\n",
    "\n",
    "#         if y is not None:\n",
    "#             for i in range(1, nb_obs - self.seq_length - self.prediction_window):\n",
    "#                 target.append(\n",
    "#                     torch.tensor(y[i + self.seq_length:i + self.seq_length + self.prediction_window]))\n",
    "#                 y_hist.append(\n",
    "#                     torch.tensor(y[i + self.seq_length - 1:i + self.seq_length + self.prediction_window - 1]))\n",
    "#             target_var, y_hist_var = torch.cat(target), torch.cat(y_hist)\n",
    "#             return TensorDataset(features_var, target_var, y_hist_var)\n",
    "#         return TensorDataset(features_var)\n",
    "\n",
    "#     def get_loaders(self, batch_size: int):\n",
    "#         '''\n",
    "#         Preprocess and frame the dataset\n",
    "#         :param batch_size: batch size\n",
    "#         :return: DataLoaders associated to training and testing data\n",
    "#         '''\n",
    "#         X_train, X_test, y_train, y_test = self.preprocess_data()\n",
    "\n",
    "#         train_dataset = self.frame_series(X_train, y_train)\n",
    "#         test_dataset = self.frame_series(X_test, y_test)\n",
    "\n",
    "#         train_iter = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         test_iter = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#         return train_iter, test_iter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('TAAC_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4f251cf0b22a23ca88a79bdc1e3e56992e90498b36dfcdde46ef40d449004d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
